{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"configuration/","title":"Configuration","text":"<p>Configuration for TopoStats is done using a YAML configuration that is specified on the command line when invoking. A default configuration file is provided in the TopoStats repository at <code>topostats/default_config.yaml</code>. The file contains comments indicating valid values for many of the fields. If no configuration file is provided this default configuration is loaded automatically and used.</p> <p>You can make a copy of the <code>default_config.yaml</code> and modify it for your own use. Once saved you can run TopoStats with this configuration file as shown below.</p> <pre><code>run_topostats --config my_config.yaml\n</code></pre> <p>On completion a copy of the configuration that was used is written to the output directory.</p>"},{"location":"configuration/#fields","title":"Fields","text":"<p>Aside from the comments in YAML file itself the fields are described below.</p> Section Sub-Section Data Type Default Description <code>base_dir</code> string <code>./</code> Directory to recursively search for files within. <code>output_dir</code> string <code>./output</code> Directory that output should be saved to. <code>warnings</code> string <code>ignore</code> Turns off warnings being shown. <code>cores</code> integer <code>4</code> Number of cores to run parallel processes on. <code>quiet</code> false <code>file_ext</code> string <code>.spm</code> File extensions to search for. <code>loading</code> <code>channel</code> string <code>Height</code> The channel of data to be processed, what this is will depend on the file-format you are processing and the channel you wish to process. <code>filter</code> <code>run</code> boolean <code>true</code> Whether to run the filtering stage, without this other stages won't run so leave as <code>true</code>. <code>threshold_method</code> str <code>std_dev</code> Threshold method for filtering, options are <code>ostu</code>, <code>std_dev</code> or <code>absolute</code>. <code>otsu_threshold_multiplier</code> float <code>1.0</code> <code>threshold_std_dev</code> float <code>1.0</code> <code>threshold_absolute_lower</code> float <code>-1.0</code> <code>threshold_absolute_upper</code> float <code>1.0</code> <code>gaussian_size</code> float <code>0.5</code> The number of standard deviations to build the Gaussian kernel and thus affects the degree of blurring. See skimage.filters.gaussian and <code>sigma</code> for more information <code>gaussian_mode</code> string <code>nearest</code> <code>grains</code> <code>run</code> boolean <code>true</code> Whether to run grain finding. Options <code>true</code>, <code>false</code> <code>absolute_smallest_grain_size</code> int <code>100</code> The smallest size of grains to be included (in pixels), anything smaller than this is considered noise and removed. <code>threshold_method</code> float <code>std_dev</code> Threshold method for grain finding.  Options : <code>otsu</code>, <code>std_dev</code>, <code>absolute</code> <code>otsu_threshold_multiplier</code> <code>1.0</code> Factor by which the derived Otsu Threshold should be scaled. <code>threshold_std_dev</code> <code>1.0</code> <code>threshold_absolute_lower</code> <code>1.0</code> <code>threshold_absolute_upper</code> <code>1.0</code> <code>absolute_area_threshold</code> dictionary <code>...upper</code> list <code>[500,800]</code> Height above surface [Low, High] in nm^2 (also takes null) <code>...lower</code> <code>[null, null]</code> Height below surface [Low, High] in nm^2 (also takes null) <code>direction</code> <code>upper</code> Defines whether to look for grains above or below thresholds or both. Options: <code>upper</code>, <code>lower</code>, <code>both</code> <code>background</code> float <code>0.0</code> <code>grainstats</code> <code>run</code> boolean <code>true</code> Whether to calculate grain statistics. Options : <code>true</code>, <code>false</code> <code>cropped_size</code> float <code>40.0</code> Force cropping of grains to this length (in nm) of square cropped images (can take <code>-1</code> for grain-sized box) <code>save_cropped_grains</code> boolean <code>true</code> Options : true, false <code>dnatracing</code> <code>run</code> boolean <code>true</code> Whether to run DNA Tracing.  Options : true, false <code>plotting</code> <code>run</code> boolean <code>true</code> Whether to run plotting. Options : <code>true</code>, <code>false</code> <code>save_format</code> string <code>png</code> Format to save images in, see matplotlib.pyplot.savefig <code>image_set</code> string <code>all</code> Which images to plot. Options : <code>all</code>, <code>core</code> <code>zrange</code> list <code>[0, 3]</code> Low and high height range for core images (can take [null, null]) <code>colorbar</code> boolean <code>true</code> Whether to include the colorbar scale in plots. Options <code>true</code>, <code>false</code> <code>axes</code> boolean <code>true</code> Wether to include the axes in the produced plots. <code>cmap</code> string <code>nanoscope</code> Colormap to use in plotting. Options : <code>nanoscope</code>, <code>afmhot</code> <code>histogram_log_axis</code> bbolean <code>false</code> Whether to plot hisograms using a logarithmic scale or not. Options: <code>true</code>, <code>false</code>."},{"location":"configuration/#validation","title":"Validation","text":"<p>Configuration files are validated against a schema to check that the values in the configuration file are within the expected ranges or valid parameters. This helps capture problems early and should provide informative messages as to what needs correcting if there are errors.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>This document describes how to contribute to the development of this software.</p>"},{"location":"contributing/#contribution-workflow","title":"Contribution Workflow","text":""},{"location":"contributing/#create-an-issue","title":"Create an Issue","text":"<p>Before starting please search for and review the existing issues (both <code>open</code> and <code>closed</code>) and pull requests to see if anyone has reported the bug or requested the feature already or work is in progress. If nothing exists then you should create a new issue using one of the templates provided.</p>"},{"location":"contributing/#cloning-the-repository","title":"Cloning the repository","text":"<p>If you wish to make changes yourself you will have to fork the repository to your own account and then clone that if you are not a member of AFM-SPM Organisation. If you are a member then you can clone the repository and make contributions directly.</p> <pre><code># Member of AFM-SPM Organisation\ngit clone git@github.com:AFM-SPM/TopoStats.git\n# Non-member of AFM-SPM cloning fork\ngit clone git@github.com:&lt;YOUR_GITHUB_USERNAME&gt;/TopoStats.git\n</code></pre>"},{"location":"contributing/#install-additional-dependencies","title":"Install Additional Dependencies","text":"<p>If you are going to contribute you should install the additional dependencies for undertaking such work. There are three groups of additional dependencies, <code>dev</code>, <code>docs</code> and <code>tests</code> and you should install all three using <code>pip</code> as shown below.</p> <pre><code>cd TopoStats\npip install \".[dev,docs,tests]\"\n</code></pre>"},{"location":"contributing/#creating-a-branch","title":"Creating a branch","text":"<p>Typically you will now create a branch to work on the issue you wish to address. It is not compulsory but we try to use a consistent nomenclature for branches that shows who has worked on the branch, the issue it pertains to and a short description of the work. To which end you will see branches with the form <code>&lt;GITHUB_USERNAME&gt;/&lt;GITHUB_ISSUE&gt;-&lt;DESCRIPTION&gt;</code>. Some examples are shown below...</p> Branch User Issue Description <code>ns-rse/259-contributing</code> <code>ns-rse</code> 259 <code>contributing</code> short for the issue subject Add contributing section to documentation. <code>SylviaWhittle/204-nanometre-scaling</code> <code>SylviaWhittle</code> 204 <code>nanometre-scaling</code> short for the issue subject Colour scale in nanometers not pixels. <p>How you create a branch depends on how you use Git, some use the integration provided by their IDE, others dedicated clients such as GitKraken and some may use the command line interface. These instructions use the later but you are of course free to use your chosen method of managing Git and GitHub.</p> <p>In this example we branch from <code>dev</code> and create a new branch called <code>ns-rse/000-fix-an-issue</code>.</p> <pre><code># Ensure you are up-to-date on the main branch\ngit checkout main\ngit pull\n# Create and checkout a branch in one step\ngit checkout -b ns-rse/000-fix-an-issue\n# Create and checkout a branch in two steps\ngit branch dev ns-rse/000-fix-an-issue\ngit checkout ns-rse/000-fix-an-issue\n</code></pre> <p>You can now start working on your issue and making regular commits, but please bear in mind the following section on Coding Standards.</p>"},{"location":"contributing/#coding-standards","title":"Coding Standards","text":"<p>To make the codebase easier to maintain we ask that you follow the guidelines below on coding style, linting, typing, documentation and testing.</p>"},{"location":"contributing/#coding-stylelinting","title":"Coding Style/Linting","text":"<p>Using a consistent coding style has many benefits (see Linting : What is all the fluff about?). For this project we aim to adhere to PEP8 - the style Guide for Python Code and do so using the formatting linters black and flake8.  Many popular IDEs such as VSCode, PyCharm, Spyder and Emacs all have support for integrating these linters into your workflow such that when you save a file the linting/formatting is automatically applied.</p> <p>We also like to ensure the code passes pylint which helps identify code duplication and reduces some of the code smells that we are all prone to making. A <code>.pylintrc</code> is included in the repository. Currently this isn't strictly applied but it is planned for part of the CI/CD pipeline and so we would be grateful if you could lint your code before making Pull Requests.</p>"},{"location":"contributing/#pre-commit","title":"Pre-commit","text":"<p>pre-commit is a powerful and useful tool that runs hooks on your code prior to making commits. For a more detailed exposition see pre-commit : Protecting your future self.</p> <p>The repository includes <code>pre-commit</code> as a development dependency as well as a <code>.pre-commit-config.yaml</code>. To use these locally install <code>pre-commit</code> in your virtual environment and then install the configuration and all the configured hooks (NB this will download specific virtual environments that <code>pre-commit</code> uses when running hooks so the first time this is run may take a little while).</p> <pre><code>pip install .[dev]\npre-commit install --install-hooks\n</code></pre> <p>Currently there are hooks to remove trailing whitespace, check YAML configuration files and a few other common checks as well as hooks for <code>black</code> and <code>flake8</code>. If these fail then you will not be able to make a commit until they are fixed. The <code>black</code> hook will automatically format failed files so you can simply <code>git add</code> those and try committing straight away. <code>flake8</code> does not correct files automatically so the errors will need manually correcting.</p>"},{"location":"contributing/#typing","title":"Typing","text":"<p>Whilst Python is a dynamically typed language (that is the type of an object is determined dynamically) the use of Type Hints is strongly encouraged as it makes reading and understanding the code considerably easier for contributors. For more on Type Hints see PEP483 and PEP484</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>All classes, methods and functions should have Numpy Docstrings defining their functionality, parameters and return values and pylint will note and report the absence of docstrings by way of the <code>missing-function-docstring</code> condition.</p> <p>Further, when new methods are incorporated into the package that introduce changes to the configuration they should be documented under Parameter Configuration</p>"},{"location":"contributing/#testing","title":"Testing","text":"<p>New features should have unit-tests written and included under the <code>tests/</code> directory to ensure the functions work as expected. The pytest framework is used for running tests along with a number of plugins (pytest-regtest for regression testing; pytest-mpl for testing generated Matplotlib images).</p>"},{"location":"contributing/#configuration","title":"Configuration","text":"<p>As described in Parameter Configuration options are primarily passed to TopoStats via a YAML configuration file. When introducing new features that require configuration options you will have to ensure that the default configuration file (<code>topostats/default.yaml</code>) is updated to include your options.</p> <p>Further the <code>topostats.validation.validate.config()</code> function, which checks a valid configuration file with all necessary fields has been passed when invoking <code>run_topostats</code>, will also need updating to include new options in the Schema against which validation of configuration files is made.</p>"},{"location":"contributing/#ide-configuration","title":"IDE Configuration","text":"<p>Linters such as <code>black</code>, <code>flake8</code> and <code>pylint</code> can be configured to work with your IDE so that say Black and/or formatting is applied on saving a file or the code is analysed with <code>pylint</code> on saving and errors reported. Setting up and configuring IDEs to work in this manner is beyond the scope of this document but some links to articles on how to do so are provided.</p> <ul> <li>Linting Python in Visual Studio Code</li> <li>Code Analysis \u2014 Spyder for <code>pylint</code> for Black see How to use   code formatter Black with Spyder.</li> <li>Code Quality Assistance Tips and Tricks, or How to Make Your Code Look Pretty? |   PyCharm</li> <li>Reformat and rearrange code | PyCharm</li> </ul>"},{"location":"data_dictionary/","title":"Data dictionary","text":""},{"location":"data_dictionary/#data-dictionary","title":"Data Dictionary","text":"<p>The resulting statistics file has the following fields.</p> Column / field / feature Description Units <code>centre_x</code> x co-ordinate of object centre. m <code>centre_y</code> y co-ordinate of object centre. m <code>radius_min</code> <code>radius_max</code> <code>radius_mean</code> <code>radius_median</code> <code>height_min</code> <code>height_max</code> <code>height_median</code> <code>height_mean</code> <code>volume</code> <code>area</code> <code>area_cartesian_bbox</code> <code>smallest_bounding_width</code> <code>smallest_bounding_length</code> <code>smallest_bounding_area</code> <code>aspect_ratio</code> <code>threshold</code> <code>max_feret</code> <code>min_feret</code> <code>Contour Lengths</code> <code>Circular</code> <code>End to End Distance</code> <code>Image Name</code> <code>Basename</code>"},{"location":"installation/","title":"Installation","text":"<p>NB - If you have trouble installing TopoStats please do checkout the discussion for possible solutions. If your problem isn't covered then please do not hesitate to ask a question.</p> <p>TopoStats is a Python package designed to run at the command line. You may have Python installed on your system but should use a Python Virtual Environment such as Miniconda and install it under the Virtual Environment. The versions of Python supported are Python &gt;=3.8 and so when creating your virtual environment you should specify this <code>3.8</code> as the minimum.</p>"},{"location":"installation/#setting-up-conda","title":"Setting up Conda","text":"<p>Once you have downloaded and installed Miniconda you can create a virtual environment for installing TopoStats with a version of Python that meets the requirements. We will call this environment <code>topostats</code> (specified with the <code>--name topostats</code> option) and to use Python 3.10 (the option <code>python=3.10</code>). After creating it we can, as per the instructions printed out, activate the environment.</p> <pre><code>conda create --name topostats python=3.10\nconda activate topostats\n</code></pre> <p>You are now ready to install TopoStats.</p> <p>NB If you are using an Apple M1 Macbook then you need to install Anaconda &gt;= 2022.05.</p>"},{"location":"installation/#installing-topostats","title":"Installing TopoStats","text":"<p>There are two options for installing TopoStats depending on your usage</p> <ol> <li>Python Package Index - appropriate if you are just using TopoStats and don't need to dig into    the code.</li> <li>Cloning the GitHub Repository - if you want to look at the code, contribute to it, debug errors or perhaps test a new    feature before a release.</li> </ol>"},{"location":"installation/#pypi-installation","title":"PyPI Installation","text":"<p>After activating your <code>topostats</code> Conda environment you can install TopoStats from PyPI using the following command.</p> <pre><code>pip install topostats\n</code></pre> <p>This will install TopoStats under your virtual environment and the command <code>run_topostats</code> will be available at the command line. You will probably want to download a copy of the <code>default_config.yaml</code> to edit and use for running your analysis. Please see the usage section for more information on running TopoStats.</p>"},{"location":"installation/#cloning-from-github","title":"Cloning from GitHub","text":"<p>NB Cloning and installing from GitHub is only required if you wish to contribute to or debug problems with TopoStats, if you only intend on using it then please install from PyPI.</p> <p>If you do not have Git already installed please see Git. If you intend to contribute to the development of TopoStats please read through the contributing section.</p> <p>If you are familiar with the command line then you can clone and install TopoStats with the following after activating your virtual environment.</p> <pre><code>git clone https://github.com/AFM-SPM/TopoStats.git\n# If you have SSH access configured to GitHub then you can use\ngit clone git@github.com:AFM-SPM/TopoStats.git\n</code></pre>"},{"location":"installation/#cloning-using-gitkraken","title":"Cloning Using GitKraken","text":"<p>If you are using GitKraken you can clone the repository by selecting \"Clone\" and then \"GitHub.com\" and typing <code>TopoStats</code> into the box next to \"Repository to Clone\" and you should be presented with the option of selecting \"TopoStats\" from the AFM-SPM organisation.</p> <p>Alternatively you can \"Clone with URL\" and enter <code>https://github.com/AFM-SPM/TopoStats.git</code> as the URL to clone from, selecting a destination to clone to.</p>"},{"location":"installation/#installing-topostats-from-the-cloned-repository","title":"Installing TopoStats from the Cloned Repository","text":"<p>Once cloned you will have to open a Terminal and navigate to the directory you cloned and after activating your virtual environment install TopoStats with the following.</p> <pre><code>cd /path/to/where/topostats/was/cloned/TopoStats\npip install .\n</code></pre> <p>If you wish to make changes to the code and test then make a <code>git branch</code>, make your changes and install in editable mode, i.e. <code>pip install -e .</code>.</p> <p>If you wish to develop features or address an existing issue please refer to the contributing section.</p>"},{"location":"installation/#tests","title":"Tests","text":"<p>One of the major changes in the refactoring is the introduction of unit tests. These require certain packages to be installed which are not installed to your virtual environment by setuptools in the above steps. If you are develop and making changes to the code base you will likely want to be able to run the tests. Install the necessary dependencies to do so with...</p> <pre><code>cd TopoStats\ngit checkout dev\npip install \".[tests]\"\npytest\n</code></pre>"},{"location":"installation/#git","title":"Git","text":"<p>Git is a version control system for managing software development and is required to be installed on your computer in order to clone the TopoStats repository. Instructions on installing Git can be found at Git Guides - install git.</p> <p>A nice Graphical User Interface for working with Git is GitKraken which includes everything you need.</p>"},{"location":"introduction/","title":"Introduction","text":"<p>TopoStats is a Python package that aims to simplify batch processing Atomic Force Microscopy (AFM) images.</p> <p>Input directories are recursively searched for files of a given type. Each image is then loaded and processed. Multiple images can be processed in parallel.</p> <p>Once an image has been loaded the specified channel of data extracted along with the pixel to nanometre scaling. This data is then aligned and the tilt is removed. Configurable thresholds are then used to generate masks and a second round of tilt removal and row alignment is performed.</p> <p>Molecules/regions of interest known as Grains are then detected based on user specified thresholds and the detected regions are labelled and have preliminary statistics calculated. The labelled regions of each grain then have individual statistics calculated capturing the height, volume, radius and the location of the centroid.</p> <p>Optionally DNA Tracing is then performed, which traces the backbone of the DNA molecules to calculate further statistics such as whether grains are linear or circular, their contour length and end-to-end distances etc.</p> <p>The resulting statistics are written to a CSV file and optionally plots are then generated from various stages of the processing as well as cropped images of each grain. The amount of images produced is also configurable.</p> <p>An schematic overview of the classes and methods that are run in processing files can be found in the workflows page along with more detailed information on installation, usage, configuration and contributing.</p> <p>If you have questions, please post them on the discussion, if you think you've encountered a bug whilst running the code or suggestions for improvements please create an issue in the GitHub project page.</p>"},{"location":"related_software/","title":"Related Software","text":"<p>TopoStats is one of many pieces of software available for working with Atomic Force Microscopy data, other packages are detailed here. If you know of another package please consider making a pull request to add it to the list.</p>"},{"location":"related_software/#python","title":"Python","text":"<ul> <li>afmformats reading common AFM file formats.</li> <li>gwyfile a pure Python interface to reading and writing Gwyddion files.</li> <li>magni compressive sampling and reconstruction of Atomic Force Microscopy images.</li> <li>nanite loading, fitting and rating AFM force-distance data.</li> <li>nanoforce import and analyse AFM force curves produced using Nanoscope 5 &amp; 6 and Nanosurf <code>.nid</code> files.</li> <li>nanoscope read data files collected using Bruker, Veeco, and Digital Instruments Atomic Force Microscopes (AFMs) using Nanoscope v5.12 - v10.00 acquisition software</li> <li>NSFOpen Read data and parameters from Nanosurf NID files.</li> <li>pycroscopy Python Package for scientific analysis of nanoscience data.</li> <li>pySPM read, handle and plot Scanning Probme Microscopy (SPM) images and ToF-SIMS data.</li> </ul>"},{"location":"related_software/#c","title":"C++","text":"<ul> <li>libasd library for reading asd files, includes Python 3 bindings.</li> </ul>"},{"location":"related_software/#other","title":"Other","text":"<ul> <li>gwyddion a modular program for Scanning Probe Microscopy (SPM) data visualisation and analysis.</li> </ul>"},{"location":"usage/","title":"Usage","text":"<p>After having installed TopoStats you are ready to run it. For convenience TopoStats provides a command line interface <code>run_topostats</code> that will load a default configuration file and process all images with reasonable default configuration options.</p> <p>However, because the location of your image files can not be known in advance you must make a copy of the default configuration and modify it to work with your files. This guide will hopefully take you through the process of running TopoStats and customising the configuration file with which it is run. If you encounter any problems please ask questions in the Discussions. If you think you have encountered a bug or have a feature suggestion please create an Issue.</p>"},{"location":"usage/#organising-scans","title":"Organising Scans","text":"<p>You should place all files you wish to batch process in a single directory. They can be nested in separate folders as TopoStats will scan for all images within this directory but currently it will only process one scan type at a time (i.e. <code>.spm</code> or <code>.jpk</code> or <code>.asd</code>). This may change in the future.</p>"},{"location":"usage/#running-topostats","title":"Running TopoStats","text":"<p>The default location that TopoStats looks for scans is the directory from which it is invoked. Once you start your shell/terminal you will therefore need to do two things.</p> <ol> <li>Navigate to the location of the scans you wish to process using <code>cd /path/to/where/scans/are/located</code>.</li> <li>Activate the virtual environment under which you installed TopoStats (refer to installed if unsure).</li> </ol> <p>You can now run topostats by invoking <code>run_topostats</code> and you should start to see some output similar to that below.</p> <pre><code>cd /path/to/where/scans/are/located\nrun_topostats\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Configuration is valid.\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Plotting configuration is valid.\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Configuration file loaded from      : None\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Scanning for images in              : /home/neil/work/projects/topostats/TopoStats\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Output directory                    : output\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Looking for images with extension   : .spm\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Images with extension .spm in /home/neil/work/projects/topostats/TopoStats : 32\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Thresholding method (Filtering)     : std_dev\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Thresholding method (Grains)        : std_dev\n...\n</code></pre>"},{"location":"usage/#configuring-topostats","title":"Configuring TopoStats","text":"<p>Configuration of TopoStats is done through a YAML file and a full description of the fields used can be found under the configuration section.</p> <p>Here we will go through common changes that you are likely to want to make to the default configuration and how to make them.</p>"},{"location":"usage/#copying-default_configyaml","title":"Copying <code>default_config.yaml</code>","text":"<p>If you have used Git to clone the TopoStats repository from GitHub the default configuration can be found in the sub-directory <code>topostats/default_config.yaml</code>. If you have installed TopoStats from PyPI then a sample configuration file can be downloaded from here (right-click on the link and select <code>Save As</code> to save the file to your computer).</p> <p>Save or copy this file to the same directory all of your scan files are located and call it <code>my_config.yaml</code>.</p> <pre><code>cp /&lt;path&gt;/&lt;to&gt;/&lt;where&gt;/&lt;topostats&gt;/&lt;is&gt;/&lt;cloned&gt;/TopoStats/topostats/default_config.yaml /&lt;where&gt;/&lt;scans&gt;/&lt;are&gt;/my_config.yaml\n</code></pre>"},{"location":"usage/#editing-my_configyaml","title":"Editing <code>my_config.yaml</code>","text":"<p>IMPORTANT This file is an ASCII text file and  you should use NotePad (Windows), TextEdit (OSX) or Nano/Emacs/Vim (GNU/Linux) or any other text editor. Do not use Microsoft Word or any other Word Processor to edit this file.</p> <p>You can now start customising the configuration you are going to run TopoStats with. All fields have defaults but the ones you may want to change are....</p> <ul> <li><code>base_dir</code> (default: <code>./</code>) the directory in which to search for scans. By default this is <code>./</code> which represents the   directory from which <code>run_topostats</code> is called and it is good practice to have one configuration file per batch of   scans that are being processed.</li> <li><code>output_dir</code> (default: <code>output</code>) the location where the output is saved, by default this is the directory <code>output</code>   which will be   created if it doesn't exist. If you wish for the output to be somewhere else specify it here. If you   want <code>Processed</code> directories to sit within the directories that images are found then simply set the <code>output_dir</code> to   the same value as <code>base_dir</code>.</li> <li><code>cores</code> (default: <code>4</code>) the number of parallel processes to run processing of all found images. Set this to a maximum   of one less than the number of cores on your computers CPU. If unsure leave as is.</li> <li><code>file_ext</code> (default: <code>.spm</code>) the file extension of scans to search for within the current directory. The default is   <code>.spm</code> but other file format support is in the pipeline.</li> <li><code>plotting</code> : <code>image_set</code> (default <code>core</code>) specifies which steps of the processing to plot images of. The value <code>all</code>   gets images for all stages, `core** saves only a subset of images.</li> </ul> <p>Most of the other configuration options can be left on their default values for now. Once you have made any changes save the file and return to your terminal.</p>"},{"location":"usage/#running-topostats-with-my_configyaml","title":"Running TopoStats with <code>my_config.yaml</code>","text":"<p>To use your new configuration file you need to inform <code>run_topostats</code> to use that file rather than the defaults, this is done using the <code>--config my_config.yaml</code> file.</p> <p>NB this assumes that you are in the same directory as your scans where you have saved the <code>my_config.yaml</code> file that you edited. That doesn't have to be the case but it makes life easier for if you are not familiar with absolute and relative paths.</p> <pre><code>run_topostats --config my_configy.yaml\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Configuration is valid.\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Plotting configuration is valid.\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Configuration file loaded from      : None\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Scanning for images in              : /home/neil/work/projects/topostats/TopoStats\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Output directory                    : output\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Looking for images with extension   : .spm\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Images with extension .spm in /home/neil/work/projects/topostats/TopoStats : 1\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Thresholding method (Filtering)     : std_dev\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Thresholding method (Grains)        : std_dev\n...\n</code></pre> <p>On a successful completion you should see output similar to this at the bottom.</p> <pre><code>Processing images from tests, results are under output: 100%|XXXXXXXXXXXXXXXX| 1/1 [00:03&lt;00:00,  3.60s/it][Tue, 15 Nov 2022 13:49:14] [INFO    ] [topostats] All statistics combined for 1 images(s) are saved to : output/all_statistics.csv}\n[Tue, 15 Nov 2022 13:49:14] [INFO    ] [topostats] Unable to generate folderwise statistics as 'all_statis_df' is empty\n[Tue, 15 Nov 2022 13:49:14] [INFO    ] [topostats] Writing configuration to : output/config.yaml\n</code></pre>"},{"location":"usage/#output","title":"Output","text":"<p>The output from running TopoStats is saved in the location defined in the configuration file by <code>output_dir</code>. The default is the directory <code>output</code> within the directory from which <code>run_topostats</code> is invoked unless it has been modified in a copy of the default configuration as described above.</p> <p>At the top level of the output directory are two files <code>config.yaml</code> and <code>all_statistics.csv</code></p> <ul> <li><code>config.yaml</code> : a copy of the configuration used to process the images.</li> <li><code>all_statistics.csv</code> : a Comma Separated Variable ASCII plain-text file of the grain and DNA tracing statistics.</li> </ul> <p>The remaining directories of results is contingent on the structure of files within the <code>base_dir</code> that is specified in the configuration. If all files are in the top-level directory (i.e. no nesting) then you will have just a <code>Processed</code> directory. If there is a nested structure then there will be a <code>Processed</code> directory in each folder that an image with the specified <code>file_ext</code> has been found. This is perhaps best illustrated by way of example.</p> <p>If you have the following three <code>.spm</code> files within your current directory, one at the top level, one under <code>level1</code> and one under <code>level1/a</code>...</p> <pre><code>[4.0K Nov 15 13:55]  .\n|-- [4.0K Nov 15 13:54]  ./level1\n|   |-- [4.0K Nov 15 13:54]  ./level1/a\n|   |-- [ 32M Nov 15 13:54]  ./level1/a/minicircle.spm\n|   |-- [ 32M Nov 15 13:54]  ./level1/minicircle.spm\n|-- [ 32M Nov 15 13:54]  ./minicircle.spm\n</code></pre> <p>...then under <code>output</code> (the default for<code>output_dir</code>) you will see the following directory structure.</p> <pre><code>[4.0K Nov 15 14:06]  output\n|-- [ 381 Nov 15 14:06]  output/all_statistics.csv\n|-- [7.4K Nov 15 14:06]  output/config.yaml\n|-- [4.0K Nov 15 14:06]  output/level1\n|   |-- [4.0K Nov 15 14:06]  output/level1/a\n|   |   |-- [4.0K Nov 15 14:06]  output/level1/a/Processed\n|   |-- [4.0K Nov 15 14:06]  output/level1/Processed\n|-- [4.0K Nov 15 14:06]  output/Processed\n</code></pre> <p>...where there is one <code>Processed</code> directory at the sub-directory level that each image was found.</p> <p>NB If you want <code>Processed</code> directories to sit within the directories that images are found then simply set the <code>output_dir</code> to the same value as <code>base_dir</code>.</p> <p>Within each <code>Processed</code> directory is a directory for each file found with the specified <code>file_ext</code> and within these are the resulting images from processing scans. If the <code>plotting</code> : <code>image_set</code> is <code>core</code> then there is a single image for each. If this option is <code>all</code> then there is also a sub-directory for each image found within which there are the directories <code>filters</code>, <code>grains/lower</code> and <code>grains/upper</code> which contain additional images from the processing stages and an accompanying histogram for each image showing the distribution of pixel heights for that image.</p>"},{"location":"workflow/","title":"Workflow","text":"<p>This section gives a broad overview of the steps taken in processing images.</p>"},{"location":"workflow/#topotracing-processing-a-single-spm-file","title":"Topotracing : Processing a single <code>.spm</code> file.","text":"<p>Topotracing loads images from <code>.spm</code> files and extracts the specified channel, performing various filtering stages (<code>Filters()</code> class) before finding grains (<code>Grains()</code> class) and then calculating statistics for each grain (<code>GrainStats()</code> class). The Gaussian filtered image and labelling of grains is then passed onto DNA Tracing.</p> <pre><code>%%{init: {'theme': 'base',\n         }\n}%%\ngraph TD;\n\n  subgraph Background Flattening\n  A1([Load YAML Configuration]) --&gt; A2([Load SPM])\n  A2 --&gt; A3([Extract channel from SPM])\n  A3 --&gt; A4([Initial Align])\n  A4 --&gt; A5([Initial Tilt Removal])\n  A5 --&gt; A6([Thresholding Otsu])\n  A6 --&gt; A7([Mask Generation])\n  A7 --&gt; A8([Masked Align])\n  A8 --&gt; A9([Masked Tilt Removal])\n  A9 --&gt; A10([Background Zeroing])\n  end\n  subgraph Grain Finding\n  A10 --&gt; B1([Lower Thresholding])\n  B1 --&gt; B2([Guassian Filtering])\n  B2 --&gt; B3([Tidy Edges])\n  B3 --&gt; B4([Preliminary Statistics])\n  B4 --&gt; B5([Size Thresholding])\n  B5 --&gt; B6([Label Regions])\n  end\n  subgraph Grain Statistics\n  B6 --&gt; C2([Calculate Points])\n  C2 --&gt; C8([Height &amp; Volume Statistics])\n  C2 --&gt; C3([Calculate Edges])\n  C2 --&gt; C4([Calculate Centroid])\n  C3 --&gt; C5([Calculate Radius Statistics])\n  C3 --&gt; C6([Convex Hull / Graham Scan])\n  C6 --&gt; C7([Minimum Bounding Box Statistics])\n  end\n  subgraph DNA Tracing\n  B2 --&gt; D1([More Analysis])\n  B5 --&gt; D1\n  end\n  style A1 fill:#648FFF,stroke:#000000\n  style A2 fill:#648FFF,stroke:#000000\n  style A3 fill:#648FFF,stroke:#000000\n  style A4 fill:#648FFF,stroke:#000000\n  style A5 fill:#648FFF,stroke:#000000\n  style A6 fill:#648FFF,stroke:#000000\n  style A7 fill:#648FFF,stroke:#000000\n  style A8 fill:#648FFF,stroke:#000000\n  style A9 fill:#648FFF,stroke:#000000\n  style A10 fill:#648FFF,stroke:#000000\n  style B1 fill:#DC267F,stroke:#000000\n  style B2 fill:#DC267F,stroke:#000000\n  style B3 fill:#DC267F,stroke:#000000\n  style B4 fill:#DC267F,stroke:#000000\n  style B5 fill:#DC267F,stroke:#000000\n  style B6 fill:#DC267F,stroke:#000000\n  style C2 fill:#FE6100,stroke:#000000\n  style C3 fill:#FE6100,stroke:#000000\n  style C4 fill:#FE6100,stroke:#000000\n  style C5 fill:#FE6100,stroke:#000000\n  style C6 fill:#FE6100,stroke:#000000\n  style C7 fill:#FE6100,stroke:#000000\n  style C8 fill:#FE6100,stroke:#000000\n  style D1 fill:#785EF0,stroke:#000000\n</code></pre>"},{"location":"workflow/#dna-tracing-processing-a-single-grain","title":"DNA Tracing : Processing a single grain","text":"<pre><code>%%{init: {'theme': 'base',\n         }\n}%%\ngraph TD;\n\n  subgraph dnaTrace\n  A1([\"get_numpy_arrays() | Load Numpy arrays to dictionary indexed by number\"]) --&gt; A2([\"skimage.filters.gaussian() | Filter full image\"])\n  A2 -- For each image --&gt; A3([\"get_disordered_trace() | extracts mask\"])\n  A3 --&gt; A4([\"purge_obvious_crap() | Removes ites if len() &lt; 10 (i.e. small objects) \"])\n  A4 --&gt; A5([\"linear_or_circular on unordered traces() | linear or circular molecule\"])\n  A5 --&gt; A6([\"get_ordered_traces() | Reorders points in the array?\"])\n  A6 --&gt; A7([\"linear_or_circular() on ordered traces\"])\n  A7 --&gt; A8([\"get_fitted_traces()\"])\n  A8 --&gt; A9([\"get_splined_traces()\"])\n  A9 --&gt; A10([\"measure_contour_length()\"])\n  A10 --&gt; A11([\"measure_end_to_end_distance()\"])\n  A11 --&gt; A12([\"report_basic_stats()\"])\n  end\n\n  subgraph \"get_disordered_trace()\"\n  A3 --&gt; B1([\"ndimage.binary_dilation() | extracts mask\"])\n  B1 --&gt; B2([\"scipy.ndimage.gaussian_filter molecule()\"])\n  B2 --&gt; B3([\"getSkeleton()\"])\n  B3 --&gt; A4\n  end\n\n  subgraph \"getSkeleton()\"\n  B3 --&gt; C1([\"Skeletonize | to be replaced by get_skeleton()\"])\n  C1 --&gt; B3\n  end\n\n  style A1 fill:#648FFF,stroke:#000000\n  style A2 fill:#648FFF,stroke:#000000\n  style A3 fill:#648FFF,stroke:#000000\n  style A4 fill:#648FFF,stroke:#000000\n  style A5 fill:#648FFF,stroke:#000000\n  style A6 fill:#648FFF,stroke:#000000\n  style A7 fill:#648FFF,stroke:#000000\n  style A8 fill:#648FFF,stroke:#000000\n  style A9 fill:#648FFF,stroke:#000000\n  style A10 fill:#648FFF,stroke:#000000\n  style A11 fill:#648FFF,stroke:#000000\n  style A12 fill:#648FFF,stroke:#000000\n  style B1 fill:#DC267F,stroke:#000000\n  style B2 fill:#DC267F,stroke:#000000\n  style B3 fill:#DC267F,stroke:#000000\n  style C1 fill:#FE6100,stroke:#000000\n</code></pre>"}]}