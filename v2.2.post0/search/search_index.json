{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to TopoStats's documentation","text":"<p>TopoStats batch processes scans from Atomic Force Microscopy. Please read through the various documentation sections listed below to find out more.</p> <ul> <li>Introduction</li> <li>Installation</li> <li>Usage</li> <li>Configuration</li> <li>Notebooks</li> <li>Workflow</li> <li>Data Dictionary</li> <li>Advanced</li> <li>Contributing</li> <li>Related Software</li> <li>Glossary</li> <li>FAQ</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>Configuration for TopoStats is done using a YAML configuration file that is specified on the command line when invoking. If no configuration file is provided this default configuration is loaded automatically and used.</p> <p>The current configuration file is provided in the TopoStats repository at <code>topostats/default_config.yaml</code> but please be aware this may not work with your installed version, particularly if you installed from PyPI.</p>"},{"location":"configuration/#generating-a-configuration","title":"Generating a configuration","text":"<p>You can always generate a configuration file appropriate for the version you have installed (bar v2.0.0 as this option was added afterwards). This writes the default configuration to the specified filename (i.e. it does not have to be called <code>config.yaml</code> it could be called <code>spm-2023-02-20.yaml</code>). There are a few options available (use <code>topostats create-config --help</code> for further details).</p> <pre><code>topostats create-config\n</code></pre>"},{"location":"configuration/#using-a-custom-configuration","title":"Using a custom configuration","text":"<p>If you have generated a configuration file you can modify and edit a configuration it to change the parameters (see fields below). Once these changes have been saved, you can run TopoStats with this configuration file as shown below.</p> <pre><code>topostats process --config my_config.yaml\n</code></pre> <p>On completion a copy of the configuration that was used is written to the output directory so you have a record of the parameters used to generate the results you have. This file can be used in subsequent runs of TopoStats.</p>"},{"location":"configuration/#yaml-structure","title":"YAML Structure","text":"<p>YAML files have key and value pairs, the first word, e.g. <code>base_dir</code> is the key this is followed by a colon to separate it from the value that it takes, by default <code>base_dir</code> takes the value <code>./</code> (which means the current directory) and so the entry in the file is a single line with <code>base_dir: ./</code>. Other data structures are available in YAML files including nested values and lists.</p> <p>A list in YAML consists of a key (e.g. <code>above:</code>) followed by the values in square brackets separated by commas such as <code>above: [ 500, 800 ]</code>. This means the <code>above</code> key is a list of the values <code>500</code> and <code>800</code>. Long lists can be split over separate lines as shown below</p> <pre><code>above:\n  - 100\n  - 200\n  - 300\n  - 400\n</code></pre>"},{"location":"configuration/#fields","title":"Fields","text":"<p>Aside from the comments in YAML file itself the fields are described below.</p> Section Sub-Section Data Type Default Description <code>base_dir</code> string <code>./</code> Directory to recursively search for files within.[^1] <code>output_dir</code> string <code>./output</code> Directory that output should be saved to.[^1] <code>log_level</code> string <code>info</code> Verbosity of logging, options are (in increasing order) <code>warning</code>, <code>error</code>, <code>info</code>, <code>debug</code>. <code>cores</code> integer <code>2</code> Number of cores to run parallel processes on. <code>file_ext</code> string <code>.spm</code> File extensions to search for. <code>loading</code> <code>channel</code> string <code>Height</code> The channel of data to be processed, what this is will depend on the file-format you are processing and the channel you wish to process. <code>filter</code> <code>run</code> boolean <code>true</code> Whether to run the filtering stage, without this other stages won't run so leave as <code>true</code>. <code>threshold_method</code> str <code>std_dev</code> Threshold method for filtering, options are <code>ostu</code>, <code>std_dev</code> or <code>absolute</code>. <code>otsu_threshold_multiplier</code> float <code>1.0</code> Factor by which the derived Otsu Threshold should be scaled. <code>threshold_std_dev</code> dictionary <code>10.0, 1.0</code> A pair of values that scale the standard deviation, after scaling the standard deviation <code>below</code> is subtracted from the image mean to give the below/lower threshold and the <code>above</code> is added to the image mean to give the above/upper threshold. These values should always be positive. <code>threshold_absolute</code> dictionary <code>-1.0, 1.0</code> Below (first) and above (second) absolute threshold for separating data from the image background. <code>gaussian_size</code> float <code>0.5</code> The number of standard deviations to build the Gaussian kernel and thus affects the degree of blurring. See skimage.filters.gaussian and <code>sigma</code> for more information. <code>gaussian_mode</code> string <code>nearest</code> <code>grains</code> <code>run</code> boolean <code>true</code> Whether to run grain finding. Options <code>true</code>, <code>false</code> <code>row_alignment_quantile</code> float <code>0.5</code> Quantile (0.0 to 1.0) to be used to determine the average background for the image. below values may improve flattening of large features. <code>smallest_grain_size_nm2</code> int <code>100</code> The smallest size of grains to be included (in nm^2), anything smaller than this is considered noise and removed. NB must be <code>&gt; 0.0</code>. <code>threshold_method</code> float <code>std_dev</code> Threshold method for grain finding. Options : <code>otsu</code>, <code>std_dev</code>, <code>absolute</code> <code>otsu_threshold_multiplier</code> <code>1.0</code> Factor by which the derived Otsu Threshold should be scaled. <code>threshold_std_dev</code> dictionary <code>10.0, 1.0</code> A pair of values that scale the standard deviation, after scaling the standard deviation <code>below</code> is subtracted from the image mean to give the below/lower threshold and the <code>above</code> is added to the image mean to give the above/upper threshold. These values should always be positive. <code>threshold_absolute</code> dictionary <code>-1.0, 1.0</code> Below (first), above (second) absolute threshold for separating grains from the image background. <code>direction</code> <code>above</code> Defines whether to look for grains above or below thresholds or both. Options: <code>above</code>, <code>below</code>, <code>both</code> <code>smallest_grain_size</code> int <code>50</code> Catch-all value for the minimum size of grains. Measured in nanometres squared. All grains with area below than this value are removed. <code>absolute_area_threshold</code> dictionary <code>[300, 3000], [null, null]</code> Area thresholds for above the image background (first) and below the image background (second), which grain sizes are permitted, measured in nanometres squared. All grains outside this area range are removed. <code>remove_edge_intersecting_grains</code> boolean <code>true</code> Whether to remove grains that intersect the image border. Do not change this unless you know what you are doing. This will ruin any statistics relating to grain size, shape and DNA traces. <code>grainstats</code> <code>run</code> boolean <code>true</code> Whether to calculate grain statistics. Options : <code>true</code>, <code>false</code> <code>cropped_size</code> float <code>40.0</code> Force cropping of grains to this length (in nm) of square cropped images (can take <code>-1</code> for grain-sized box) <code>edge_detection_method</code> str <code>binary_erosion</code> Type of edge detection method to use when determining the edges of grain masks before calculating statistics on them. Options : <code>binary_erosion</code>, <code>canny</code>. <code>dnatracing</code> <code>run</code> boolean <code>true</code> Whether to run DNA Tracing. Options : true, false <code>min_skeleton_size</code> int <code>10</code> The minimum number of pixels a skeleton should be for statistics to be calculated on it. Anything smaller than this is dropped but grain statistics are retained. <code>skeletonisation_method</code> str <code>topostats</code> Skeletonisation method to use, possible options are <code>zhang</code>, <code>lee</code>, <code>thin</code> (from Scikit-image Morphology module) or the original bespoke TopoStas method <code>topostats</code>. <code>spline_step_size</code> float <code>7.0e-9</code> The sampling rate of the spline in metres. This is the frequency at which points are sampled from fitted traces to act as guide points for the splining process using scipy's splprep. <code>spline_linear_smoothing</code> float <code>5.0</code> The amount of smoothing to apply to splines of linear molecule traces. <code>spline_circular_smoothing</code> float <code>0.0</code> The amount of smoothing to apply to splines of circular molecule traces. <code>pad_width</code> int 10 Padding for individual grains when tracing. This is sometimes required if the bounding box around grains is too tight and they touch the edge of the image. <code>cores</code> int 1 Number of cores to use for tracing. NB Currently this is NOT used and should be left commented in the YAML file. <code>plotting</code> <code>run</code> boolean <code>true</code> Whether to run plotting. Options : <code>true</code>, <code>false</code> <code>style</code> str <code>topostats.mplstyle</code> The default loads a custom matplotlibrc param file that comes with TopoStats. Users can specify the path to their own style file as an alternative. <code>save_format</code> string <code>null</code> Format to save images in, <code>null</code> defaults to <code>png</code> see matplotlib.pyplot.savefig <code>savefig_dpi</code> string / float <code>null</code> Dots Per Inch (DPI), if <code>null</code> then the value <code>figure</code> is used, for other values (typically integers) see [#further-customisation] and Matplotlib. Low DPI's improve processing time but can reduce the plotted trace (but not the actual trace) accuracy. <code>pixel_interpolation</code> string <code>null</code> Interpolation method for image plots. Recommended default 'null' prevents banding that occurs in some images. If interpolation is needed, we recommend <code>gaussian</code>. See matplotlib imshow interpolations documentation for details. <code>image_set</code> string <code>all</code> Which images to plot. Options : <code>all</code>, <code>core</code> (flattened image, grain mask overlay and trace overlay only). <code>zrange</code> list <code>[0, 3]</code> Low (first number) and high (second number) height range for core images (can take [null, null]). NB <code>low &lt;= high</code> otherwise you will see a <code>ValueError: minvalue must be less than or equal to maxvalue</code> error. <code>colorbar</code> boolean <code>true</code> Whether to include the colorbar scale in plots. Options <code>true</code>, <code>false</code> <code>axes</code> boolean <code>true</code> Whether to include the axes in the produced plots. <code>num_ticks</code> null / int <code>null</code> Number of ticks to have along the x and y axes. Options : <code>null</code> (auto) or an integer &gt;1 <code>cmap</code> string <code>null</code> Colormap/colourmap to use (defaults to 'nanoscope' if null (defined in <code>topostats/topostats.mplstyle</code>). Other options are 'afmhot', 'viridis' etc., see Matplotlib : Choosing Colormaps. <code>mask_cmap</code> string <code>blu</code> Color used when masking regions. Options <code>blu</code>, <code>jet_r</code> or any valid Matplotlib colour. <code>histogram_log_axis</code> boolean <code>false</code> Whether to plot hisograms using a logarithmic scale or not. Options: <code>true</code>, <code>false</code>. <code>summary_stats</code> <code>run</code> boolean <code>true</code> Whether to generate summary statistical plots of the distribution of different metrics grouped by the image that has been processed. <code>config</code> str <code>null</code> Path to a summary config YAML file that configures/controls how plotting is done. If one is not specified either the command line argument <code>--summary_config</code> value will be used or if that option is not invoked the default <code>topostats/summary_config.yaml</code> will be used."},{"location":"configuration/#summary-configuration","title":"Summary Configuration","text":"<p>Plots summarising the distribution of metrics are generated by default. The behaviour is controlled by a configuration file. The default example can be found in <code>topostats/summary_config.yaml</code>. The fields of this file are described below.</p> Section Sub-Section Data Type Default Description <code>output_dir</code> <code>str</code> <code>./output/</code> Where output plots should be saved to. <code>csv_file</code> <code>str</code> <code>null</code> Where the results file should be loaded when running <code>toposum</code> <code>file_ext</code> <code>str</code> <code>png</code> File type to save images as. <code>pickle_plots</code> <code>bool</code> True Whether to save images to a Python pickle. <code>var_to_label</code> <code>str</code> <code>null</code> Optional YAML file that maps variable names to labels, uses <code>topostats/var_to_label.yaml</code> if null. <code>molecule_id</code> <code>str</code> <code>molecule_number</code> Variable containing the molecule number. <code>image_id</code> <code>str</code> <code>image</code> Variable containing the image identifier. <code>hist</code> <code>bool</code> <code>True</code> Whether to plot a histogram of statistics. <code>bins</code> <code>int</code> <code>20</code> Number of bins to plot in histogram. <code>stat</code> <code>str</code> <code>count</code> What metric to plot on histogram valid values are <code>count</code> (default), <code>frequency</code>, <code>probability</code>, <code>percent</code>, <code>density</code> <code>kde</code> <code>bool</code> <code>True</code> Whether to include a Kernel Density Estimate on histograms. NB if both <code>hist</code> and <code>kde</code> are true they are overlaid. <code>violin</code> <code>bool</code> <code>True</code> Whether to generate Violin Plots. <code>figsize</code> <code>list</code> <code>[16, 9]</code> Figure size (x then y dimensions). <code>alpha</code> <code>float</code> <code>0.5</code> Level of transparency to use when plotting. <code>palette</code> <code>str</code> <code>bright</code> Seaborn color palette. Options <code>colorblind</code>, <code>deep</code>, <code>muted</code>, <code>pastel</code>, <code>bright</code>, <code>dark</code>, <code>Spectral</code>, <code>Set2</code> <code>stats_to_sum</code> <code>list</code> <code>str</code> A list of strings of variables to plot, comment (placing a <code>#</code> at the start of the line) and uncomment as required. Possible values are <code>area</code>, <code>area_cartesian_bbox</code>, <code>aspect_ratio</code>, <code>banding_angle</code>, <code>contour_length</code>, <code>end_to_end_distance</code>, <code>height_max</code>, <code>height_mean</code>, <code>height_median</code>, <code>height_min</code>, <code>radius_max</code>, <code>radius_mean</code>, <code>radius_median</code>, <code>radius_min</code>, <code>smallest_bounding_area</code>, <code>smallest_bounding_length</code>, <code>smallest_bounding_width</code>, <code>volume</code>"},{"location":"configuration/#validation","title":"Validation","text":"<p>Configuration files are validated against a schema to check that the values in the configuration file are within the expected ranges or valid parameters. This helps capture problems early and should provide informative messages as to what needs correcting if there are errors.</p>"},{"location":"configuration/#matplotlib-style","title":"Matplotlib Style","text":"<p>TopoStats generates a number of images of the scans at various steps in the processing. These are plotted using the Python library Matplotlib. A custom <code>matplotlibrc</code> file is included in TopoStats which defines the default parameters for generating images. This covers all aspects of a plot that can be customised, for example we define custom colour maps <code>nanoscope</code> and <code>afmhot</code>. By default the former is configured to be used. Other parameters that are customised are the <code>font.size</code> which affects axis labels and titles.</p> <p>If you wish to modify the look of all images that are output you can generate a copy of the default configuration using <code>topostats create-matplotlibrc</code> command which will write the output to <code>topostats.mplstyle</code> by default (NB there are flags which allow you to specify the location and filename to write to, see <code>topostats create-matplotlibrc --help</code> for further details).</p> <p>You should read and understand this commented file in detail. Once changes have been made you can run TopoStats using this custom file using the following command (substituting <code>my_custom_topostats.mplstyle</code> for whatever you have saved your file as).</p> <pre><code>topostats process --matplotlibrc my_custom_topostats.mplstyle\n</code></pre> <p>NB Plotting with Matplotlib is highly configurable and there are a plethora of options that you may wish to tweak. Before delving into customising <code>matplotlibrc</code> files it is recommended that you develop and build the style of plot you wish to generate using Jupyter Notebooks and then translate them to the configuration file. Detailing all of the possible options is beyond the scope of TopoStats but the Matplotlib documentation is comprehensive and there are some sample Jupyter Notebooks (see <code>notebooks/03-plotting-scans.ipynb</code>) that guide you through the basics.</p>"},{"location":"configuration/#further-customisation","title":"Further customisation","text":"<p>Whilst the overall look of images is controlled in this manner there is one additional file that controls how images are plotted in terms of filenames, titles and image types and whether an image is part of the <code>core</code> subset (flattened image, grain mask overlay and trace overlay) that are always generated or not.</p> <p>This is the <code>topostats/plotting_dictionary.yaml</code> which for each image stage defines whether it is a component of the <code>core</code> subset of images that are always generated, sets the <code>filename</code>, the <code>title</code> on the plot, the <code>image_type</code> (whether it is a binary image), the <code>savefig_dpi</code> which controls the Dots Per Inch (essentially the resolution). Each image has the following structure.</p> <pre><code>z_threshed:\n  title: \"Height Thresholded\"\n  image_type: \"non-binary\"\n  savefig_dpi: 100\n  core_set: true\n</code></pre> <p>Whilst it is possible to edit this file it is not recommended to do so.</p> <p>The following section describes how to override the DPI settings defined in this file and change the global <code>cmap</code> (colormap/colourmap) used in plotting and output format.</p>"},{"location":"configuration/#dpi","title":"DPI","text":"<p>During development it was found that setting high DPI globally for all images had a detrimental impact on processing speeds, slowing down the overall processing time. The solution we have implemented is to use the <code>topostats/plotting_dictionary.yaml</code> file and set the <code>savefig_dpi</code> parameter on a per-image basis.</p> <p>If you wish to change the DPI there are two options, you can change the value for all images by modifying the setting in your a custom configuration by modifying the <code>savefig_dpi</code> from <code>null</code> to your desired value. The example below shows a section of the configuration file you can generate and setting this value to <code>400</code>.</p> <pre><code>plotting:\n  run: true # Options : true, false\n  style: topostats.mplstyle # Options : topostats.mplstyle or path to a matplotlibrc params file\n  savefig_format: null # Options : null (defaults to png) or see https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html\n  savefig_dpi: 400 # Options : null (defaults to format) see https://afm-spm.github.io/TopoStats/main/configuration.html#further-customisation and https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html\n</code></pre> <p>The value in the configuration file (or the default if none is specified) can also be configured at run-time using the <code>--savefig-dpi ###</code> option to the <code>topostats process</code>. This will over-ride both the default or any value specified in a custom configuration you may have set. The following sets this to <code>400</code></p> <pre><code>topostats process --savefig-dpi 400\n</code></pre> <p>NB Changing the DPI in this manner will apply to all images and may significantly reduce processing speed as it takes longer to write images with high DPI to disk.</p> <p>If you wish to have fine grained control over the DPI on a per-image basis when batch processing then your only recourse is to change the values in <code>topostats/plotting_dictionary.yaml</code>. Where this is depends on how you have installed TopoStats, if it is from a clone of the Git repository then it can be found in <code>TopoStats/topostats/plotting_dictionary.yaml</code>. If you have installed from PyPI using <code>pip install topostats</code> then it will be under the virtual environment you have created e.g. <code>~/.virtualenvs/topostats/lib/python3.11/site-packages/topostats/topostats/plotting_dictionary.yaml</code> if you are using plain virtual environments or <code>~/miniconda3/envs/topostats/lib/python3.11/site-packages/topostats/topostats/plotting_dictionary.yaml</code> if you are using Conda environments and chose <code>~/miniconda3</code> as the base directory when installing Conda.</p> <p>If you have installed TopoStats from the cloned Git repository the file will be under <code>TopoStats/topostats/plotting_dictionary.yaml</code>.</p> <p>NB The exact location will be highly specific to your system so the above are just guides as to where to find things.</p>"},{"location":"configuration/#colormap","title":"Colormap","text":"<p>The colormap used to plot images is set globally in <code>topostats/default_config.yaml</code>. TopoStats includes two custom colormaps <code>nanoscope</code> and <code>afmhot</code> but any colormap recognised by Matplotlib can be used (see the Matplotlib Colormap reference for choices).</p> <p>If you want to modify the colormap that is used you have two options. Firstly you can generate a configuration file and modify the field <code>cmap</code> to your choice. The example below shows changing this from <code>null</code> (which defaults to <code>nanoscope</code> as defined in <code>topostats.mplstyle</code>) to <code>rainbow</code>.</p> <pre><code>plotting:\n  ...\n  cmap: rainbow # Colormap/colourmap to use (default is 'nanoscope' which is used if null, other options are 'afmhot', 'viridis' etc.)\n</code></pre> <p>Alternatively it is possible to specify the colormap that is used on the command line using the <code>--cmap</code> option to <code>topostats process</code>. This will over-ride both the default or any value specified in a custom configuration you may have set. The following sets this to <code>rainbow</code>.</p> <pre><code>topostats process --cmap rainbow\n</code></pre>"},{"location":"configuration/#saved-image-format","title":"Saved Image format","text":"<p>Matplotlib, and by extension TopoStats, supports saving images in a range of different formats including <code>png</code> (Portable Network Graphic), <code>svg</code> (Scalable Vector Graphics) and <code>pdf</code> (Portable Document Format). The default is <code>png</code> but, as with both DPI and Colormap, these can be easily changed via a custom configuration file or command line options to change these without having to edit the Matplotlib Style file.</p> <p>If you want to modify the output file format that is used you have two options. Firstly you can generate a configuration file and modify the field <code>savefig_format</code> to your choice. The example below shows changing this from <code>null</code> (which defaults to <code>png</code> as defined in <code>topostats.mplstyle</code>) to <code>svg</code>.</p> <pre><code>plotting:\n  ...\n  savefig_format: svg # Options : null (defaults to png) or see https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html\n</code></pre> <p>Alternatively it is possible to specify the output image format that is used on the command line using the <code>--savefig-format</code> option to <code>topostats process</code>. This will over-ride both the default or any value specified in a custom configuration you may have set. The following sets this to <code>svg</code>.</p> <pre><code>topostats process --savefig-format svg\n</code></pre> <p>NB Note that these options are not mutually exclusive and can therefore be combined along with any of the other options available to <code>topostats process</code>. The following would use a DPI of <code>400</code>, set the colormap to <code>rainbow</code> and the output format to <code>svg</code> when running Topostats and would over-ride options in any custom configuration file or matplotlib style file.</p> <pre><code>topostats process --savefig-dpi 400 --cmap rainbow --savefig-format svg\n</code></pre> <p>[^1] When writing file paths you can use absolute or relative paths. On Windows systems absolute paths start with the drive letter (e.g. <code>c:/</code>) on Linux and OSX systems they start with <code>/</code>. Relative paths are started either with a <code>./</code> which denotes the current directory or one or more <code>../</code> which means the higher level directory from the current directory. You can always find the current directory you are in using the <code>pwd</code> (<code>p</code>rint <code>w</code>orking <code>d</code>irectory). If your work is in <code>/home/user/path/to/my/data</code> and <code>pwd</code> prints <code>/home/user</code> then the relative path to your data is <code>./path/to/my/data</code>. The <code>cd</code> command is used to <code>c</code>hange <code>d</code>irectory.</p> <pre><code>pwd\n/home/user/\n# Two ways of changing directory using a relative path\ncd ./path/to/my/data\npwd\n/home/user/path/to/my/data\n# Using an absolute path\ncd /home/user/path/to/my/data\npwd\n/home/user/path/to/my/data\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>This document describes how to contribute to the development of this software.</p>"},{"location":"contributing/#contribution-workflow","title":"Contribution Workflow","text":""},{"location":"contributing/#create-an-issue","title":"Create an Issue","text":"<p>Before starting please search for and review the existing issues (both <code>open</code> and <code>closed</code>) and pull requests to see if anyone has reported the bug or requested the feature already or work is in progress. If nothing exists then you should create a new issue using one of the templates provided.</p>"},{"location":"contributing/#cloning-the-repository","title":"Cloning the repository","text":"<p>If you wish to make changes yourself you will have to fork the repository to your own account and then clone that if you are not a member of AFM-SPM Organisation. If you are a member then you can clone the repository and make contributions directly.</p> <pre><code># Member of AFM-SPM Organisation\ngit clone git@github.com:AFM-SPM/TopoStats.git\n# Non-member of AFM-SPM cloning fork\ngit clone git@github.com:&lt;YOUR_GITHUB_USERNAME&gt;/TopoStats.git\n</code></pre>"},{"location":"contributing/#install-additional-dependencies","title":"Install Additional Dependencies","text":"<p>If you are going to contribute you should install the additional dependencies for undertaking such work. There are three groups of additional dependencies, <code>dev</code>, <code>docs</code> and <code>tests</code> and you should install all three using <code>pip</code> as shown below.</p> <pre><code>cd TopoStats\npip install \".[dev,docs,tests]\"\n</code></pre>"},{"location":"contributing/#creating-a-branch","title":"Creating a branch","text":"<p>Typically you will now create a branch to work on the issue you wish to address. It is not compulsory but we try to use a consistent nomenclature for branches that shows who has worked on the branch, the issue it pertains to and a short description of the work. To which end you will see branches with the form <code>&lt;GITHUB_USERNAME&gt;/&lt;GITHUB_ISSUE&gt;-&lt;DESCRIPTION&gt;</code>. Some examples are shown below...</p> Branch User Issue Description <code>ns-rse/259-contributing</code> <code>ns-rse</code> 259 <code>contributing</code> short for the issue subject Add contributing section to documentation. <code>SylviaWhittle/204-nanometre-scaling</code> <code>SylviaWhittle</code> 204 <code>nanometre-scaling</code> short for the issue subject Colour scale in nanometers not pixels. <p>How you create a branch depends on how you use Git, some use the integration provided by their IDE, others dedicated clients such as GitKraken and some may use the command line interface. These instructions use the later but you are of course free to use your chosen method of managing Git and GitHub.</p> <p>In this example we branch from <code>dev</code> and create a new branch called <code>ns-rse/000-fix-an-issue</code>.</p> <pre><code># Ensure you are up-to-date on the main branch\ngit checkout main\ngit pull\n# Create and checkout a branch in one step\ngit checkout -b ns-rse/000-fix-an-issue\n# Create and checkout a branch in two steps\ngit branch dev ns-rse/000-fix-an-issue\ngit checkout ns-rse/000-fix-an-issue\n</code></pre> <p>You can now start working on your issue and making regular commits, but please bear in mind the following section on Coding Standards.</p>"},{"location":"contributing/#coding-standards","title":"Coding Standards","text":"<p>To make the codebase easier to maintain we ask that you follow the guidelines below on coding style, linting, typing, documentation and testing.</p>"},{"location":"contributing/#coding-stylelinting","title":"Coding Style/Linting","text":"<p>Using a consistent coding style has many benefits (see Linting : What is all the fluff about?). For this project we aim to adhere to PEP8 - the style Guide for Python Code and do so using the formatting linters black and ruff. Ruff implements the checks made by Flake8, isort and pydocstyle and has some overlap with both Black and Pylint.</p> <p>We also like to ensure the code passes pylint which helps identify code duplication and reduces some of the code smells that we are all prone to making. A <code>.pylintrc</code> is included in the repository. Currently this isn't strictly applied but it is planned for part of the CI/CD pipeline and so we would be grateful if you could lint your code before making Pull Requests.</p> <p>Many popular IDEs such as VSCode, PyCharm, Spyder and Emacs all have support for integrating these linters into your workflow such that when you save a file the linting/formatting is automatically applied.</p>"},{"location":"contributing/#pre-commit","title":"Pre-commit","text":"<p>pre-commit is a powerful and useful tool that runs hooks on your code prior to making commits. For a more detailed exposition see pre-commit : Protecting your future self.</p> <p>The repository includes <code>pre-commit</code> as a development dependency as well as a <code>.pre-commit-config.yaml</code>. To use these locally install <code>pre-commit</code> in your virtual environment and then install the configuration and all the configured hooks (NB this will download specific virtual environments that <code>pre-commit</code> uses when running hooks so the first time this is run may take a little while).</p> <pre><code>pip install \".[dev]\"\npre-commit install --install-hooks\n</code></pre> <p>Currently there are hooks to remove trailing whitespace, check YAML configuration files and a few other common checks as well as hooks for <code>black</code> and <code>ruff</code>. If these fail then you will not be able to make a commit until they are fixed. The <code>black</code> hook will automatically format failed files so you can simply <code>git add</code> those and try committing straight away. <code>flake8</code> does not correct files automatically so the errors will need manually correcting.</p> <p>If you do not enable and resolve issues reported by <code>pre-commit</code> locally before making a pull request you will find the <code>pre-commit.ci</code> GitHub Action will fail, preventing your Pull Request from being merged. You can shorten the feedback loop and speed up the resolution of errors by enabling <code>pre-commit</code> locally and resolving issues before making your commits.</p>"},{"location":"contributing/#typing","title":"Typing","text":"<p>Whilst Python is a dynamically typed language (that is the type of an object is determined dynamically) the use of Type Hints is strongly encouraged as it makes reading and understanding the code considerably easier for contributors. For more on Type Hints see PEP483 and PEP484</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>All classes, methods and functions should have Numpy Docstrings defining their functionality, parameters and return values and pylint will note and report the absence of docstrings by way of the <code>missing-function-docstring</code> condition.</p> <p>Further, when new methods are incorporated into the package that introduce changes to the configuration they should be documented under Parameter Configuration. pre-commit has the markdownlint-cli2 hook enabled to lint all Markdown files and will where possible automatically fix things, but some issues need resolving manually.</p>"},{"location":"contributing/#testing","title":"Testing","text":"<p>New features should have unit-tests written and included under the <code>tests/</code> directory to ensure the functions work as expected. The pytest framework is used for running tests along with a number of plugins (pytest-regtest for regression testing; pytest-mpl for testing generated Matplotlib images).</p>"},{"location":"contributing/#configuration","title":"Configuration","text":"<p>As described in Parameter Configuration options are primarily passed to TopoStats via a YAML configuration file. When introducing new features that require configuration options you will have to ensure that the default configuration file (<code>topostats/default.yaml</code>) is updated to include your options.</p> <p>Further the <code>topostats.validation.validate.config()</code> function, which checks a valid configuration file with all necessary fields has been passed when invoking <code>topostats</code> sub-commands, will also need updating to include new options in the Schema against which validation of configuration files is made.</p>"},{"location":"contributing/#ide-configuration","title":"IDE Configuration","text":"<p>Linters such as <code>black</code>, <code>flake8</code> and <code>pylint</code> can be configured to work with your IDE so that say Black and/or formatting is applied on saving a file or the code is analysed with <code>pylint</code> on saving and errors reported. Setting up and configuring IDEs to work in this manner is beyond the scope of this document but some links to articles on how to do so are provided.</p> <ul> <li>Linting Python in Visual Studio Code</li> <li>Code Analysis \u2014 Spyder for <code>pylint</code> for Black see How to use   code formatter Black with Spyder.</li> <li>Code Quality Assistance Tips and Tricks, or How to Make Your Code Look Pretty? |   PyCharm</li> <li>Reformat and rearrange code | PyCharm</li> </ul>"},{"location":"data_dictionary/","title":"Data Dictionary","text":"<p>Output from TopoStats includes two sets of statistics in ASCII text <code>.csv</code> files. The tables below detail the columns of these files, the data types, a description and their units where appropriate.</p>"},{"location":"data_dictionary/#all_statisticscsv","title":"<code>all_statistics.csv</code>","text":"<p>The <code>all_statistics.csv</code> file contains details on each grain that has been detected and traced and has the following fields.</p> <p>The <code>all_statistics.csv</code> file contains details on each grain that has been detected and traced and has the following fields.</p> Column / field / feature Description Type Units <code>image</code> Filename (minus extension) of scan. <code>str</code> N/A <code>threshold</code> Whether grain is <code>above</code> or <code>below</code> a threshold. <code>str</code> N/A <code>molecule_number</code> Number of found grain (starts at <code>0</code>) <code>int</code> N/A <code>centre_x</code> x coordinate of grain centre. <code>float</code> m <code>centre_y</code> y coordinate of grain centre. <code>float</code> m <code>radius_min</code> minimum distance from the centroid to edge of the grain. <code>float</code> m <code>radius_max</code> maximum distance from the centroid to edge of the grain. <code>float</code> m <code>radius_mean</code> mean distance from the centroid to the edge of the grain. <code>float</code> m <code>radius_median</code> median distance from the centroid to the edge of the grain. <code>float</code> m <code>height_min</code> Minimum height of grain. <code>float</code> m <code>height_max</code> Maximum height of grain. <code>float</code> m <code>height_median</code> Median height of grain. <code>float</code> m <code>height_mean</code> Mean height of grain. <code>float</code> m <code>volume</code> Volume of the grain calculated as the number of pixels multiplied by each height and scaled to metres. <code>float</code> m^3 <code>area</code> Area of the grain itself calculated as the number of pixels scaled to metres. <code>float</code> m^2 <code>area_cartesian_bbox</code> Area of the bounding box for the grain along the cartesian axes. (Not the smallest bounding box). <code>float</code> m^2 <code>smallest_bounding_width</code> Width of the smallest bounding box for the grain (not along cartesian axes). <code>float</code> m <code>smallest_bounding_length</code> Length of the smallest bounding box for the grain (not along cartesian axes). <code>float</code> m <code>smallest_bounding_area</code> Area of the smallest bounding box for the grain (not along cartesian axes). <code>float</code> m^2 <code>aspect_ratio</code> Aspect ratio of the grain (length / width), always &gt;= 1. <code>float</code> N/A <code>max_feret</code> Longest length of the grain (see Feret diameter). <code>float</code> m <code>min_feret</code> Shortest width of the grain (see Feret diameter). <code>float</code> m <code>contour_length</code> UNKNOWN <code>float</code> m <code>circular</code> Whether the grain is a circular loop or not. <code>float</code> <code>True</code> / <code>False</code> <code>end_to_end_distance</code> UNKNOWN <code>float</code> m <code>basename</code> Directory in which images was found. <code>str</code> N/A"},{"location":"data_dictionary/#image_statscsv","title":"<code>image_stats.csv</code>","text":"<p>The <code>image_stats.csv</code> summarises the metrics</p> Column / field / feature Description Type Units <code>image</code> Filename of image statistics pertain to. <code>str</code> N/A <code>image_size_x_m</code> Width of image. <code>float</code> m <code>image_size_y_m</code> Height of image. <code>float</code> m <code>image_area_m2</code> Area of image (width x height). <code>float</code> m^2 <code>image_size_x_px</code> Width of image in pixels. <code>int</code> N/A <code>image_size_y_px</code> Height of image in pixels. <code>int</code> N/A <code>image_area_px2</code> Area of image in pixels squared. <code>int</code> N/A <code>grains_number_above</code> Number of grains found above threshold. <code>int</code> N/A <code>grains_per_m2_above</code> Density of grains above upper threshold. <code>int</code> N/A <code>grains_number_below</code> Number of grains found below threshold. <code>int</code> N/A <code>grains_per_m2_below</code> Density of grains below lower threshold. <code>int</code> N/A <code>rms_roughness</code> Route Mean Square Roughness, the square root of the mean squared heights across the surface[^1] <code>float</code> N/A <p>[^1][Surface Roughness](https://www.sciencedirect.com/topics/materials-science/surface-roughness); Surface roughness - Wikipedia</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>This is a collection of questions (and answers) to problems that arise with using TopoStats software for processing Atomic Force Microscopy images.</p> <p>If you have questions that are not answered here please consider starting a new discussion in the TopoStats Discussion section of the GitHub repository. Developers will endeavour to help you resolve your problem and it may get added to this page.</p>"},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#my-files-image-files-are-on-a-mounted-google-drive-can-i-process-them-there","title":"My files image files are on a mounted Google Drive, can I process them there?","text":"<p>Maybe! We have had mixed success with processing images that are located on Google Drive and mounted on your local computer. If you find you are encountering errors such as <code>FileExistsError</code> (see #201) then please copy your files to a local drive on your computer for processing and then copy the results back to the network drive.</p>"},{"location":"faq/#common-errors","title":"Common Errors","text":"<p>Common errors that are encountered along with explanations are listed below.</p>"},{"location":"glossary/","title":"Glossary","text":""},{"location":"glossary/#a","title":"A","text":""},{"location":"glossary/#alignment","title":"alignment","text":""},{"location":"glossary/#b","title":"B","text":""},{"location":"glossary/#c","title":"C","text":""},{"location":"glossary/#csv","title":"csv","text":"<p>Comma Separated Values (CSV) is an ASCII plain-text file format where columns of data are separated by commas.</p>"},{"location":"glossary/#d","title":"D","text":""},{"location":"glossary/#dna","title":"DNA","text":"<p>Deoxyribonucleic acid (DNA) is a double-helix polymer of four molecules Cytosine, Guanine, Adenine and Thymine. It is the genetic material of the vast majority of organisms (some viruses use Ribonucleic Acid (RNA) as their genetic material).</p>"},{"location":"glossary/#e","title":"E","text":""},{"location":"glossary/#f","title":"F","text":""},{"location":"glossary/#flattening","title":"flattening","text":"<p>The process of removing tilt from an image.</p>"},{"location":"glossary/#g","title":"G","text":""},{"location":"glossary/#git","title":"Git","text":"<p>Git is a free, open source version control system for managing software projects and development.</p>"},{"location":"glossary/#github","title":"GitHub","text":"<p>GitHub is a website for sharing and collaboratively working on software that is version controlled using Git.</p>"},{"location":"glossary/#grain","title":"Grain","text":"<p>The name given to DNA/RNA/protein structures observed in scans.</p>"},{"location":"glossary/#h","title":"H","text":""},{"location":"glossary/#i","title":"I","text":""},{"location":"glossary/#j","title":"J","text":""},{"location":"glossary/#k","title":"K","text":""},{"location":"glossary/#l","title":"L","text":""},{"location":"glossary/#m","title":"M","text":""},{"location":"glossary/#n","title":"N","text":""},{"location":"glossary/#o","title":"O","text":""},{"location":"glossary/#p","title":"P","text":""},{"location":"glossary/#python","title":"Python","text":"<p>Python is the high-level interpreted programming language in which TopoStats is written.</p>"},{"location":"glossary/#q","title":"Q","text":""},{"location":"glossary/#r","title":"R","text":""},{"location":"glossary/#s","title":"S","text":""},{"location":"glossary/#scars","title":"scars","text":"<p>Artefacts seen in some scans that can be removed during processing. Typically they appear as horizontal high bands. These are removed and the missing values interpolated.</p>"},{"location":"glossary/#skeleton","title":"skeleton","text":"<p>The single-pixel width outline of a molecule after a grain has undergone tracing.</p>"},{"location":"glossary/#t","title":"T","text":""},{"location":"glossary/#tilt","title":"tilt","text":""},{"location":"glossary/#tracing","title":"tracing","text":"<p>The processing step where by a grain is reduced to a single pixel or skeleton. Typically a number of statistics on the shape, length and curvature are calculated after molecules have been traced.</p>"},{"location":"glossary/#u","title":"U","text":""},{"location":"glossary/#v","title":"V","text":""},{"location":"glossary/#w","title":"W","text":""},{"location":"glossary/#x","title":"X","text":""},{"location":"glossary/#y","title":"Y","text":""},{"location":"glossary/#z","title":"Z","text":""},{"location":"installation/","title":"Installation","text":"<p>NB - If you have trouble installing TopoStats please do checkout the discussion for possible solutions. If your problem isn't covered then please do not hesitate to ask a question.</p> <p>TopoStats is a Python package designed to run at the command line. If you are using Microsoft Windows you should use Powershell. You may have Python installed on your system but should use a Python Virtual Environment such as Miniconda and install and use TopoStats under the Virtual Environment. The versions of Python supported are Python &gt;=3.8 and so when creating your virtual environment you should specify this <code>3.8</code> as the minimum.</p>"},{"location":"installation/#setting-up-conda","title":"Setting up Conda","text":"<p>Once you have downloaded and installed Miniconda you can create a virtual environment for installing TopoStats for installing and running TopoStats. We will call this environment <code>topostats</code> (specified with the <code>--name topostats</code> option) and use Python 3.10 (the option <code>python=3.10</code>). After creating it we can, as per the instructions printed out, activate the environment.</p> <pre><code>conda create --name topostats python=3.10\nconda activate topostats\n</code></pre> <p>You are now ready to install TopoStats.</p> <p>NB If you are using an Apple M1 Macbook then you need to install Anaconda &gt;= 2022.05.</p>"},{"location":"installation/#installing-topostats","title":"Installing TopoStats","text":"<p>There are two options for installing TopoStats depending on your usage</p> <ol> <li>Python Package Index - appropriate if you are just using TopoStats and don't need to dig into    the code.</li> <li>Cloning the GitHub Repository - if you want to look at the code, contribute to it, debug errors or perhaps test a new    feature before a release.</li> </ol>"},{"location":"installation/#pypi-installation","title":"PyPI Installation","text":"<p>After activating your <code>topostats</code> Conda environment you can install TopoStats from PyPI using the following command.</p> <pre><code>pip install topostats\n</code></pre> <p>This will install TopoStats under your virtual environment and the command <code>topostats</code> will be available at the command line. It has a number of sub-commands which can be displayed by invoking it without any options. You can upgrade <code>topostats</code> by using the <code>--upgrade</code> flag...</p> <pre><code>pip install --upgrade topostats\n</code></pre> <p>You can always install a specific version from PyPI</p> <pre><code>pip install topostats==2.0.0\n</code></pre> <p>For more information on using <code>pip</code> to install and manage packages please refer to the pip documentation.</p>"},{"location":"installation/#cloning-from-github","title":"Cloning from GitHub","text":"<p>You may wish to consider cloning and installing TopoStats from GitHub if...</p> <ul> <li>You wish to try out new features that have been developed since the last release (if you find problems please create   an issue).</li> <li>If you have found an issue in a released version and want to see if it has been fixed in the unreleased version.</li> <li>If you wish to develop and extend TopoStats with new features yourself.</li> </ul> <p>If you do not have Git already installed please see Git. If you intend to contribute to the development of TopoStats please read through the contributing section.</p> <p>If you are familiar with the command line then you can clone and install TopoStats with the following after activating your virtual environment.</p> <pre><code>git clone https://github.com/AFM-SPM/TopoStats.git\n# If you have SSH access configured to GitHub then you can use\ngit clone git@github.com:AFM-SPM/TopoStats.git\n</code></pre>"},{"location":"installation/#cloning-using-gitkraken","title":"Cloning Using GitKraken","text":"<p>If you are using GitKraken you can clone the repository by selecting \"Clone\" and then \"GitHub.com\" and typing <code>TopoStats</code> into the box next to \"Repository to Clone\" and you should be presented with the option of selecting \"TopoStats\" from the AFM-SPM organisation.</p> <p>Alternatively you can \"Clone with URL\" and enter <code>https://github.com/AFM-SPM/TopoStats.git</code> as the URL to clone from, selecting a destination to clone to.</p>"},{"location":"installation/#installing-topostats-from-the-cloned-repository","title":"Installing TopoStats from the Cloned Repository","text":"<p>Once cloned you will have to open a Terminal and navigate to the directory you cloned and after activating your virtual environment install TopoStats with the following.</p> <pre><code>cd /path/to/where/topostats/was/cloned/TopoStats\npip install .\n</code></pre> <p>If you wish to make changes to the code and test then make a <code>git branch</code>, make your changes and install in editable mode, i.e. <code>pip install -e .</code>.</p> <p>If you plan to contribute to development by adding features or address an existing issue please refer to the contributing section.</p> <p>We include notebooks which show how to use different aspects of TopoStats. If you wish to try these out the Jupyter Noteooks then you can install the dependencies that are required from the cloned TopoStats repository using...</p> <pre><code>pip install \".[notebooks]\"\n</code></pre>"},{"location":"installation/#tests","title":"Tests","text":"<p>One of the major changes in the refactoring is the introduction of unit tests. These require certain packages to be installed which are not installed to your virtual environment by setuptools in the above steps. If you are intending to modify or contribute to the development of TopoStats and making changes to the code base you will likely want to be able to run the tests. Install the necessary dependencies to do so with...</p> <pre><code>cd TopoStats\ngit checkout dev\npip install \".[tests]\"\npytest\n</code></pre>"},{"location":"installation/#git","title":"Git","text":"<p>Git is a version control system for managing software development and is required to be installed on your computer in order to clone the TopoStats repository. Instructions on installing Git can be found at Git Guides - install git.</p> <p>A nice Graphical User Interface for working with Git is GitKraken which includes everything you need.</p>"},{"location":"introduction/","title":"Introduction","text":"<p>TopoStats is a Python package that aims to simplify batch processing Atomic Force Microscopy (AFM) images.</p> <p>Input directories are recursively searched for files of a given type. Each image is then loaded and processed. Multiple images can be processed in parallel.</p> <p>Once an image has been loaded the specified channel of data extracted along with the pixel to nanometre scaling. This data is then aligned and the tilt is removed. Configurable thresholds are then used to generate masks and a second round of tilt removal and row alignment is performed.</p> <p>Molecules/regions of interest known as Grains are then detected based on user specified thresholds and the detected regions are labelled and have preliminary statistics calculated. The labelled regions of each grain then have individual statistics calculated capturing the height, volume, radius and the location of the centroid.</p> <p>Optionally DNA Tracing is then performed, which traces the backbone of the DNA molecules to calculate further statistics such as whether grains are linear or circular, their contour length and end-to-end distances etc.</p> <p>The resulting statistics are written to a CSV file and optionally plots are then generated from various stages of the processing as well as cropped images of each grain. The amount of images produced is also configurable.</p> <p>An schematic overview of the classes and methods that are run in processing files can be found in the workflow page along with more detailed information on installation, usage, configuration and contributing.</p> <p>If you have questions, please post them on the discussion, if you think you've encountered a bug whilst running the code or suggestions for improvements please create an issue in the GitHub project page.</p>"},{"location":"notebooks/","title":"Notebooks","text":"<p>A series of Jupyter Notebooks are provided that demonstrate how to use the TopoStats package in a more interactive manner, calling individual steps. This is useful as it allows the user to explore interactively and with rapid feedback the parameters that may need adjusting in order to process a batch of scans. The notebooks can be found in the <code>notebook/</code> directory after cloning the GitHub repository.</p> Notebook Description <code>00-Walkthrough-minicircle.ipynb</code> Step-by-step walkthrough of processing <code>minicircle.spm</code> from the <code>tests/resources/</code> directory. <code>01-Walthrhgouh-interactive.ipynb</code> Work in Progress As above but uploading a single scan. Will be deployed in Google Colab/Binder for interactive use. <code>02-Summary-statistics-and-plots.ipynb</code> Plotting statistics interactively. <code>03-Plotting-scans.ipynb</code> Plotting NumPy arrays of scans from different stages of processing."},{"location":"notebooks/#installation","title":"Installation","text":"<p>To be able to run the Notebooks you need some additional Python packages installed. You will have to clone the repository from GitHub (see installation) and then install the Notebook dependencies with the following commands under your Virtual Environment (e.g. Conda)...</p> <pre><code>cd TopoStats\npip install \".[notebooks]\"\n</code></pre>"},{"location":"notebooks/#running-notebooks","title":"Running Notebooks","text":"<p>Start a Jupyter server under the Virtual Environment you have installed the dependencies from and a web-browser page will open from which you can choose which notebook to launch.</p> <pre><code>cd TopoStats/notebooks\njupyter notebook\n</code></pre> <p>For more on Jupyter Notebooks please refer to the official documentation.</p>"},{"location":"related_software/","title":"Related Software","text":"<p>TopoStats is one of many pieces of software available for working with Atomic Force Microscopy data, other packages are detailed here. If you know of another package please consider making a pull request to add it to the list.</p>"},{"location":"related_software/#python","title":"Python","text":"<ul> <li>afmformats reading common AFM file formats.</li> <li>AFMReader library developed by TopoStats team for loading different AFM   images.</li> <li>gwyfile a pure Python interface to reading and writing Gwyddion files.</li> <li>magni compressive sampling and reconstruction of Atomic Force Microscopy images.</li> <li>nanite loading, fitting and rating AFM force-distance data.</li> <li>nanoforce import and analyse AFM force curves produced using Nanoscope 5 &amp; 6   and Nanosurf <code>.nid</code> files.</li> <li>nanoscope read data files collected using Bruker, Veeco, and Digital   Instruments Atomic Force Microscopes (AFMs) using Nanoscope v5.12 - v10.00 acquisition software</li> <li>NSFOpen Read data and parameters from Nanosurf NID files.</li> <li>pycroscopy Python Package for scientific analysis of nanoscience   data.</li> <li>pySPM read, handle and plot Scanning Probme Microscopy (SPM) images and ToF-SIMS data.</li> </ul>"},{"location":"related_software/#c","title":"C++","text":"<ul> <li>libasd library for reading asd files, includes Python 3 bindings.</li> </ul>"},{"location":"related_software/#other","title":"Other","text":"<ul> <li>gwyddion a modular program for Scanning Probe Microscopy (SPM) data visualisation and analysis.</li> </ul>"},{"location":"usage/","title":"Usage","text":"<p>After having installed TopoStats you are ready to run it. For convenience TopoStats provides a command line interface <code>topostats</code> that will load a default configuration file and process all images with reasonable default configuration options.</p> <p>However, because the location of your image files can not be known in advance you must make a copy of the default configuration and modify it to work with your files. This guide will hopefully take you through the process of running TopoStats and customising the configuration file with which it is run. If you encounter any problems please ask questions in the Discussions. If you think you have encountered a bug or have a feature suggestion please create an Issue.</p>"},{"location":"usage/#organising-scans","title":"Organising Scans","text":"<p>You should place all files you wish to batch process in a single directory. They can be nested in separate folders as TopoStats will scan for all images within this directory but currently it will only process one scan type at a time (i.e. <code>.spm</code> or <code>.jpk</code> or <code>.asd</code>). This may change in the future.</p>"},{"location":"usage/#command-line-navigation","title":"Command Line Navigation","text":"<p>TopoStats currently runs as a command-line programme. To use it you will have to use a \"prompt\" or \"terminal\" (they're essentially the same thing). What you use will depend on your operating system, but the following are some simple commands on navigation. If you use Windows then for consistency it is recommended to install and use PowerShell.</p> <p>At the command line you use <code>cd</code> to <code>c</code>hange <code>d</code>irectory to the location of your files. For example if your scans are on the C-drive in <code>C:\\User\\me\\work\\spm\\2022-12-08\\scans</code> then you would</p> <pre><code>cd c:/User/me/work/spm/2022-12-08/scans\n</code></pre> <p>If you are on a Linux or OSX system then paths are not prefixed with letters and your files may be saved to <code>/home/me/work/spm/2022-12-08/scans</code>. To change directory there you would...</p> <pre><code>cd /home/me/work/spm/2022-12-08/scans\n</code></pre> <p>NB - Always use a forward-slash (<code>/</code>) when typing directory paths. Windows will display back-slash (<code>\\</code>) but understands forward-slash. Under Linux and OSX they mean different things and so you should always use forward-slash (<code>/</code>).</p> <p>You can always find out what location you are at in the command line using the <code>pwd</code> command (<code>p</code>rint <code>w</code>orking <code>d</code>irectory) and it will print out the directory you are currently at.</p> <pre><code>pwd\n/home/me/work/spm/2022-12-08/scans\n</code></pre> <p>To navigate up one directory level use <code>cd ..</code>. These can be chained together and directories separated with <code>/</code>.</p> <pre><code># Move up a single directory level\ncd ..\npwd\n/home/me/work/spm/2022-12-08\n# Move up another two directory levels\ncd ../../\npwd\n/home/me/\n</code></pre> <p>You can list the files in a directory using the <code>ls</code> command.</p> <pre><code>ls\nsample_image_scan_2022-12-08-1204.spm\n</code></pre> <p>To learn more about the command line see the Introduction to the Command Line for Genomics.</p>"},{"location":"usage/#running-topostats","title":"Running TopoStats","text":"<p>The default location that TopoStats looks for scans is the directory from which it is invoked. Once you start your shell/terminal you will therefore need to do two things.</p> <ol> <li>Navigate to the location of the scans you wish to process using <code>cd /path/to/where/scans/are/located</code>.</li> <li>Activate the virtual environment under which you installed TopoStats (refer to installed if unsure).</li> </ol> <p>You can now run topostats by invoking <code>topostats process</code> and you should start to see some output similar to that below.</p> <pre><code>cd /path/to/where/scans/are/located\ntopostats process\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Configuration is valid.\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Plotting configuration is valid.\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Configuration file loaded from      : None\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Scanning for images in              : /home/neil/work/projects/topostats/TopoStats\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Output directory                    : output\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Looking for images with extension   : .spm\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Images with extension .spm in /home/neil/work/projects/topostats/TopoStats : 32\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Thresholding method (Filtering)     : std_dev\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Thresholding method (Grains)        : std_dev\n...\n</code></pre> <p>On a successful completion you should see a message similar to the following which indicates various aspects of the run along with information about how to give feedback, report bugs and cite the software.</p> <pre><code>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ COMPLETE ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n  Base Directory              : /home/neil/work/projects/topostats/TopoStats\n  File Extension              : .spm\n  Files Found                 : 1\n  Successfully Processed      : 1 (100.0%)\n  Configuration               : output/config.yaml\n  All statistics              : output/all_statistics.csv\n  Distribution Plots          : output/summary_distributions\n\n  Email                       : topostats@sheffield.ac.uk\n  Documentation               : https://afm-spm.github.io/topostats/\n  Source Code                 : https://github.com/AFM-SPM/TopoStats/\n  Bug Reports/Feature Request : https://github.com/AFM-SPM/TopoStats/issues/new/choose\n  Citation File Format        : https://github.com/AFM-SPM/TopoStats/blob/main/CITATION.cff\n\n  If you encounter bugs/issues or have feature requests please report them at the above URL\n  or email us.\n\n  If you have found TopoStats useful please consider citing it. A Citation File Format is\n  linked above and available from the Source Code page.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n</code></pre> <p>The command <code>topostats process</code> has a number of additional flags for passing different options. These can be viewed using the <code>-h</code> or <code>--help</code> flag.</p> <pre><code> \u2771 topostats process --help\nusage: topostats process [-h] [-c CONFIG_FILE] [-s SUMMARY_CONFIG] [--matplotlibrc MATPLOTLIBRC] [-b BASE_DIR] [-j CORES] [-l LOG_LEVEL] [-f FILE_EXT] [--channel CHANNEL] [-o OUTPUT_DIR] [--save-plots SAVE_PLOTS] [-m MASK] [-w WARNINGS]\n\nProcess AFM images. Additional arguments over-ride those in the configuration file.\n\noptions:\n  -h, --help            show this help message and exit\n  -c CONFIG_FILE, --config-file CONFIG_FILE\n                        Path to a YAML configuration file.\n  -s SUMMARY_CONFIG, --summary-config SUMMARY_CONFIG\n                        Path to a YAML configuration file for summary plots and statistics.\n  --matplotlibrc MATPLOTLIBRC\n                        Path to a matplotlibrc file.\n  -b BASE_DIR, --base-dir BASE_DIR\n                        Base directory to scan for images.\n  -j CORES, --cores CORES\n                        Number of CPU cores to use when processing.\n  -l LOG_LEVEL, --log-level LOG_LEVEL\n                        Logging level to use, default is 'info' for verbose output use 'debug'.\n  -f FILE_EXT, --file-ext FILE_EXT\n                        File extension to scan for.\n  --channel CHANNEL     Channel to extract.\n  -o OUTPUT_DIR, --output-dir OUTPUT_DIR\n                        Output directory to write results to.\n  --save-plots SAVE_PLOTS\n                        Whether to save plots.\n  -m MASK, --mask MASK  Mask the image.\n  -w WARNINGS, --warnings WARNINGS\n                        Whether to ignore warnings.\n</code></pre>"},{"location":"usage/#reducing-output","title":"Reducing Output","text":"<p>If you find the output too verbose or of no use you can reduce it by setting the <code>log_level</code> to either <code>error</code> or <code>warning</code>. This can be done either in the configuration file (see Configuration below) or using the <code>-l</code>/<code>--log-level</code> flag for example <code>topostats process --log_level warning</code>.</p>"},{"location":"usage/#configuring-topostats","title":"Configuring TopoStats","text":"<p>Configuration of TopoStats is done through a YAML file and a full description of the fields used can be found under the configuration section.</p> <p>Here we will go through generating a configuration file to edit and some of the common changes that you are likely to want to make to the default configuration and how to make them.</p>"},{"location":"usage/#generating-configuration-file","title":"Generating Configuration File","text":"<p>TopoStats will use some reasonable default parameters by default, but typically you will want to customise the parameters that are used. This is achieved using a configuration file. This is a YAML file that contains parameters for different settings. For convenience you can generate a sample configuration file in your current working directory using the <code>topostats create-config</code> sub-command. It takes a single argument, the name of the file to save the configuration to (e.g. <code>config.yaml</code> or <code>settings.yaml</code>), and it will write the current default configuration to that file.</p> <pre><code>topostats create-config --filename my_config.yaml\nls -l\nmy_config.yaml\nsample_image_scan_2022-12-08-1204.spm\n</code></pre> <p>You can now edit and/or rename the <code>my_config.yaml</code>. It can be called anything you want, e.g. <code>todays_first_run_configuration.yaml</code> is a valid name.</p>"},{"location":"usage/#editing-configyaml","title":"Editing <code>config.yaml</code>","text":"<p>IMPORTANT This file is an ASCII text file and you should use NotePad (Windows), TextEdit (OSX) or Nano/Emacs/Vim (GNU/Linux) or any other text editor. Do not use Microsoft Word or any other Word Processor to edit this file.</p> <p>You can now start customising the configuration you are going to run TopoStats with. All fields have defaults but the ones you may want to change are....</p> <ul> <li><code>base_dir</code> (default: <code>./</code>) the directory in which to search for scans. By default this is <code>./</code> which represents the   directory from which <code>topostats process</code> is called and it is good practice to have one configuration file per batch of   scans that are being processed.</li> <li><code>output_dir</code> (default: <code>output</code>) the location where the output is saved, by default this is the directory <code>output</code>   which will be created if it doesn't exist. If you wish for the output to be somewhere else specify it here. If you   want <code>Processed</code> directories to sit within the directories that images are found then simply set the <code>output_dir</code> to   the same value as <code>base_dir</code>.</li> <li><code>log_level</code> (default: <code>info</code>) the verbosity of output to the console and log file, the options in order of verbosity   are <code>debug</code> &gt; <code>info</code> &gt; <code>warning</code> &gt; <code>error</code>. If you want less output set to <code>warning</code> or <code>error</code>. If you encounter   errors please set to <code>debug</code> and run again and include the log in your bug   report.</li> <li><code>cores</code> (default: <code>2</code>) the number of parallel processes to run processing of all found images. Set this to a maximum   of one less than the number of cores on your computers CPU. If unsure leave as is, but chances are you can increase   this to at least <code>4</code> quite safely.</li> <li><code>file_ext</code> (default: <code>.spm</code>) the file extension of scans to search for within the current directory. The default is   <code>.spm</code> but other file format support is in the pipeline.</li> <li><code>plotting</code> : <code>image_set</code> (default <code>core</code>) specifies which steps of the processing to plot images of. The value <code>all</code>   gets images for all stages, <code>core</code> saves only a subset of images.</li> </ul> <p>Most of the other configuration options can be left on their default values for now. Once you have made any changes save the file and return to your terminal.</p>"},{"location":"usage/#running-topostats-with-my_configyaml","title":"Running TopoStats with <code>my_config.yaml</code>","text":"<p>To use your new configuration file you need to inform <code>topostats process</code> to use that file rather than the defaults, this is done using the <code>--config config.yaml</code> file.</p> <p>NB this assumes that you are in the same directory as your scans where you have saved the <code>my_config.yaml</code> file that you edited. That doesn't have to be the case but it makes life easier for if you are not familiar with absolute and relative paths.</p> <pre><code>topostats process --config my_config.yaml\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Configuration is valid.\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Plotting configuration is valid.\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Configuration file loaded from      : None\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Scanning for images in              : /home/neil/work/projects/topostats/TopoStats\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Output directory                    : output\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Looking for images with extension   : .spm\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Images with extension .spm in /home/neil/work/projects/topostats/TopoStats : 1\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Thresholding method (Filtering)     : std_dev\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Thresholding method (Grains)        : std_dev\n...\n</code></pre> <p>On successful completion you should see the same message noted above.</p>"},{"location":"usage/#output","title":"Output","text":"<p>The output from running TopoStats is saved in the location defined in the configuration file by <code>output_dir</code>. The default is the directory <code>output</code> within the directory from which <code>topostats process</code>. This may differ if you have used your own customised configuration file (specifically if you have modified the <code>output_dir:</code> option).</p> <p>At the top level of the output directory are two files <code>config.yaml</code> and <code>all_statistics.csv</code></p> <ul> <li><code>config.yaml</code> : a copy of the configuration used to process the images.</li> <li><code>all_statistics.csv</code> : a Comma Separated Variable ASCII plain-text file of the grain and DNA tracing statistics.</li> </ul> <p>The remaining directories of results is contingent on the structure of files within the <code>base_dir</code> that is specified in the configuration. If all files are in the top-level directory (i.e. no nesting) then you will have just a <code>Processed</code> directory. If there is a nested structure then there will be a <code>Processed</code> directory in each folder that an image with the specified <code>file_ext</code> has been found. This is perhaps best illustrated by way of example.</p> <p>If you have the following three <code>.spm</code> files within your current directory, one at the top level, one under <code>level1</code> and one under <code>level1/a</code>...</p> <pre><code>[4.0K Nov 15 13:55]  .\n|-- [4.0K Nov 15 13:54]  ./level1\n|   |-- [4.0K Nov 15 13:54]  ./level1/a\n|   |-- [ 32M Nov 15 13:54]  ./level1/a/minicircle.spm\n|   |-- [ 32M Nov 15 13:54]  ./level1/minicircle.spm\n|-- [ 32M Nov 15 13:54]  ./minicircle.spm\n</code></pre> <p>...then under <code>output</code> (the default for<code>output_dir</code>) you will see the following directory structure.</p> <pre><code>[4.0K Nov 15 14:06]  output\n|-- [ 381 Nov 15 14:06]  output/all_statistics.csv\n|-- [7.4K Nov 15 14:06]  output/config.yaml\n|-- [4.0K Nov 15 14:06]  output/level1\n|   |-- [4.0K Nov 15 14:06]  output/level1/a\n|   |   |-- [4.0K Nov 15 14:06]  output/level1/a/Processed\n|   |-- [4.0K Nov 15 14:06]  output/level1/Processed\n|-- [4.0K Nov 15 14:06]  output/Processed\n</code></pre> <p>...where there is one <code>Processed</code> directory at the sub-directory level that each image was found.</p> <p>NB If you want <code>Processed</code> directories to sit within the directories that images are found then simply set the <code>output_dir</code> to the same value as <code>base_dir</code>.</p> <p>Within each <code>Processed</code> directory is a directory for each file found with the specified <code>file_ext</code> and within these are the resulting images from processing scans. If the <code>plotting</code> : <code>image_set</code> is <code>core</code> then there is a single image for each. If this option is <code>all</code> then there is also a sub-directory for each image found within which there are the directories <code>filters</code>, <code>grains/below</code> and <code>grains/above</code> which contain additional images from the processing stages and an accompanying histogram for each image showing the distribution of pixel heights for that image.</p>"},{"location":"usage/#summary-plots","title":"Summary Plots","text":"<p>By default TopoStats will take the data that has been summarised across all files and generate a series of plots, histograms with Kernel Density Estimates (KDE) overlaid and Violin plots. The default location of these if no custom configuration file is used is <code>output/summary_distributions</code>. If you have used a custom configuration file it will be the sub-directory <code>summary_distributions</code> nested under the directory specified for the <code>output</code>, e.g. if you used the current directory as output you will have a <code>summary_distributions</code> directory present.</p> <p>Sometimes you may have a <code>all_statistics.csv</code> from a run and wish to plot distributions of additional statistics that were not already plotted. This can be achieved using the command line programme <code>toposum</code> which is included.</p> <p>NB Because of the inherent complexity of plots this script is, by design, limited in the scope to which plots can be configured. It uses the plotting library Seaborn (which is built on top of Matplotlib) to produce basic plots, which are not intended for publication. If you want to tweak or customise plots it is recommended to load <code>all_statistics.csv</code> into a Jupyter Notebook and generate the plots you want there. A sample notebook is included to show how to do this.</p>"},{"location":"usage/#configuring-summary-plots","title":"Configuring Summary Plots","text":"<p>Configuration of summary plots is also via a YAML configuration file a description of the fields can be found under configuration page. You can generate a sample configuration by invoking the <code>--create-config-file</code> option to <code>toposum</code></p> <pre><code>toposum --create-config-file custom_summary_config.yaml\n</code></pre> <p>The file <code>custom_summary_config.yaml</code> can then be edited to change what plots are generated, where they are saved to and so forth. Typically you will only want to adjust a few settings such as toggling the types of plots (<code>hist</code>, <code>kde</code> and <code>violin</code>), the number of <code>bins</code> in a histogram or the statistic to plot in histograms (<code>count</code>, <code>frequency</code> etc.). You can change the <code>palette</code> that is used by Seaborn and crucially toggle which statistics are summarised by commenting and uncommenting the statistic names under <code>stats_to_sum</code>.</p>"},{"location":"usage/#labels","title":"Labels","text":"<p>Labels for the plots are generated from the file <code>topostats/var_to_label.yaml</code> which provides a dictionary that maps the variable name as the dictionary <code>key</code> to its description stored in the dictionary <code>value</code>. If you wish to customise these you can do so and pass it to <code>toposum</code> using the <code>--plotting_dictionary</code> which takes as an argument the path to the file you have created.</p>"},{"location":"usage/#pickles","title":"Pickles","text":"<p>The option <code>pickle_plots: True</code> will save to the specified <code>output_dir</code> the file <code>distribution_plots.pkl</code> which is a binary format that saves the plots that have been generated and saved in nested dictionaries so that they can be loaded again. The Notebook <code>notebooks/02-Summary-statistics-and-plots.ipynb</code> shows how to load these and make simple modifications to the the plots.</p>"},{"location":"workflow/","title":"Workflow","text":"<p>This section gives a broad overview of the steps taken in processing images.</p>"},{"location":"workflow/#topotracing-processing-a-single-spm-file","title":"Topotracing : Processing a single <code>.spm</code> file","text":"<p>Topotracing loads images from <code>.spm</code> files and extracts the specified channel, performing various filtering stages (<code>Filters()</code> class) before finding grains (<code>Grains()</code> class) and then calculating statistics for each grain (<code>GrainStats()</code> class). The Gaussian filtered image and labelling of grains is then passed onto DNA Tracing.</p> <pre><code>%%{init: {'theme': 'base',\n         }\n}%%\ngraph TD;\n\n  subgraph Background Flattening\n  A1([Load YAML Configuration]) --&gt; A2([Load SPM])\n  A2 --&gt; A3([Extract channel from SPM])\n  A3 --&gt; A4([Initial Align])\n  A4 --&gt; A5([Initial Tilt Removal])\n  A5 --&gt; A6([Thresholding Otsu])\n  A6 --&gt; A7([Mask Generation])\n  A7 --&gt; A8([Masked Align])\n  A8 --&gt; A9([Masked Tilt Removal])\n  A9 --&gt; A10([Background Zeroing])\n  end\n  subgraph Grain Finding\n  A10 --&gt; B1([Lower Thresholding])\n  B1 --&gt; B2([Gaussian Filtering])\n  B2 --&gt; B3([Tidy Edges])\n  B3 --&gt; B4([Preliminary Statistics])\n  B4 --&gt; B5([Size Thresholding])\n  B5 --&gt; B6([Label Regions])\n  end\n  subgraph Grain Statistics\n  B6 --&gt; C2([Calculate Points])\n  C2 --&gt; C8([Height &amp; Volume Statistics])\n  C2 --&gt; C3([Calculate Edges])\n  C2 --&gt; C4([Calculate Centroid])\n  C3 --&gt; C5([Calculate Radius Statistics])\n  C3 --&gt; C6([Convex Hull / Graham Scan])\n  C6 --&gt; C7([Minimum Bounding Box Statistics])\n  end\n  subgraph DNA Tracing\n  B2 --&gt; D1([More Analysis])\n  B5 --&gt; D1\n  end\n  style A1 fill:#648FFF,stroke:#000000\n  style A2 fill:#648FFF,stroke:#000000\n  style A3 fill:#648FFF,stroke:#000000\n  style A4 fill:#648FFF,stroke:#000000\n  style A5 fill:#648FFF,stroke:#000000\n  style A6 fill:#648FFF,stroke:#000000\n  style A7 fill:#648FFF,stroke:#000000\n  style A8 fill:#648FFF,stroke:#000000\n  style A9 fill:#648FFF,stroke:#000000\n  style A10 fill:#648FFF,stroke:#000000\n  style B1 fill:#DC267F,stroke:#000000\n  style B2 fill:#DC267F,stroke:#000000\n  style B3 fill:#DC267F,stroke:#000000\n  style B4 fill:#DC267F,stroke:#000000\n  style B5 fill:#DC267F,stroke:#000000\n  style B6 fill:#DC267F,stroke:#000000\n  style C2 fill:#FE6100,stroke:#000000\n  style C3 fill:#FE6100,stroke:#000000\n  style C4 fill:#FE6100,stroke:#000000\n  style C5 fill:#FE6100,stroke:#000000\n  style C6 fill:#FE6100,stroke:#000000\n  style C7 fill:#FE6100,stroke:#000000\n  style C8 fill:#FE6100,stroke:#000000\n  style D1 fill:#785EF0,stroke:#000000\n</code></pre>"},{"location":"workflow/#dna-tracing-processing-a-single-grain","title":"DNA Tracing : Processing a single grain","text":"<pre><code>%%{init: {'theme': 'base',\n         }\n}%%\ngraph TD;\n\n  subgraph dnaTrace\n  A1([\"get_numpy_arrays() | Load Numpy arrays to dictionary indexed by number\"]) --&gt; A2([\"skimage.filters.gaussian() | Filter full image\"])\n  A2 -- For each image --&gt; A3([\"get_disordered_trace() | extracts mask\"])\n  A3 --&gt; A4([\"purge_obvious_crap() | Removes ites if len() &lt; 10 (i.e. small objects) \"])\n  A4 --&gt; A5([\"linear_or_circular on unordered traces() | linear or circular molecule\"])\n  A5 --&gt; A6([\"get_ordered_traces() | Reorders points in the array?\"])\n  A6 --&gt; A7([\"linear_or_circular() on ordered traces\"])\n  A7 --&gt; A8([\"get_fitted_traces()\"])\n  A8 --&gt; A9([\"get_splined_traces()\"])\n  A9 --&gt; A10([\"measure_contour_length()\"])\n  A10 --&gt; A11([\"measure_end_to_end_distance()\"])\n  A11 --&gt; A12([\"report_basic_stats()\"])\n  end\n\n  subgraph \"get_disordered_trace()\"\n  A3 --&gt; B1([\"ndimage.binary_dilation() | extracts mask\"])\n  B1 --&gt; B2([\"scipy.ndimage.gaussian_filter molecule()\"])\n  B2 --&gt; B3([\"getSkeleton()\"])\n  B3 --&gt; A4\n  end\n\n  subgraph \"getSkeleton()\"\n  B3 --&gt; C1([\"Skeletonize | to be replaced by get_skeleton()\"])\n  C1 --&gt; B3\n  end\n\n  style A1 fill:#648FFF,stroke:#000000\n  style A2 fill:#648FFF,stroke:#000000\n  style A3 fill:#648FFF,stroke:#000000\n  style A4 fill:#648FFF,stroke:#000000\n  style A5 fill:#648FFF,stroke:#000000\n  style A6 fill:#648FFF,stroke:#000000\n  style A7 fill:#648FFF,stroke:#000000\n  style A8 fill:#648FFF,stroke:#000000\n  style A9 fill:#648FFF,stroke:#000000\n  style A10 fill:#648FFF,stroke:#000000\n  style A11 fill:#648FFF,stroke:#000000\n  style A12 fill:#648FFF,stroke:#000000\n  style B1 fill:#DC267F,stroke:#000000\n  style B2 fill:#DC267F,stroke:#000000\n  style B3 fill:#DC267F,stroke:#000000\n  style C1 fill:#FE6100,stroke:#000000\n</code></pre>"},{"location":"advanced/","title":"Overview","text":"<p>You can read more detailed information about the methods implemented in TopoStats in the pages below.</p> <ul> <li>Flattening</li> <li>Grain Finding</li> <li>Thresholding</li> <li>Grain Stats</li> <li>Disordered Tracing</li> <li>Nodestats</li> <li>Ordered Tracing</li> <li>Splining</li> </ul>"},{"location":"advanced/disordered_tracing/","title":"Disordered Tracing","text":"<p>This section gives an in-depth overview of the steps taken in the disordered tracing module.</p>"},{"location":"advanced/disordered_tracing/#at-a-glance-simple-representations","title":"At a Glance : Simple Representations","text":"<p>The <code>disordered_tracing.py</code> module handles all the functions associated with obtaining single-pixel wide, line representations of masked objects.</p> <p>The quality and likeness of the resultant pruned skeleton thus depends on the quality of the mask, the effectiveness of smoothing parameters, the method of skeletonisation, and the quality of automating the pruning of incorrect skeletal branches.</p> <p></p> <p>This module measures the number of junctions and endpoints for each pruned skeleton object and appends these columns to the <code>all_statistics.csv</code>. In addition, the <code>all_disordered_segment_statistics.csv</code> file is produced which measures the length, type, connections, and pixel value (typically height); minimum, middle, median, mean and standard deviation for each skeleton segment between junctions using Skan. The branch types are given by:</p> <ul> <li>0: Endpoint-to-endpoint</li> <li>1: Endpoint-to-junction</li> <li>2: Junction-to-junction</li> <li>3: Isolated cycle</li> </ul> <p>Some useful points to bear in mind:</p> <ul> <li>Bad mask, bad skeleton - If the mask closes holes seen in the image, all skeletonisation methods will produce a   single line for this region.</li> <li>No skeletons in image - The disordered trace <code>core</code> image may not show the resultant skeletons if the plotting   <code>dpi</code> value is too low (varies based on image size).</li> <li>Hard to remove branches - If there are still spurious branches to prune after modifying the   <code>mask_smoothing_params</code> and <code>pruning_params</code>, try increasing the <code>filters.gaussian_size</code> value to smooth the image the   mask is created from.</li> <li>Masked colours are relative - Any mask colours that may be produced by plots are relative to the mask values in   that image as they will always span the masked colourmap, and will not compare well across images if the range of mask   values differ.</li> </ul>"},{"location":"advanced/disordered_tracing/#processing-steps","title":"Processing Steps","text":""},{"location":"advanced/disordered_tracing/#1-smoothing-the-mask","title":"1. Smoothing the Mask","text":"<p>Generated masks can be quite jagged which causes a large increase in spurious skeletal branches which do not best represent the image. This can be resolved by first smoothing the incoming mask.</p> <p></p> <p>Smoothing uses either binary dilation (expanding the borders of the mask <code>dilation_iterations</code> times) or an otsu threshold applied to a gaussian filtered mask (with a smoothing factor of <code>gaussian_sigma</code>). If both values are provided an input in the configuration file, they will compete, and the winning result is the one with the smallest number of added pixels.</p> <p>The reason for the competition is an attempt to conserve mask topology i.e. any small holes it has which may become closed upon dilation / gaussian blurring. The dilation method seems to work best for larger masks where there are more mask pixels, and the gaussian smoothing better when there are small holes.</p> <p>In addition, this smoothing step also tries to preserve mask topology by re-adding any holes back into the mask that lie within the <code>holearea_min_max</code> threshold. This has the resulting effect of only smoothing the outer edge of the mask.</p> <p>If required, the function can also take <code>null</code> values for both <code>dilation_iteration</code> and <code>gaussian_sigma</code> values to return the original grain. This is useful for retaining the sample topology when the masking / segmentation is good enough, for example when you use finely-trained deep learning models.</p>"},{"location":"advanced/disordered_tracing/#2-skeletonisation","title":"2. Skeletonisation","text":"<p>Skeletonisation is the process of reducing a binary image to a single-pixel wide representation. This can be done using the algorithms provided by scikit-image such as <code>zhang</code> (rule based + erosion), <code>lee</code> (connectivity preserving), <code>medial_axis</code> (pixels with &gt;1 closest boundary pixels), or <code>topostats</code> - a modification of Zhang's algorithm which uses the underlying height information to order and then remove only a percentage of the pixels marked for removal on each skeletonisation iteration. The skeletonisation methods are handled by the <code>tracing/skeletonize.py</code> module.</p> <p></p> <p>We have found that by including the height information into the skeletonisation process and removing the lowest <code>height_bias</code> percent, we can bias the skeleton to lie on the DNA backbone and generate a better representation of the molecule, especially at crossing points and regions where the mask is less accurate.</p> <p></p>"},{"location":"advanced/disordered_tracing/#3-pruning","title":"3. Pruning","text":"<p>Pruning is the act of removing spurious branches from a skeleton which does not follow the underlying masks' shape. To this end, TopoStats provides a variety of methods and parameters to help clean up the skeletons.</p> <p></p> <p>Pruning can be done by branch length (using the <code>max_length</code> configuration parameter) and / or the branch height (using the <code>height_threshold</code>, <code>method_values</code> and <code>method_outliers</code> configuration parameters).</p> <p>Length pruning is the simplest, iteratively comparing the length (in nm) of each branch containing an endpoint with the <code>max_length</code> parameter. If it's length is below this value, it is deemed as a spurious branch and removed, along with any junction pixels until a single-pixel skeleton remains.</p> <p>Height pruning also iteratively compares the branches pixel values (height) to a <code>height_threshold</code> parameter, again, any branch heights which are below this value are pruned. However, as there are multiple pixels in a branch, we provide different methods to obtain a single branch height value for comparison i.e. <code>method_values</code>:</p> <ul> <li><code>min</code> - the minimum value of all the branch pixel values.</li> <li><code>median</code>- the median value of all the branch pixel values.</li> <li><code>mid</code> - the middle value (averaged if two) of the ordered branch pixels. This is particularly useful for pruning false   bridges where the height dips in the middle of the branch.</li> </ul> <p>Additionally, how these branch height values compare to the <code>height_threshold</code> is also considered i.e <code>method_outliers</code>:</p> <ul> <li><code>abs</code> - prunes branch values below the absolute value of the <code>height_threshold</code>.</li> <li><code>mean_abs</code> - prunes branch values below the whole skeleton mean pixel value - absolute threshold. This is useful for   non-surface samples or periodic structures e.g. in DNA we expect the mean height to be around 2nm, but high resolution   imaging may cause this to dip to 0.8nm (the depth of a major groove), so we'd want to prune branch heights below this.</li> <li><code>iqr</code> - prunes branch values below 1.5x inter-quartile range (IQR) of all the branches. Height pruning cannot produce   more than one skeleton and so avoids breaking up the skeleton into multiple parts.</li> </ul>"},{"location":"advanced/disordered_tracing/#outputs","title":"Outputs","text":"<p>The <code>&lt;image&gt;_&lt;threshold&gt;_disordered_trace</code> image shows the pruned skeletons that are used to obtain the below metrics and passed onto future processing stages.</p> <p>For each grain, the following new columns are added to the <code>grainstats.csv</code> file:</p> Column Name Description Data Type <code>grain_endpoints</code> The number of pixels designated as endpoints (only 1 neighbour) in the pruned skeleton. <code>integer</code> <code>grain_junctions</code> The number of pixels designated as junctions (&gt;2 neighbours) in the pruned skeleton. <code>integer</code> <code>total_branch_length</code> The sum of all branch lengths in the pruned skeleton. <code>float</code> <p> </p> <p></p>"},{"location":"advanced/disordered_tracing/#disordered-segment-statistics","title":"Disordered Segment Statistics","text":"<p>An <code>all_disordered_segment_statistics.csv</code> file is produced for each image which measures the following metrics from each segment in each pruned skeleton:</p> Column Name Description Data Type <code>image</code> The image name being processed. <code>string</code> <code>threshold</code> The direction of the grain threshold being applied. <code>string</code> <code>grain_number</code> The number of the grain being processed in the image. <code>integer</code> <code>index</code> The branch index. <code>integer</code> <code>branch-distance</code> The distance (in nm) of the branch. <code>float</code> <code>branch-type</code> Branch classification of endpoint-to-endpoint (0), endpoint-to-junction (1), junction-to-junction (2), isolated cycle (3). <code>integer</code> <code>connected_segments</code> The index of the branch segments that this current branch is connected to via a junction point. <code>list</code> <code>mean-pixel-value</code> The mean of the branch pixel values (height), in nm. <code>float</code> <code>stdev-pixel-value</code> The standard deviation of the branch pixel values (height), in nm. <code>float</code> <code>min-value</code> The minimum value of the branch pixel values (height), in nm. <code>float</code> <code>median-value</code> The median value of the branch pixel values (height), in nm. <code>float</code> <code>mid-value</code> The value of a pixel halfway along the ordered branch (height), in nm. <code>float</code> <code>basename</code> The directory path containing the image. <code>string</code> <p> </p> <p></p>"},{"location":"advanced/disordered_tracing/#diagnostic-images","title":"Diagnostic Images","text":"<p>Images produced by the <code>plotting.image_set: all</code> for this module are:</p> <ul> <li><code>21-smoothed_grains</code> - The smoothed mask, used to check that the image topology is retained (holes) before   skeletonisation.</li> <li><code>22-original_skeletons</code> - Skeletonised mask, used to ensure the skeletons follow the underlying structures. Used to   check if the skeletonisation parameters are suitable.</li> <li><code>23-branch_indexes</code> - An integer mask of the pruned skeleton with branch pixel values matching the index value in the   data. Used to cross reference the <code>all_disordered_segment_statistics.csv</code> data with an image. Using the default   colourmap for this (viridris), darker (purple) colours are lower indexes, and brighter (yellow) colours are higher.</li> <li><code>24-branch_types</code> - An integer mask of the pruned skeleton with branch pixel values matching the <code>branch-type</code>   (numbers and definitions in the \"At a Glance\" section). This can be used to count and check if the skeletonisation   process correctly identifies the different branch types. Using the default colourmap for this (viridris), darker   (purple) colours are lower indexes, and brighter (yellow) colours are higher.</li> </ul> <p></p>"},{"location":"advanced/disordered_tracing/#possible-uses","title":"Possible Uses","text":"<p>This module would lend itself to measuring branched structures and may aid the identification of particular regions by filtering-out segments based on their branch type.</p> <p>We have used this module to identify and measure the length of reverse forks in stalled DNA replication intermediates as these structures should produce closed loops, however, the reverse forks can be identified as branches with endpoints and their metrics identified from the data. It has also been used as part of the pipeline to obtain ordered traces along topologically complex DNA molecules and topological classifications in our paper; Under or Over? Tracing Complex DNA Topologies with High Resolution Atomic Force Microscopy.</p>"},{"location":"advanced/flattening/","title":"Flattening","text":"<p>Flattening is the process of taking a raw AFM image, and removing the image artefacts that are present due to the scanning probe microscopy (SPM) and AFM imaging. These encompass, but are not limited to; row alignment from the raster scanning motion, and polynomial flattening of a surface from piezoelectric bowing. For surface based samples, such as DNA on Mica, this results in an image where the background mica is flat and the sample is clearly visible resting on the surface.</p> <p>Here is a raw, unprocessed AFM image:</p> <p></p> <p>You can see there is a large tilt in the image from the bottom right to the top left, as well as lots of horizontal banding throughout the rows in the image. These artefacts are removed during the flattening process in TopoStats knows as <code>Filters</code>.</p>"},{"location":"advanced/flattening/#at-a-glance-removing-afm-imaging-artefacts","title":"At a Glance - Removing AFM Imaging Artefacts","text":"<p>Images are processed by:</p> <ul> <li>Row alignment (make each row median the same height)</li> <li>Tilt &amp; polynomial removal (fit a plane and quadratic polynomial to the image and subtract)</li> <li>Scar removal (remove long, thin, bright streaks in the data)</li> <li>Zero the average height (lower the image by the mean height) to make the background roughly centred at zero nm</li> <li>Masking (detect objects on the surface and flatten the image again, ignoring the data on the surface)</li> <li>Secondary flattening (re-process the data using the mask to tell us where the background is, and zero the data using   the mean of the background mask)</li> <li>Gaussian filter (to smooth pixel differences / high-gain noise)</li> </ul> <p></p>"},{"location":"advanced/flattening/#row-alignment","title":"Row alignment","text":"<p>The first step in the flattening process is row alignment. Row alignment is a process that adjusts the height of each row of the image so that they all share the same median height value. This \"median\" value is set by the <code>row_alignment_quartile</code> where the default of 0.5 is the median value, but can be adjusted depending on how much data is considered background. This gets rid of some of the horizontal banding and produces an image where the rows are aligned, but the image still has a clear tilt.</p> <p></p>"},{"location":"advanced/flattening/#tilt-removal","title":"Tilt removal","text":"<p>After row alignment, tilt removal is applied. This is a simple process of fitting and subtracting a plane to the image, resulting in a mostly flat image. However as you can see in the following image, it's not perfect and there still exists \"shadows\" on rows with lots of non-background data. Two images are provided here, one with the full z-range and one with an adjusted height range (z-range) to show the remaining artefacts better, such as the low regions or \"shadows\" on rows with lots of non-background data.</p> <p></p> <p></p>"},{"location":"advanced/flattening/#polynomial-removal","title":"Polynomial removal","text":"<p>After the tilt, we remove the polynomial trends. In some images, there is also quadratic or occasionally cubic bowing to the image too. We remove this by fitting a two dimensional quadratic polynomial to the image (in the horizontal direction), and subtracting it from the image. We then do the same for a nonlinear polynomial (z = axy) to eliminate \u201csaddle\u201d trends in the data. We could do all of these at the same time, but we like to be able to see the iterative differences.</p>"},{"location":"advanced/flattening/#scar-removal-optional","title":"Scar removal (optional)","text":"<p>We then optionally run scar removal on the image. This is a special function that detects scars - long, thin, bright / dark streaks in the data, caused by physical problems in the AFM process. They are found by the parameters; <code>threshold_low</code> and <code>threshold_high</code> identifying great height changes between rows, and filtered for scars via <code>max_scar_width</code> and <code>min_scar_length</code> in pixel lengths. We are using a different image here as an example since our lovely minicircles.spm image doesn\u2019t have any scars.</p> <p></p> <p></p> <p>Note that scar removal can distort data, and it\u2019s best to take data without scars if you can.</p>"},{"location":"advanced/flattening/#zero-the-average-height","title":"Zero the average height","text":"<p>We then lower the image by its mean height which causes the background of the image to be roughly centred at zero nm. If this function is provided a foreground mask such as in the second iteration of flattening, this function zeros the data only on the background data. Data zeroing is important since the raw AFM heights are relative, and these processing steps can shift the background height away from zero, so this makes it easier to obtain comparative height metrics.</p>"},{"location":"advanced/flattening/#masking","title":"Masking","text":"<p>Now consider that all the processing we have done has assumed that every pixel of the image is background. We assumed that there were no objects on the surface, messing up our fitting, and row alignment. If there was a large amount of DNA on one side of the image, then the slope will be affected by it, and so flatten the image poorly.</p> <p>Because of this, once we have done our initial flattening, we detect our objects on the surface, and then flatten the image again! But this time, ignoring the data on the surface, and only considering the background.</p> <p>How do we do that? Well first, we need to find the data on the surface. We do this by thresholding. The type of threshold (standard deviation - <code>std_dev</code>, absolute - <code>absolute</code>, otsu - <code>otsu</code>), and the threshold values are set by the config file (have a look!). Any pixels that are below the threshold, are considered background (sample surface). Any pixels that are above the threshold are considered to be data (useful sample objects). This binary classification allows us to make a binary mask of where is foreground data, and where is background.</p> <p>For more information on thresholding and how to set it, see the thresholding page.</p> <p>Here is the binary mask for minicircle.spm:</p> <p></p> <p>So you can see how all the interesting foreground (high) regions are now masked in white, and the background is in black.</p> <p>This allows TopoStats to use only the background (black pixels) in its calculations for slope removal, row alignment etc.</p> <p>So we re-do all the previous processing, but with this new useful binary mask to guide us.</p>"},{"location":"advanced/flattening/#secondary-flattening","title":"Secondary flattening","text":"<p>After re-processing the data using the mask to tell us where the background is, we get a better, more accurately flattened image. We can see the \"shadows\" on rows with lots of data have now been flattened correctly.</p> <p>From here, we can go on to do things like finding our objects of interest (grains) and get stats about them.</p> <p></p>"},{"location":"advanced/flattening/#gaussian-filter","title":"Gaussian filter","text":"<p>Finally, we apply a Gaussian filter to the image to smooth height differences and remove high-gain noise. This allows you to get smoother data but will start to blur out important features if you apply it too strongly. The default strength is a sigma of 1.0, but you can adjust this in the config file under <code>gaussian_size</code>. The <code>gaussian_mode</code> parameter suggests how values at the border should be handled, see skimage.filters.gaussian for more details.</p> <p>Here are some examples of different gaussian sizes:</p> <p></p>"},{"location":"advanced/grain_finding/","title":"Grain finding","text":""},{"location":"advanced/grain_finding/#at-a-glance-identifying-objects-of-interest","title":"At a Glance - Identifying Objects of Interest","text":"<p>TopoStats automatically tries to find grains (objects of interest) in your AFM images. There are several steps to this.</p> <ul> <li>Height thresholding: We find grains based on their height in the image.</li> <li>Remove edge grains: We remove grains that intersect the image border.</li> <li>Size thresholding: We remove grains that are too small or too large.</li> <li>Optional: U-Net mask improvement: We can use a U-Net to improve the mask of each grain.</li> </ul>"},{"location":"advanced/grain_finding/#height-thresholding","title":"Height thresholding","text":"<p>Grain finding is the process of detecting useful objects in your AFM images. This might be DNA, proteins, holes in a surface or ridges on a surface. In the standard operation of TopoStats, the way we find objects is based on a height threshold. This means that we detect where things are based on how high up they are.</p> <p>For example, with our example minicircles.spm image, we have DNA that is poking up from the sample surface, represented by bright regions in the image, alongside impurities and proteins, also above the surface:</p> <p></p> <p>If we want to select the DNA, then we can take only the regions of the image that are above a certain height threshold (standard deviation - <code>std_dev</code>, absolute - <code>absolute</code>, otsu - <code>otsu</code>).</p> <p>Here are several thresholds to show you what happens as we increase the absolute height threshold:</p> <p></p> <p>Notice that the amount of data decreases, until we are only left with the very highest points.</p> <p>The aim is to choose a threshold that keeps the data you want, while removing the background and other low objects that you don\u2019t want including. So in this example, a threshold of 0.5 would be best, since it keeps the DNA while removing the background.</p> <p>There are lots of objects in this mask that we don't want to analyse, but we can remove those using area thresholds in the next steps. These objects have been detectd because while they are small, they are still high up and above the background.</p> <p>For more information on the types of thresholding, and how to set them, see the thresholding page.</p>"},{"location":"advanced/grain_finding/#remove-edge-grains","title":"Remove edge grains","text":"<p>Some grains may intersect the image border. In these cases, the grain will not be able to have accuracte statistics calculated for it, since it is not fully in the image. Because of this, we have the option of removing grains that intersect the image border with the <code>remove_edge_intersecting_grains</code> flag in the config file. This simply removes any grains that intersect the image border.</p> <p>Here is a before and after example of removing edge grains:</p> <p></p>"},{"location":"advanced/grain_finding/#size-thresholding","title":"Size thresholding","text":"<p>In our thresholded image, you will notice that we have a lot of small grains that we do not want to analyse in our image. We can get rid of those with size thresholding. This is where TopoStats will remove grains based on their area, leaving only the right size of molecules. You will need to play around with the thresholds to get the right results.</p> <p>You can set the size threshold using the <code>absolute_area_threshold</code> in the config file. This sets the minimum and maximum area of the grains that you want to keep, in nanometers squared. Eg if you want to keep grains that are between 10nm^2 and 100nm^2, you would set <code>absolute_area_threshold</code> to <code>[10, 100]</code>.</p> <p></p>"},{"location":"advanced/grain_finding/#optional-u-net-mask-improvement","title":"Optional: U-Net mask improvement","text":"<p>As an additional optional step, each grain that reaches this stage can be improved by using a U-Net to mask the grain again. This requires a U-Net model path to be supplied in the config file.</p> <p>The U-Net model will take the bounding box of each grain, makes it square, and passees it to a trained U-Net model which makes a prediction for a better mask, which then replaces the original mask.</p> <p>Here is an example comparing absolute height thresholding to U-Net masking for one of our projects. The white boxes indicate regions where the height threhsold performs poorly and is improved by the U-Net mask.</p> <p></p>"},{"location":"advanced/grain_finding/#multi-class-masking","title":"Multi-class masking","text":"<p>TopoStats supports masking with multiple classes. This means that you could use a U-Net to mask DNA and proteins separately.</p> <p>This requires a U-Net that has been trained on multiple classes.</p> <p>Here is an example of multi-class masking using a U-Net which was used for one of our projects.</p> <p></p>"},{"location":"advanced/grain_finding/#technical-details","title":"Technical details","text":""},{"location":"advanced/grain_finding/#details-multi-class-masking","title":"Details: Multi-class masking","text":"<p>Multi class masking is implemented by having each image be a tensor of shape N x N x C, where N is the image size, and C is the number of classes. Each class is a binary mask, where 1 is the class, and 0 is not the class. The first channel is background, where 1 is background, and 0 is not background. The rest of the channels are arbitrary, and defined by how the U-Net was trained, however we conventially recommend that the first class be for DNA (if applicable) and the next classes for other objects.</p>"},{"location":"advanced/grainstats/","title":"Grainstats","text":""},{"location":"advanced/grainstats/#at-a-glance-measures-objects","title":"At a Glance - Measures Objects","text":"<p>TopoStats automatically tries to measure the grains (objects of interest) found in the grain finding section, in your AFM images, and outputs them into the <code>all_statistics.csv</code> file.</p> <p>The metrics are briefly summarised in the table below:</p> Column Name Description Data Type <code>center_x/y</code> The center of the grain. <code>float</code> <code>radius_min/max/mean/median</code> The distance from the center to each pixel on the perimeter. <code>float</code> <code>height_min/max/mean/median</code> The pixel values underlying the grain mask. <code>float</code> <code>area</code> The area of the pixel-wise grain mask. <code>float</code> <code>volume</code> Volume of the pixel-wise grain mask. <code>float</code> <code>area_cartesian_bbox</code> The area of a box bounding the grain along cardinal directions. <code>float</code> <code>smallest_bounding_width/length</code> The shortest bounding box length and perpendicular width of the grain in non-cardinal directions. <code>float</code> <code>smallest_bounding_area</code> The area of the smallest possible box bounding the grain. <code>float</code> <code>aspect_ratio</code> Ratio of the smallest bounding width to smallest bounding length. <code>float</code> <code>max/min_feret</code> The largest and shortest distance of the calipers rotating the grain between calipers. See feret diameter. <code>float</code> <p> </p> <p></p>"},{"location":"advanced/nodestats/","title":"NodeStats","text":"<p>This section gives an in-depth overview of the steps taken in the NodeStats module.</p>"},{"location":"advanced/nodestats/#at-a-glance-crossing-analyses","title":"At a Glance : Crossing Analyses","text":"<p>The <code>nodestats.py</code> module handles all the functions associated with identifying and analysing the crossing regions (nodes) and crossing branches in pruned skeletons.</p> <p>The quality of the resultant metrics and over/underlying branch classifications depend on the quality of the pruned skeleton, the effectiveness of automating the joining of skeleton junction points through the parameters.</p> <p></p> <p>This module identifies crossing regions from nearby skeleton junctions and analyses each branch emanating out from the crossing to pair them, then determines the overlying and underlying strand using the full-width half-maximum from each height trace passing through the crossing. It adds the number of identified crossings and the minimum and average pseudo confidence values to the <code>all_statistics.csv</code>.</p> <p>Some quick FYI's:</p> <ul> <li>Bad skeletons, bad classifications - If the skeleton does not lie along the backbone of the structure, it's   resulting height trace and thus stacking order calculation may not be accurate.</li> <li>No skeletons in image - The NodeStats <code>all</code> images may not show the resultant skeletons if the plotting <code>dpi</code>   value is too low (varies based on image size) as these single pixel lines cannot be resolved by matplotlib.</li> <li>Incorrect pairing - Pairing of the branches emanating from a crossing is based on the assumption that the regions   being compared (<code>branch_pairing_length</code>) are below half the persistence length of the material, and as such they   should follow straight lines through the crossing region.</li> <li>Masked colours are relative - Any mask colours that may be produced by plots are relative to the mask values in   that image as they will always span the masked colourmap, and will not compare well across images if the range of mask   values differ.</li> <li>Odd numbered crossings - These can result from poor masking / pruning and are handled by looking further around   odd nodes to join them (<code>node_extend_dist</code>), and if they are still odd, the default behaviour results from the   <code>pair_odd_branches</code> parameter to pair as many as possible leaving one remaining, or to not pair any of them.</li> </ul>"},{"location":"advanced/nodestats/#processing-steps","title":"Processing Steps","text":""},{"location":"advanced/nodestats/#1-identify-skeleton-junctions","title":"1. Identify Skeleton Junctions","text":"<p>The pruned skeletons undergo a 2D convolution with a 3x3 kernel of 1's, and the skeleton is the remapped onto the resultant image. This produces a non-binary skeleton where the value of each skeleton pixel is the count of it's neighbours + 1. This is used to produce a new skeleton image in with pixels labelled as:</p> <ul> <li>1 - Endpoints</li> <li>2 - Skeleton segments</li> <li>3 - Junctions</li> </ul> <p></p> <p>This produces a skeleton mask (blue) where the junctions (green) and endpoints (pink) can be seen.</p>"},{"location":"advanced/nodestats/#2-clean-up-the-crossing-regions","title":"2. Clean-up the Crossing Regions","text":"<p>The alignment of the skeleton onto the crossing backbone is key to obtaining good analyses from this module, especially for accurate topological classifications and calculation of writhe signs. For this reason, a small area around the junctions points are re-filled with the mask and skeletonised again using the height biasing skeletonisation approach.</p> <p></p> <p>This has been useful to align the skeletons at branch crossing points however, the modification to the skeleton has also been known to cause problems as no additional pruning steps are done.</p>"},{"location":"advanced/nodestats/#3-connect-the-junctions","title":"3. Connect the Junctions","text":"<p>Skeleton segments which represent a crossing may not join up perfectly at a single pixel (junction) and as a result of the skeletonisation procedure, may be offset from one another and need to be combined to represent the crossing region or \"node\". Therefore, junctions closer than the <code>node_joining_length</code> of each other define a crossing region, and the pixels which span between the junctions along the skeleton are also labelled as part of the crossing.</p> <p></p> <p>Depending on the sample type or skeletonisation / pruning errors, there might exist an odd number of emanating branches after this initial pairing. In DNA samples, we typically see this where just before a crossing region, the two strands lie close to each other, creating an elongated crossing region in the skeletonisation representation. In order to capture these for analysis, the <code>node_extend_dist</code> tells only odd-branch crossing regions to look for and extend to other odd-branch crossing regions within this distance, using the shortest path possible along the skeleton. Coming after the initial connection, this means that closer crossing regions can be joined, and that 3-branch nodes can be joined through a 4-branch node to another 3-branch node.</p>"},{"location":"advanced/nodestats/#4-pair-the-branches","title":"4. Pair the Branches","text":"<p>For each of these crossing regions, the skeletons are reduced to solely a single crossing and their emanating branches. The part we are interested in, and obtain traces for are branch regions defined as extending upto the <code>branch_pairing_length</code> away from the central crossing region. This length should be below half the persistence length of the material, so that the branches should follow (roughly) straight lines through the crossing region.</p> <p></p> <p>This enables us to pair emanating branches based on the angles between their vectors using bipartite matching to obtain the best pairing combinations to use for subsequent steps.</p>"},{"location":"advanced/nodestats/#5-height-traces","title":"5. Height Traces","text":"<p>For each paired branch in the crossing region, we can use a series of dilations to obtain two more accompanying branches to average our traces across a few pixels and reduce the error in the skeleton position along the backbone at the crossing. Using the underlying height values, and calculating the distance radially (not along the path) from the highest point in the crossing (the node centre), a height trace of the topographic crossing can be used to find the full-width half-maximum (FWHM) value of the crossing peak and determine which crossing branch lies atop (largest value) or beneath (smallest value) the other.</p> <p></p> <p>From the FWHM of the height traces, a pseudo confidence value is obtained using the equation below. It calculates the ratio of the minimum to maximum value across all FWHM pairs of all crossing branches, N, (using combinatorics to pair crossing branches if &gt;2):</p> \\[ \\text{crossing confidence} = \\frac{1}{N} \\sum_{\\text{FWHM\\_pairs}} \\left(1 - \\frac{\\min(\\text{FWHM\\_pairs})}{\\max(\\text{FWHM\\_pairs})}\\right) \\]"},{"location":"advanced/nodestats/#outputs","title":"Outputs","text":"<p>The <code>&lt;image&gt;_&lt;threshold&gt;_nodes</code> image shows the crossing regions highlighted in green, skeleton segments in blue, and endpoints in pink.</p> <p>For each grain, the following new columns are added to the <code>grainstats.csv</code> file:</p> Column Name Description Data Type <code>num_crossings</code> The number of crossing regions found in the grain. Note: this will be equal to or lower than the number of junctions explained in the previous section. <code>integer</code> <code>avg_crossing_confidence</code> The average of all pseudo crossing confidences. Used to estimate quality of predictions. <code>integer</code> <code>min_crossing_confidence</code> The minimum of all pseudo crossing confidences. Used to estimate quality of predictions. <code>float</code> <p> </p> <p></p> <p>Note: Most information obtained during the NodeStats processing can be obtained from the <code>&lt;image_name&gt;.topostats</code> file found within the <code>processed</code> folder and contains a multitude of grain and branch statistics such as:</p> <ul> <li>number of nodes per grain</li> <li>node confidences</li> <li>node coordinates</li> <li>number of branches per node</li> <li>branch distance array</li> <li>branch height array</li> <li>branch FWHM dictionary containing the FWHM, half max, and peak values</li> <li>matched and unmatched branch angles</li> </ul> <p> Note: The writhe sign is identified in <code>ordered_traces.py</code>, not <code>nodestats</code> as the path needs directionality provided by the ordering. It is added to the NodeStats dictionary as this carries the node statistics.</p>"},{"location":"advanced/nodestats/#diagnostic-images","title":"Diagnostic Images","text":"<p>Images produced by the <code>plotting.image_set: all</code> for this module are:</p> <ul> <li><code>25-convolved_skeleton</code> - The result of the convolution operation described in 1. The default \"blue_green_pink\"   colourmap for this image represent skeleton segments, junctions, and endpoints respectively.</li> <li><code>26-node_centres</code> - The highest pixel in each crossing region (green) with the same colourmap as above. This is where   the '0' distance point corresponds to in the height traces.</li> <li><code>nodes</code> folder:</li> <li><code>grain_&lt;X&gt;_node_&lt;Y&gt;_node_area_skeleton&gt;</code> - Contains the reduced area of grain number X and node number Y which     consists of the node in question and only it's emanating branches.</li> <li><code>grain_&lt;X&gt;_node_&lt;Y&gt;_node_branch_mask&gt;</code> - Visualises the crossing branches where the length depends on     <code>branch_pairing_length</code>.</li> <li><code>grain_&lt;X&gt;_node_&lt;Y&gt;_node_avg_mask&gt;</code> - Visualises the dilated crossing branches used to obtain an average height     trace along the crossing branches.</li> <li><code>grain_&lt;X&gt;_node_&lt;Y&gt;_linetrace_halfmax&gt;</code> - Shows the height trace of each crossing branch, and was used to determine     the FWHM. The vertical lines show the location of where the half-maximum was taken from.</li> </ul> <p></p>"},{"location":"advanced/nodestats/#possible-uses","title":"Possible Uses","text":"<p>This module would lend itself useful for measuring and quantifying complex overlapping structures, and is a requirement for the <code>nodestats</code> method in <code>ordered_tracing</code> which orders the trace along a complex topology.</p> <p>We have used this module to broadly quantify conformational differences between relaxed and supercoiled DNA samples based on the number of crossing regions seen in samples of each grain. Additionally the FWHM results have been used to confirm the classification topologically complex molecules via the highest confidence. The branch statistics have been used to identify and quantify different conformations of topologically complex DNA and the effect of surface deposition. Finally, splitting the odd-branched nodes enabled us to identify and measure the replicated and unreplicated DNA segments in replication intermediates despite their complex writhed path. These analyses can be seen in our paper; Under or Over? Tracing Complex DNA Topologies with High Resolution Atomic Force Microscopy.</p>"},{"location":"advanced/ordered_tracing/","title":"Ordered Tracing","text":"<p>This section gives an in-depth overview of the steps taken in the ordered tracing module.</p>"},{"location":"advanced/ordered_tracing/#at-a-glance-traversing-a-path","title":"At a Glance : Traversing a Path","text":"<p>The <code>ordered_tracing.py</code> module handles all the functions associated with ordering the pixels one-after-another from the disordered trace (pruned skeleton).</p> <p>The quality of the resultant metrics and ordered coordinates will depend on the ordering method chosen, whether the skeleton matches the conformation, and if selected, the performance (pairing) of the branch crossing points in NodeStats.</p> <p></p> <p>This module orders the disordered trace pixel-by-pixel (<code>topostats</code> method) or segment-by-segment (<code>nodestats</code> method), giving direction to the trace and creating a path to follow. It adds the number of identified molecules (found by restarting the trace when using the <code>nodestats</code> method) and whether the trace contains endpoints and is therefore circular or not to the <code>all_statistics.csv</code>.</p> <p>Some quick FYI's:</p> <ul> <li>Multiple molecules - By design the <code>topostats</code> ordering method will only produce a single ordered trace per grain,   however, the <code>nodestats</code> method may identify multiple (useful for separating overlapping structures etc).</li> <li>No skeletons in image - The Ordered Traces <code>all</code> image set may not show the resultant skeletons if the plotting   <code>dpi</code> value is too low (varies based on image size) as these single pixel lines cannot be resolved by matplotlib.</li> <li>Circular field not circular - The grain stats (and molecule stats) field labelled \"circular\" does not check if the   path starts and ends at the same place. Instead, it is a True/False indicator of the presence of any endpoints.</li> <li>Failed NodeStats ordering - If the \"nodestats\" method is selected in the configurations file and an error occurs   in the NodeStats module for a particular grain, and that grain isn't added to the noodestats dictionary, it will   default to \"topostats\" ordering.</li> <li>Overlapping traces - Because the crossing region is shared by both crossing strands, the pixels in the mask may   show a discontinuity in one \"leg\" of the crossing in the ordered trace image, this is purely visual and does not   affect the underlying data.</li> </ul>"},{"location":"advanced/ordered_tracing/#processing-steps","title":"Processing Steps","text":""},{"location":"advanced/ordered_tracing/#nodestats-method","title":"NodeStats Method","text":""},{"location":"advanced/ordered_tracing/#1-compile-segments","title":"1. Compile Segments","text":"<p>The first stage is to use the branch coordinates and disordered trace skeleton to compile a labelled image and associated set of coordinates and their stacking orders. To do this, the crossing branch coordinates are removed from the disordered trace and all remaining segments labelled 1 to N. Then, branch-by-branch the crossing coordinates are replaced with following labels, resulting in an image where each skeleton and crossing segment are labelled.</p> <p></p>"},{"location":"advanced/ordered_tracing/#2-the-nodestats-tracing-loop","title":"2. The NodeStats Tracing Loop","text":"<p>The trace aims to use the labelled segments described above to link to the full set of coordinates and build, segment-by-segment, an ordered path.</p> <p>It starts by looking for any endpoints, and uses the index of the endpoint segment to start the trace, if there are no endpoints, the starting index will all ways be the first labelled index (1 in the image, 0 in the coordinate list) - this also has the advantage of starting from a non-crossing segment. The coordinates are added to the ordered trace and the currently indexed segment is then removed from the guide image containing all the segments. Then, the maximum value found in a 3x3 area around the final coordinate in the ordered trace is used to identify the following trace segment. This repeats until a segment terminates and the only values in that 3x3 area are 0's (background).</p> <p></p> <p>If there are no more remaining segments, the trace is finished. However, if any segments remain, a new ordered trace is started, again from an endpoint segment if possible, hence why this method is able to untangle two interlinked molecules such as DNA catenanes.</p> <p> *Skeletons dilated for visual aid</p>"},{"location":"advanced/ordered_tracing/#22-a-simple-trace","title":"2.2. A Simple Trace","text":"<p>While the above is happening, ordered trace segment coordinates are also being added to a simplified ordered trace to be used with the Topolypackage to determine it's topological species. A pseudo Z (height) is used to distinguish between skeleton segments (Z=0) and crossing segments (Z=-1, 1, 2...). However, these traces (N, X, Y, Z) can be too long and cause Topoly to hang, so they are reduced to ~100 coordinates. Each reduced points of each segments ensure the points capture the start and end of each segment.</p> <p></p> <p>Topoly then uses this NXYZ coordinate trace to produce a topological classification using Homfly polynomials. In addition, another trace is built up which flips the stacking order of the lowest confidence crossing to generate a secondary topology.</p> <ul> <li>Future work may aim to ensure these reduced segments don't accidentally cross.</li> </ul>"},{"location":"advanced/ordered_tracing/#23-writhe-sign","title":"2.3. Writhe Sign","text":"<p>The ordered trace and crossing branch coordinates are also used to calculate the writhe of each crossing via the cross product of the branches normalised vectors, with their direction following the path of the trace. The writhe sign is calculated as \"+\" or \"-\". If a crossing contains &gt; 2 crossing branches, the single crossing region is split into pairs and the writhe calculated in brackets i.e. \"+(-++)\"</p> <p></p> <p>For overlapping molecules, the writhe signs are still calculated but this is greatly dependent on the directionality of each molecule. In the case of DNA catenanes, a positive or negative writhe would depend on the relative direction of each trace. In these cases, the writhe string should only be used to observe relative writhe signs i.e. \"++-+\" equals \"--+-\", as directionality cannot be observed in AFM images.</p> <ul> <li>Future work may be to separate self and intra-molecule writhes.</li> </ul>"},{"location":"advanced/ordered_tracing/#topostats-method","title":"TopoStats Method","text":""},{"location":"advanced/ordered_tracing/#1-classifying-circularity","title":"1. Classifying Circularity","text":"<p>The TopoStats tracing algorithms slightly differ depending on whether the trace is circular or not. The disordered trace is defined as circular by counting the number of neighbours of each pixel in the skeleton. If a skeleton contains at least one skeleton with only one neighbour it is defined as non-circular.</p> <p></p>"},{"location":"advanced/ordered_tracing/#2-circular-and-linear-topostats-tracing","title":"2. Circular and Linear TopoStats Tracing","text":"<p>If the disordered trace is not deemed circular, the trace starts by first finding and selecting an endpoint (pixel with one neighbour).</p> <p>If the disordered trace is deemed circular, the trace starts by first finding and selecting a point with two neighbours. Then, the selected coordinate is added to the ordered path, and deleted from the skeleton. One of the neighbouring points are randomly selected.</p> <p>The selected point is then added to the path and deleted from the skeleton enabling the path to then follow the sole remaining pixel to traverse the path. However, above assumes there is always only one pixel to follow around the path, but if there are more, the trace will look up to 4 pixels earlier in the ordered trace, and try to identify the next pixel which has the lowest angular change.</p> <p></p>"},{"location":"advanced/ordered_tracing/#outputs","title":"Outputs","text":"<p>The <code>&lt;image&gt;_&lt;threshold&gt;_ordered_traces</code> image shows the direction of ordered coordinates.</p> <p>For each grain, the following new columns are added to the <code>grainstats.csv</code> file:</p> Column Name Description Data Type <code>num_molecules</code> The number of molecules found by following the tracing paths. Note: This will always be 1 for the TopoStats method. <code>integer</code> <code>circular</code> Whether the disordered trace contains an endpoint. <code>bool</code> <code>writhe_string</code> The writhe sign (+/-) which describes the crossing directionality. If a crossing contains &gt; 2 crossing branches, the single crossing region is split into pairs and the writhe calculated in brackets i.e. \"+(-++)\". <code>str</code> <p> </p> <p></p> <p>For each molecule found by the ordering algorithm(s), the following new columns are added to the <code>molstats.csv</code> file:</p> Column Name Description Data Type <code>circular</code> The number of molecules found by following the tracing paths. Note: This will always be 1 for the TopoStats method. <code>integer</code> <code>topology</code> Whether the disordered trace contains an endpoint. <code>bool</code> <code>topology_flip</code> The number of molecules found by following the tracing paths. Note: This will always be 1 for the TopoStats method. <code>integer</code> <code>processing</code> The method used for ordering. <code>str</code> <p> </p> <p></p> <p>Note: Most information obtained during the Ordered Tracing processing can be obtained from the <code>&lt;image_name&gt;.topostats</code> file found within the <code>processed</code> folder and contains a multitude of molecule related objects such as:</p> <ul> <li>bounding box</li> <li>cumulative distance along the trace</li> <li>height along the trace</li> <li>the ordered coordinates</li> <li>molecule statistics explained above</li> <li>The writhe sign for each crossing is added to the NodeStats dictionary.</li> </ul> <p></p>"},{"location":"advanced/ordered_tracing/#diagnostic-images","title":"Diagnostic Images","text":"<p>Images produced by the <code>plotting.image_set: all</code> for this module are:</p> <ul> <li><code>27-trace_segments</code> - The labelled skeleton and crossing segments that the NodeStats trace is built from.</li> <li><code>28-all_molecule_crossings</code> - The skeleton, underlying and overlying crossing segments in blue, pink and green   respectively. The colourmap used is three-tone meaning that middle crossing branches when there are more than two   branches will appear blue too.</li> <li><code>29-all_molecules</code> - A labelled image of all the identified molecules in the grain found by the ordering method. In   the default colourmap, only three traces will show - blue, green, and pink.</li> </ul> <p></p>"},{"location":"advanced/ordered_tracing/#possible-uses","title":"Possible Uses","text":"<p>This module would lend itself useful for separating overlapping or intertwined structures, and is a requirement for the <code>splining</code> module which smooths the ordered traces to obtain contour lengths of individual molecules.</p> <p>Additionally, you could use this module and the writhe string obtained to assess the supercoiling degree (via writhe only, not twist) in DNA samples as well as a preliminary step to obtaining the lengths of the individual molecules.</p>"},{"location":"advanced/splining/","title":"Splining","text":"<p>This section gives an in-depth overview of the steps taken in the splining module.</p>"},{"location":"advanced/splining/#at-a-glance-smoothing-pixelated-traces","title":"At a Glance : Smoothing Pixelated Traces","text":"<p>The <code>splining.py</code> module handles all the functions associated with smoothing the ordered pixel-wise trace from the \"ordered_tracing\" step, in order to produce curves which more closely follow the samples structure.</p> <p>The quality of the resultant metrics and smoothed coordinates will depend on the splining method chosen, whether the ordering worked successfully, and whether the skeleton matches the underlying sample conformation.</p> <p></p> <p>This smooths the ordered trace by using an average of splines through the ordered coordinates (<code>spline</code> method) or using the mean coordinate of a rolling window (<code>rolling_window</code> method), helping to resolve length errors in jagged in the skeletons. It adds the contour length and end-to-end euclidean distance to the <code>all_mol_statistics.csv</code> and the sum and average of these respectively to the <code>all_statistics.csv</code>.</p> <p>Some quick FYI's:</p> <ul> <li>Constricted Traces - Using the <code>rolling_window</code> method with a rolling window size that is not far below the   persistence length can result in over-smoothing where the spline is dragged away from regions of high curvature.</li> <li>Unrepresentative Splines - The <code>spline</code> method's parameters can be quite temperamental so it's recommended that   <code>spline_linear_smoothing</code>, <code>spline_circular_smoothing</code> and <code>spline_degree</code> are not changed unless understood.</li> <li>Unrepresentative Splines 2 - The <code>spline</code> method may produce unwanted results if the <code>spline_step_size</code> is too   long. This is because the way the splines are averaged assumes that each spline point is sampled close to that of   another's.</li> </ul>"},{"location":"advanced/splining/#processing-steps","title":"Processing Steps","text":""},{"location":"advanced/splining/#splining-method","title":"Splining Method","text":""},{"location":"advanced/splining/#1-trace-coordinate-subsets","title":"1. Trace Coordinate Subsets","text":"<p>The first stage is to use the ordered coordinates and the <code>spline_step_size</code> parameter to define how many splines we want to average together. As the <code>spline_step_size</code> is the distance (in nm) at which to take every i'th coordinate for its spline. This means the number of splines to average is calculated below:</p> \\[ N = max(\\text{spline step size} \\div \\text{px to nm}, 1) \\] <p>The coordinates used for each spline are then obtained from the initial ordered trace, i.e. the spline coordinate indexes: [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4], where spline 1 takes every 4th coordinate, starting at position 0, then spline 2 takes every 4th coordinate starting at position 1, etc.</p> <p></p>"},{"location":"advanced/splining/#2-obtaining-splines","title":"2. Obtaining Splines","text":"<p>For each of these coordinate subsets, we use the Sci-Py library to find the B-Spline representation of the 2D curve using the smoothness parameters <code>spline_linear_smoothing</code>, <code>spline_circular_smoothing</code>, and the <code>spline_degree</code>, found in the configuration file. The <code>spline_linear_smoothing</code> and <code>spline_circular_smoothing</code> define the degrees of smoothing for circular or linear molecules obtained in the Ordered Tracing step, which also tells TopoStats whether to make the spline periodic if circular. Larger smoothing values mean more smoothing and smaller values indicate less smoothing. The <code>spline_degree</code> should take odd values, or even with large smoothing. Read more on the Sci-Py documentation above.</p> <p></p> <p>These splines are then averaged together via their index i.e. first spline coordinates are averaged, then second etc... until a single averaged B-Spline remain. However, because the splined are averaged on their index, it is assumed that each index is within close proximity, thus, the <code>spline_step_size</code> must be small enough for this assumption to hold true and for the splining to work as intended.</p>"},{"location":"advanced/splining/#rolling-window-method","title":"Rolling Window Method","text":""},{"location":"advanced/splining/#1-rolling-average","title":"1. Rolling Average","text":"<p>This method simply uses a rolling average of the ordered trace coordinates within the <code>rolling_window_size</code> to produce the new smoothed trace. The rolling window will move one coordinate along each time, and continues until the start of the window returns back to its initial position.</p> <p></p> <p>For linear smooth traces, the same as above occurs however, the initial and final coordinates are also added to the smoothed trace as to not drastically reduce the length of the smoothed trace.</p> <p></p>"},{"location":"advanced/splining/#outputs","title":"Outputs","text":"<p>The <code>&lt;image&gt;_&lt;threshold&gt;_ordered_traces</code> image shows the direction of ordered coordinates.</p> <p>For each grain, the following new columns are added to the <code>grainstats.csv</code> file:</p> Column Name Description Data Type <code>total_contour_length</code> The total length along the splined trace of all identified molecules. <code>float</code> <code>average_end_to_end_distance</code> The average distance from two endpoints of the spline of all identified linear molecules. <code>float</code> <p> </p> <p></p> <p>For each molecule found by the ordering algorithm(s), the following new columns are added to the <code>molstats.csv</code> file:</p> Column Name Description Data Type <code>contour_length</code> The length along the splined trace of the molecule. <code>float</code> <code>end_to_end_distance</code> The distance from two endpoints of the spline of the linear molecule. <code>float</code> <p> </p> <p></p> <p>Note: Most information obtained during the Splining processing can be obtained from the <code>&lt;image_name&gt;.topostats</code> file found within the <code>processed</code> folder and contains a multitude of molecule related objects such as:</p> <ul> <li>bounding box</li> <li>spline coordinates</li> <li>tracing statistics of contour length and end to end distance</li> </ul> <p></p>"},{"location":"advanced/splining/#diagnostic-images","title":"Diagnostic Images","text":"<p>There are no diagnostic images produced in this step.</p>"},{"location":"advanced/splining/#possible-uses","title":"Possible Uses","text":"<p>This module would lend itself useful for accurately measuring the lengths of complex objects within samples, and obtaining an accurate representation of the underlying conformation of the sample.</p> <p>We have used this module to accurately measure the length of topologically complex DNA samples such as knots, catenanes, and theata-curves (replication intermediates). Additionally, we've used the end-to-end distance to measure the conformational variation of linear DNA in the presence of NDP-52 to show that it may also play a role in topological regulation. See more here..</p>"},{"location":"advanced/thresholding/","title":"Thresholding","text":"<p>When flattening images and finding grains, TopoStats uses thresholding to separate the background data from the foreground data. This is done by setting a threshold value, and classifying all pixels above this value as foreground, and all pixels below this value as background.</p> <p>There are several different types of thresholding that can be used, and each has its own advantages and disadvantages.</p> <p>Below is a histogram showing the heights of the pixels in minicircle.spm after flattening. You can see that most of the pixels are at a height of 0nm, which is the background. There is a second peak at 2.5nm which is around the height that we expect DNA to be. The rest of the pixels are noise.</p> <p>A threshold will select all pixels above a certain height, and ignore the rest, ie the pixels in the orange area.</p> <p></p>"},{"location":"advanced/thresholding/#note-thresholding-above-and-below-the-surface","title":"Note: Thresholding above and below the surface","text":"<p>TopoStats has the ability to threshold both above the sample surface and below it. This allows finding grains on the surface but also holes in the surface (useful for silicon wafer analysis). This can be configured by setting the corresponding \"above\" or \"below\" thresholds in the config file. Eg if you only want to find grains above the surface, only use the \"above\" threshold options, and vice-versa.</p>"},{"location":"advanced/thresholding/#thresholding-types","title":"Thresholding types","text":""},{"location":"advanced/thresholding/#standard-deviation-thresholding","title":"Standard deviation thresholding","text":"<p>Standard deviation thresholding is a simple method of thresholding that uses the standard deviation of the image to determine the threshold value. The threshold value is calculated as:</p> \\[ \\text{threshold} = \\text{mean} + \\text{std\\_dev} \\times \\text{factor} \\] <p>Where <code>mean</code> is the mean of the image, <code>std_dev</code> is the standard deviation of the image, and <code>factor</code> is a user-defined value that determines how many standard deviations above the mean the threshold should be.</p> <p>This method is useful when you don't know the exact threshold value you want to use, and when you have a bit of noise in your image.</p>"},{"location":"advanced/thresholding/#otsu-thresholding","title":"Otsu thresholding","text":"<p>Otsu thresholding is an automatic thresholding method that tries to find the threshold value that minimizes the intra-class variance of the foreground and background pixels.</p> <p>We have added a multiplier to the Otsu thresholding method to allow for a more flexible thresholding method. The threshold value is calculated as:</p> \\[ \\text{threshold} = \\text{otsu} \\times \\text{factor} \\] <p>Where <code>otsu</code> is the threshold value calculated by the Otsu method, and <code>factor</code> is a user-defined value that allows you to adjust the threshold value.</p> <p>This method is useful when you want to automatically find the threshold value, and when you have a clear binomial distribution of pixels (heights). I.e. separation between the foreground and background pixels in your image with little noise.</p>"},{"location":"advanced/thresholding/#absolute-thresholding","title":"Absolute thresholding","text":"<p>Absolute thresholding is a simple method of thresholding that uses a user-defined threshold value to separate the foreground and background pixels.</p> <p>This method is useful when you know the exact threshold value you want to use, for example if you know your DNA lies at 2nm above the surface you can set the threshold to 1.5nm to capture the DNA without capturing the background.</p>"},{"location":"api/","title":"API","text":"<ul> <li><code>entry_point.py</code></li> <li><code>filters.py</code></li> <li><code>grains.py</code></li> <li><code>grainstats.py</code></li> <li><code>io.py</code></li> <li><code>plotting</code></li> <li><code>plottingfuncs</code></li> <li><code>run_topostats</code></li> <li><code>scars</code></li> <li><code>statistics</code></li> <li><code>theme</code></li> <li><code>thresholds</code></li> <li><code>utils</code></li> <li><code>validation</code></li> </ul>"},{"location":"api/entry_point/","title":"Entry Point Modules","text":"<p>Entry point for all TopoStats programs.</p> <p>Parses command-line arguments and passes input on to the relevant functions / modules.</p>"},{"location":"api/entry_point/#topostats.entry_point.create_legacy_run_topostats_parser","title":"<code>create_legacy_run_topostats_parser() -&gt; arg.ArgumentParser</code>","text":"<p>Create a parser reading options for the 'run_topostats' processing entry point.</p> Source code in <code>topostats\\entry_point.py</code> <pre><code>def create_legacy_run_topostats_parser() -&gt; arg.ArgumentParser:\n    \"\"\"Create a parser reading options for the 'run_topostats' processing entry point.\"\"\"\n    parser = arg.ArgumentParser(\n        description=\"Process AFM images. Additional arguments over-ride those in the configuration file.\"\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--config_file\",\n        dest=\"config_file\",\n        required=False,\n        help=\"Path to a YAML configuration file.\",\n    )\n    parser.add_argument(\n        \"-s\",\n        \"--summary_config\",\n        dest=\"summary_config\",\n        required=False,\n        help=\"Path to a YAML configuration file for summary plots and statistics.\",\n    )\n    parser.add_argument(\n        \"-b\",\n        \"--base_dir\",\n        dest=\"base_dir\",\n        type=str,\n        required=False,\n        help=\"Base directory to scan for images.\",\n    )\n    parser.add_argument(\n        \"-j\",\n        \"--cores\",\n        dest=\"cores\",\n        type=int,\n        required=False,\n        help=\"Number of CPU cores to use when processing.\",\n    )\n    parser.add_argument(\n        \"-l\",\n        \"--log_level\",\n        dest=\"log_level\",\n        type=str,\n        required=False,\n        help=\"Logging level to use, default is 'info' for verbose output use 'debug'.\",\n    )\n    parser.add_argument(\n        \"-f\",\n        \"--file_ext\",\n        dest=\"file_ext\",\n        type=str,\n        required=False,\n        help=\"File extension to scan for.\",\n    )\n    parser.add_argument(\n        \"--channel\",\n        dest=\"channel\",\n        type=str,\n        required=False,\n        help=\"Channel to extract.\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output_dir\",\n        dest=\"output_dir\",\n        type=str,\n        required=False,\n        help=\"Output directory to write results to.\",\n    )\n    parser.add_argument(\n        \"--save_plots\",\n        dest=\"save_plots\",\n        type=bool,\n        required=False,\n        help=\"Whether to save plots.\",\n    )\n    parser.add_argument(\"-m\", \"--mask\", dest=\"mask\", type=bool, required=False, help=\"Mask the image.\")\n    parser.add_argument(\n        \"-v\",\n        \"--version\",\n        action=\"version\",\n        version=f\"Installed version of TopoStats : {__version__}\",\n        help=\"Report the current version of TopoStats that is installed.\",\n    )\n    parser.add_argument(\n        \"-w\",\n        \"--warnings\",\n        dest=\"warnings\",\n        type=bool,\n        required=False,\n        help=\"Whether to ignore warnings.\",\n    )\n    return parser\n</code></pre>"},{"location":"api/entry_point/#topostats.entry_point.create_legacy_toposum_parser","title":"<code>create_legacy_toposum_parser() -&gt; arg.ArgumentParser</code>","text":"<p>Create a parser reading options for the legacy 'toposum' summarize entry point.</p> Source code in <code>topostats\\entry_point.py</code> <pre><code>def create_legacy_toposum_parser() -&gt; arg.ArgumentParser:\n    \"\"\"Create a parser reading options for the legacy 'toposum' summarize entry point.\"\"\"\n    parser = arg.ArgumentParser(\n        description=\"Summarise and plot histograms, kernel density estimates and scatter plots of TopoStats\"\n        \"grain and DNA Tracing statistics.\"\n    )\n    parser.add_argument(\"-i\", \"--input_csv\", dest=\"csv_file\", required=False, help=\"Path to CSV file to plot.\")\n    parser.add_argument(\n        \"-c\",\n        \"--config_file\",\n        dest=\"config_file\",\n        required=False,\n        help=\"Path to a YAML configuration file.\",\n    )\n    parser.add_argument(\n        \"-l\",\n        \"--var_to_label\",\n        dest=\"var_to_label\",\n        required=False,\n        help=\"Path to a YAML plotting dictionary that maps variable names to labels.\",\n    )\n    parser.add_argument(\n        \"--create-config-file\",\n        dest=\"create_config_file\",\n        type=str,\n        required=False,\n        help=\"Filename to write a sample YAML configuration file to (should end in '.yaml').\",\n    )\n    parser.add_argument(\n        \"--create-label-file\",\n        dest=\"create_label_file\",\n        type=str,\n        required=False,\n        help=\"Filename to write a sample YAML label file to (should end in '.yaml').\",\n    )\n    return parser\n</code></pre>"},{"location":"api/entry_point/#topostats.entry_point.create_parser","title":"<code>create_parser() -&gt; arg.ArgumentParser</code>","text":"<p>Create a parser for reading options.</p> Source code in <code>topostats\\entry_point.py</code> <pre><code>def create_parser() -&gt; arg.ArgumentParser:\n    \"\"\"Create a parser for reading options.\"\"\"\n    parser = arg.ArgumentParser(\n        description=\"Run various programs relating to AFM data. Add the name of the program you wish to run.\"\n    )\n    parser.add_argument(\n        \"-v\",\n        \"--version\",\n        action=\"version\",\n        version=f\"Installed version of TopoStats: {__version__}\",\n        help=\"Report the current version of TopoStats that is installed\",\n    )\n\n    subparsers = parser.add_subparsers(title=\"program\", description=\"Available programs, listed below:\", dest=\"program\")\n\n    # Create a sub-parsers for different stages of processing and tasks\n    process_parser = subparsers.add_parser(\n        \"process\",\n        description=\"Process AFM images. Additional arguments over-ride defaults or those in the configuration file.\",\n        help=\"Process AFM images. Additional arguments over-ride defaults or those in the configuration file.\",\n    )\n    process_parser.add_argument(\n        \"-c\",\n        \"--config-file\",\n        dest=\"config_file\",\n        required=False,\n        help=\"Path to a YAML configuration file.\",\n    )\n    process_parser.add_argument(\n        \"-s\",\n        \"--summary-config\",\n        dest=\"summary_config\",\n        required=False,\n        help=\"Path to a YAML configuration file for summary plots and statistics.\",\n    )\n    process_parser.add_argument(\n        \"--matplotlibrc\",\n        dest=\"matplotlibrc\",\n        required=False,\n        help=\"Path to a matplotlibrc file.\",\n    )\n    process_parser.add_argument(\n        \"-b\",\n        \"--base-dir\",\n        dest=\"base_dir\",\n        type=str,\n        required=False,\n        help=\"Base directory to scan for images.\",\n    )\n    process_parser.add_argument(\n        \"-j\",\n        \"--cores\",\n        dest=\"cores\",\n        type=int,\n        required=False,\n        help=\"Number of CPU cores to use when processing.\",\n    )\n    process_parser.add_argument(\n        \"-l\",\n        \"--log-level\",\n        dest=\"log_level\",\n        type=str,\n        required=False,\n        help=\"Logging level to use, default is 'info' for verbose output use 'debug'.\",\n    )\n    process_parser.add_argument(\n        \"-f\",\n        \"--file-ext\",\n        dest=\"file_ext\",\n        type=str,\n        required=False,\n        help=\"File extension to scan for.\",\n    )\n    process_parser.add_argument(\n        \"--channel\",\n        dest=\"channel\",\n        type=str,\n        required=False,\n        help=\"Channel to extract.\",\n    )\n    process_parser.add_argument(\n        \"-o\",\n        \"--output-dir\",\n        dest=\"output_dir\",\n        type=str,\n        required=False,\n        help=\"Output directory to write results to.\",\n    )\n    process_parser.add_argument(\n        \"--save-plots\",\n        dest=\"save_plots\",\n        type=bool,\n        required=False,\n        help=\"Whether to save plots.\",\n    )\n    process_parser.add_argument(\n        \"--savefig-format\",\n        dest=\"savefig_format\",\n        type=str,\n        required=False,\n        help=\"Format for saving figures to, options are 'png', 'svg', or other valid Matplotlib supported formats.\",\n    )\n    process_parser.add_argument(\n        \"--savefig-dpi\",\n        dest=\"savefig_dpi\",\n        type=int,\n        required=False,\n        help=\"Dots Per Inch for plots, should be integer for dots per inch.\",\n    )\n    process_parser.add_argument(\n        \"--cmap\",\n        dest=\"cmap\",\n        type=str,\n        required=False,\n        help=\"Colormap to use, options include 'nanoscope', 'afmhot' and any valid Matplotlib colormap.\",\n    )\n    process_parser.add_argument(\"-m\", \"--mask\", dest=\"mask\", type=bool, required=False, help=\"Mask the image.\")\n    process_parser.add_argument(\n        \"-w\",\n        \"--warnings\",\n        dest=\"warnings\",\n        type=bool,\n        required=False,\n        help=\"Whether to ignore warnings.\",\n    )\n    process_parser.set_defaults(func=run_topostats)\n\n    toposum_parser = subparsers.add_parser(\n        \"summary\",\n        description=\"Plotting and summary of TopoStats output statistics.\",\n        help=\"Plotting and summary of TopoStats output statistics.\",\n    )\n    toposum_parser.add_argument(\"-i\", \"--input_csv\", dest=\"csv_file\", required=False, help=\"Path to CSV file to plot.\")\n    toposum_parser.add_argument(\n        \"-c\",\n        \"--config-file\",\n        dest=\"config_file\",\n        required=False,\n        help=\"Path to a YAML plotting dictionary that maps variable names to labels.\",\n    )\n    toposum_parser.add_argument(\n        \"-l\",\n        \"--var-to-label\",\n        dest=\"var_to_label\",\n        required=False,\n        help=\"Path to a YAML plotting dictionary that maps variable names to labels.\",\n    )\n    toposum_parser.add_argument(\n        \"--create-config-file\",\n        dest=\"create_config_file\",\n        type=str,\n        required=False,\n        help=\"Filename to write a sample YAML configuration file to (should end in '.yaml').\",\n    )\n    toposum_parser.add_argument(\n        \"--create-label-file\",\n        dest=\"create_label_file\",\n        type=str,\n        required=False,\n        help=\"Filename to write a sample YAML label file to (should end in '.yaml').\",\n    )\n    toposum_parser.add_argument(\n        \"--savefig-format\",\n        dest=\"savefig_format\",\n        type=str,\n        required=False,\n        help=\"Format for saving figures to, options are 'png', 'svg', or other valid Matplotlib supported formats.\",\n    )\n    toposum_parser.set_defaults(func=run_toposum)\n\n    load_parser = subparsers.add_parser(\n        \"load\",\n        description=\"Load and save all images as .topostats files for subsequent processing.\",\n        help=\"Load and save all images as .topostats files for subsequent processing.\",\n    )\n    load_parser.add_argument(\n        \"-c\",\n        \"--config-file\",\n        dest=\"config_file\",\n        required=False,\n        help=\"Path to a YAML configuration file.\",\n    )\n\n    filter_parser = subparsers.add_parser(\n        \"filter\",\n        description=\"Load and filter images, saving as .topostats files for subsequent processing.\",\n        help=\"Load and filter images, saving as .topostats files for subsequent processing.\",\n    )\n    filter_parser.add_argument(\n        \"-c\",\n        \"--config-file\",\n        dest=\"config_file\",\n        required=False,\n        help=\"Path to a YAML configuration file.\",\n    )\n\n    grain_parser = subparsers.add_parser(\n        \"grains\",\n        description=\"Load filtered images from '.topostats' files and detect grains.\",\n        help=\"Load filtered images from '.topostats' files and detect grains.\",\n    )\n    grain_parser.add_argument(\n        \"-c\",\n        \"--config-file\",\n        dest=\"config_file\",\n        required=False,\n        help=\"Path to a YAML configuration file.\",\n    )\n\n    grainstats_parser = subparsers.add_parser(\n        \"grainstats\",\n        description=\"Load images with grains from '.topostats' files and calculate statistics.\",\n        help=\"Load images with grains from '.topostats' files and calculate statistics.\",\n    )\n    grainstats_parser.add_argument(\n        \"-c\",\n        \"--config-file\",\n        dest=\"config_file\",\n        required=False,\n        help=\"Path to a YAML configuration file.\",\n    )\n\n    dnatracing_parser = subparsers.add_parser(\n        \"dnatracing\",\n        description=\"Load images with grains from '.topostats' files and trace DNA molecules.\",\n        help=\"Load images with grains from '.topostats' files and trace DNA molecules.\",\n    )\n    dnatracing_parser.add_argument(\n        \"-c\",\n        \"--config-file\",\n        dest=\"config_file\",\n        required=False,\n        help=\"Path to a YAML configuration file.\",\n    )\n\n    tracingstats_parser = subparsers.add_parser(\n        \"tracingstats\",\n        description=\"Load images with grains from '.topostats' files and trace DNA molecules.\",\n        help=\"Load images with grains from '.topostats' files and trace DNA molecules.\",\n    )\n    tracingstats_parser.add_argument(\n        \"-c\",\n        \"--config-file\",\n        dest=\"config_file\",\n        required=False,\n        help=\"Path to a YAML configuration file.\",\n    )\n\n    create_config_parser = subparsers.add_parser(\n        \"create-config\",\n        description=\"Create a configuration file using the defaults.\",\n        help=\"Create a configuration file using the defaults.\",\n    )\n    create_config_parser.add_argument(\n        \"-f\",\n        \"--filename\",\n        dest=\"filename\",\n        required=False,\n        default=\"config.yaml\",\n        help=\"Name of YAML file to save configuration to (default 'config.yaml').\",\n    )\n    create_config_parser.add_argument(\n        \"-o\",\n        \"--output-dir\",\n        dest=\"output_dir\",\n        required=False,\n        default=\"./\",\n        help=\"Path to where the YAML file should be saved (default './' the current directory).\",\n    )\n    create_config_parser.add_argument(\n        \"-c\",\n        \"--config\",\n        dest=\"config\",\n        default=None,\n        help=\"Configuration to use, currently only one is supported, the 'default'.\",\n    )\n    create_config_parser.set_defaults(func=write_config_with_comments)\n\n    create_matplotlibrc_parser = subparsers.add_parser(\n        \"create-matplotlibrc\",\n        description=\"Create a Matplotlibrc parameters file.\",\n        help=\"Create a Matplotlibrc parameters file using the defaults.\",\n    )\n    create_matplotlibrc_parser.add_argument(\n        \"-f\",\n        \"--filename\",\n        dest=\"filename\",\n        required=False,\n        default=\"topostats.mplstyle\",\n        help=\"Name of file to save Matplotlibrc configuration to (default 'topostats.mplstyle').\",\n    )\n    create_matplotlibrc_parser.add_argument(\n        \"-o\",\n        \"--output-dir\",\n        dest=\"output_dir\",\n        required=False,\n        default=\"./\",\n        help=\"Path to where the YAML file should be saved (default './' the current directory).\",\n    )\n    create_matplotlibrc_parser.add_argument(\n        \"-c\",\n        \"--config\",\n        dest=\"config\",\n        default=\"topostats.mplstyle\",\n        help=\"Matplotlibrc style file to use, currently only one is supported, the 'topostats.mplstyle'.\",\n    )\n    create_matplotlibrc_parser.set_defaults(func=write_config_with_comments)\n\n    return parser\n</code></pre>"},{"location":"api/entry_point/#topostats.entry_point.entry_point","title":"<code>entry_point(manually_provided_args=None, testing=False) -&gt; None</code>","text":"<p>Entry point for all TopoStats programs.</p> Source code in <code>topostats\\entry_point.py</code> <pre><code>def entry_point(manually_provided_args=None, testing=False) -&gt; None:\n    \"\"\"Entry point for all TopoStats programs.\"\"\"\n    # Parse command line options, load config (or default) and update with command line options\n    parser = create_parser()\n    args = parser.parse_args() if manually_provided_args is None else parser.parse_args(manually_provided_args)\n\n    # No program specified, print help and exit\n    if not args.program:\n        parser.print_help()\n        sys.exit()\n\n    if testing:\n        return args\n\n    # call the relevant function\n    args.func(args)\n\n    return None\n</code></pre>"},{"location":"api/entry_point/#topostats.entry_point.legacy_run_topostats_entry_point","title":"<code>legacy_run_topostats_entry_point(args=None, testing=False) -&gt; None</code>","text":"<p>Legacy entry point for the run_topostats processing function.</p> Source code in <code>topostats\\entry_point.py</code> <pre><code>def legacy_run_topostats_entry_point(args=None, testing=False) -&gt; None:\n    \"\"\"Legacy entry point for the run_topostats processing function.\"\"\"\n    parser = create_legacy_run_topostats_parser()\n    args = parser.parse_args() if args is None else parser.parse_args(args)\n\n    if testing:\n        return args\n\n    run_topostats(args=args)\n\n    return None\n</code></pre>"},{"location":"api/entry_point/#topostats.entry_point.legacy_toposum_entry_point","title":"<code>legacy_toposum_entry_point(args=None, testing=False) -&gt; None</code>","text":"<p>Legacy entry point for the toposum summarizing function.</p> Source code in <code>topostats\\entry_point.py</code> <pre><code>def legacy_toposum_entry_point(args=None, testing=False) -&gt; None:\n    \"\"\"Legacy entry point for the toposum summarizing function.\"\"\"\n    parser = create_legacy_toposum_parser()\n    args = parser.parse_args() if args is None else parser.parse_args(args)\n\n    if testing:\n        return args\n\n    run_toposum(args=args)\n\n    return None\n</code></pre>"},{"location":"api/filters/","title":"Filters Modules","text":"<p>Module for filtering 2D Numpy arrays.</p>"},{"location":"api/filters/#topostats.filters.Filters","title":"<code>Filters</code>","text":"<p>Class for filtering scans.</p> Source code in <code>topostats\\filters.py</code> <pre><code>class Filters:\n    \"\"\"Class for filtering scans.\"\"\"\n\n    def __init__(\n        self,\n        image: np.ndarray,\n        filename: str,\n        pixel_to_nm_scaling: float,\n        row_alignment_quantile: float = 0.5,\n        threshold_method: str = \"otsu\",\n        otsu_threshold_multiplier: float = 1.7,\n        threshold_std_dev: dict = None,\n        threshold_absolute: dict = None,\n        gaussian_size: float = None,\n        gaussian_mode: str = \"nearest\",\n        remove_scars: dict = None,\n    ):\n        \"\"\"Initialise the class.\n\n        Parameters\n        ----------\n        image: np.ndarray\n            The raw image from the AFM.\n        filename: str\n            The filename (used for logging outputs only).\n        pixel_to_nm_scaling: float\n            Value for converting pixels to nanometers.\n        row_alignment_quantile: float\n            Quantile (0.0 to 1.0) to be used to determine the average background for the image.\n            below values may improve flattening of large features.\n        threshold_method: str\n            Method for thresholding, default 'otsu', valid options 'otsu', 'std_dev' and 'absolute'.\n        otsu_threshold_multiplier: float\n            Value for scaling the derived Otsu threshold (optional).\n        threshold_std_dev: dict\n            If using the 'std_dev' threshold method. Dictionary that contains above and below\n            threshold values for the number of standard deviations from the mean to threshold.\n        threshold_absolute: dict\n            If using the 'absolute' threshold method. Dictionary that contains above and below\n            absolute threshold values for flattening.\n        remove_scars: dict\n            Dictionary containing configuration parameters for the scar removal function.\n        \"\"\"\n        self.filename = filename\n        self.pixel_to_nm_scaling = pixel_to_nm_scaling\n        self.gaussian_size = gaussian_size\n        self.gaussian_mode = gaussian_mode\n        self.row_alignment_quantile = row_alignment_quantile\n        self.threshold_method = threshold_method\n        self.otsu_threshold_multiplier = otsu_threshold_multiplier\n        self.threshold_std_dev = threshold_std_dev\n        self.threshold_absolute = threshold_absolute\n        self.remove_scars_config = remove_scars\n        self.images = {\n            \"pixels\": image,\n            \"initial_median_flatten\": None,\n            \"initial_tilt_removal\": None,\n            \"initial_quadratic_removal\": None,\n            \"initial_scar_removal\": None,\n            \"initial_zero_average_background\": None,\n            \"masked_median_flatten\": None,\n            \"masked_tilt_removal\": None,\n            \"masked_quadratic_removal\": None,\n            \"secondary_scar_removal\": None,\n            \"scar_mask\": None,\n            \"mask\": None,\n            \"final_zero_average_background\": None,\n            \"gaussian_filtered\": None,\n        }\n        self.thresholds = None\n        self.medians = {\"rows\": None, \"cols\": None}\n        self.results = {\n            \"diff\": None,\n            \"median_row_height\": None,\n            \"x_gradient\": None,\n            \"y_gradient\": None,\n            \"threshold\": None,\n        }\n\n    def median_flatten(\n        self, image: np.ndarray, mask: np.ndarray = None, row_alignment_quantile: float = 0.5\n    ) -&gt; np.ndarray:\n        \"\"\"Flatten images using median differences.\n\n        Flatten the rows of an image, aligning the rows and centering the median around zero. When used with a mask,\n        this has the effect of centering the background data on zero.\n\n        Note this function does not handle scars.\n\n        Parameters\n        ----------\n        image: np.ndarray\n            2-D image of the data to align the rows of.\n        mask: np.ndarray\n            Boolean array of points to mask out (ignore).\n        row_alignment_quantile: float\n            Quantile (0.0 to 1.0) used for defining the average background.\n\n        Returns\n        -------\n        np.ndarray\n            Returns a copy of the input image with rows aligned\n        \"\"\"\n        image = image.copy()\n        if mask is not None:\n            read_matrix = np.ma.masked_array(image, mask=mask, fill_value=np.nan).filled()\n            LOGGER.info(f\"[{self.filename}] : Median flattening with mask\")\n        else:\n            read_matrix = image\n            LOGGER.info(f\"[{self.filename}] : Median flattening without mask\")\n\n        for row in range(image.shape[0]):\n            # Get the median of the row\n            m = np.nanquantile(read_matrix[row, :], row_alignment_quantile)\n            if not np.isnan(m):\n                image[row, :] -= m\n            else:\n                LOGGER.warning(\n                    \"\"\"f[{self.filename}] Large grain detected image can not be\nprocessed, please refer to &lt;url to page where we document common problems&gt; for more information.\"\"\"\n                )\n\n        return image\n\n    def remove_tilt(self, image: np.ndarray, mask: np.ndarray = None):\n        \"\"\"\n        Remove planar tilt from an image (linear in 2D space).\n\n        Uses a linear fit of the medians of the rows and columns to determine the linear slants in x and y directions\n        and then subtracts the fit from the columns.\n\n        Parameters\n        ----------\n        image: np.ndarray\n            2-D image of the data to remove the planar tilt from.\n        mask: np.ndarray\n            Boolean array of points to mask out (ignore).\n        img_name: str\n            Name of the image (to be able to print information in the console).\n\n        Returns\n        -------\n        np.ndarray\n            Returns a copy of the input image with the planar tilt removed\n        \"\"\"\n        image = image.copy()\n        if mask is not None:\n            read_matrix = np.ma.masked_array(image, mask=mask, fill_value=np.nan).filled()\n            LOGGER.info(f\"[{self.filename}] : Plane tilt removal with mask\")\n        else:\n            read_matrix = image\n            LOGGER.info(f\"[{self.filename}] : Plane tilt removal without mask\")\n\n        # Line of best fit\n        # Calculate medians\n        medians_x = [np.nanmedian(read_matrix[:, i]) for i in range(read_matrix.shape[1])]\n        medians_y = [np.nanmedian(read_matrix[j, :]) for j in range(read_matrix.shape[0])]\n        LOGGER.debug(f\"[{self.filename}] [remove_tilt] medians_x   : {medians_x}\")\n        LOGGER.debug(f\"[{self.filename}] [remove_tilt] medians_y   : {medians_y}\")\n\n        # Fit linear x\n        px = np.polyfit(range(0, len(medians_x)), medians_x, 1)\n        LOGGER.info(f\"[{self.filename}] : x-polyfit 1st order: {px}\")\n        py = np.polyfit(range(0, len(medians_y)), medians_y, 1)\n        LOGGER.info(f\"[{self.filename}] : y-polyfit 1st order: {py}\")\n\n        if px[0] != 0:\n            if not np.isnan(px[0]):\n                LOGGER.info(f\"[{self.filename}] : Removing x plane tilt\")\n                for row in range(0, image.shape[0]):\n                    for col in range(0, image.shape[1]):\n                        image[row, col] -= px[0] * (col)\n            else:\n                LOGGER.info(f\"[{self.filename}] : x gradient is nan, skipping plane tilt x removal\")\n        else:\n            LOGGER.info(\"[{self.filename}] : x gradient is zero, skipping plane tilt x removal\")\n\n        if py[0] != 0:\n            if not np.isnan(py[0]):\n                LOGGER.info(f\"[{self.filename}] : removing y plane tilt\")\n                for row in range(0, image.shape[0]):\n                    for col in range(0, image.shape[1]):\n                        image[row, col] -= py[0] * (row)\n            else:\n                LOGGER.info(\"[{self.filename}] : y gradient is nan, skipping plane tilt y removal\")\n        else:\n            LOGGER.info(\"[{self.filename}] : y gradient is zero, skipping plane tilt y removal\")\n\n        return image\n\n    def remove_nonlinear_polynomial(self, image: np.ndarray, mask: np.ndarray | None = None) -&gt; np.ndarray:\n        # Script has a lot of locals but I feel this is necessary for readability?\n        # pylint: disable=too-many-locals\n        \"\"\"Fit and remove a \"saddle\" shaped nonlinear polynomial from the image.\n\n        \"Saddles\" with the form a + b * x * y - c * x - d * y from the supplied image. AFM images sometimes contain a\n        \"saddle\" shape trend to their background, and so to remove them we fit a nonlinear polynomial of x and y and\n        then subtract the fit from the image.\n\n        If these trends are not removed, then the image will not flatten properly and will leave opposite diagonal\n        corners raised or lowered.\n\n        Parameters\n        ----------\n        image: np.ndarray\n            2D numpy heightmap array of floats with a polynomial trend to remove.\n        mask: np.ndarray\n            2D numpy boolean array used to mask out any points in the image that are deemed not to be part of the\n            heightmap's background data. This argument is optional.\n\n        Returns\n        -------\n        np.ndarray\n            Copy of the supplied image with the polynomial trend subtracted.\n        \"\"\"\n\n        # Define the polynomial function to fit to the image\n        def model_func(x, y, a, b, c, d):\n            return a + b * x * y - c * x - d * y\n\n        image = image.copy()\n        if mask is not None:\n            read_matrix = np.ma.masked_array(image, mask=mask, fill_value=np.nan).filled()\n        else:\n            read_matrix = image\n\n        # Construct a meshgrid of x and y points for fitting to the z heights\n        xdata, ydata = np.meshgrid(np.arange(read_matrix.shape[1]), np.arange(read_matrix.shape[0]))\n        zdata = read_matrix\n\n        # Only use data that is not nan. Nans may be in the image from the\n        # masked array. Curve fitting cannot handle nans.\n        nan_mask = ~np.isnan(zdata)\n        xdata_nans_removed = xdata[nan_mask]\n        ydata_nans_removed = ydata[nan_mask]\n        zdata_nans_removed = zdata[nan_mask]\n\n        # Convert the z data to a 1D array\n        zdata = zdata.ravel()\n        zdata_nans_removed = zdata_nans_removed.ravel()\n\n        # Stack the x, y meshgrid data after converting them to 1D\n        xy_data_stacked = np.vstack((xdata_nans_removed.ravel(), ydata_nans_removed.ravel()))\n\n        # Fit the model to the data\n        # Note: pylint is flagging the tuple unpacking regarding an internal line of scipy.optimize._minpack_py : 910.\n        # This isn't actually an issue though as the extended tuple output is only provided if the 'full_output' flag is\n        # provided as a kwarg in curve_fit.\n        popt, _pcov = curve_fit(  # pylint: disable=unbalanced-tuple-unpacking\n            lambda x, a, b, c, d: model_func(x[0], x[1], a, b, c, d),\n            xy_data_stacked,\n            zdata_nans_removed,\n        )\n\n        # Unpack the optimised parameters\n        a, b, c, d = popt\n        LOGGER.info(\n            f\"[{self.filename}] : Nonlinear polynomial removal optimal params: const: {a} xy: {b} x: {c} y: {d}\"\n        )\n\n        # Use the optimised parameters to construct a prediction of the underlying surface\n        z_pred = model_func(xdata, ydata, a, b, c, d)\n        # Subtract the fitted nonlinear polynomial from the image\n        image -= z_pred\n\n        return image\n\n    def remove_quadratic(self, image: np.ndarray, mask: np.ndarray = None) -&gt; np.ndarray:\n        \"\"\"\n        Remove the quadratic bowing that can be seen in some large-scale AFM images.\n\n        Use a simple quadratic fit on the medians of the columns of the image and then subtracts the calculated\n        quadratic from the columns.\n\n        Parameters\n        ----------\n        image: np.ndarray\n            2-D image of the data to remove the quadratic from.\n        mask: np.ndarray\n            Boolean array of points to mask out (ignore).\n\n        Returns\n        -------\n        np.ndarray\n            Returns a copy of the input image with the quadratic bowing removed\n        \"\"\"\n        image = image.copy()\n        if mask is not None:\n            read_matrix = np.ma.masked_array(image, mask=mask, fill_value=np.nan).filled()\n            LOGGER.info(f\"[{self.filename}] : Remove quadratic bow with mask\")\n        else:\n            read_matrix = image\n            LOGGER.info(f\"[{self.filename}] : Remove quadratic bow without mask\")\n\n        # Calculate medians\n        medians_x = [np.nanmedian(read_matrix[:, i]) for i in range(read_matrix.shape[1])]\n\n        # Fit quadratic x\n        px = np.polyfit(range(0, len(medians_x)), medians_x, 2)\n        LOGGER.info(f\"[{self.filename}] : x polyfit 2nd order: {px}\")\n\n        # Handle divide by zero\n        if px[0] != 0:\n            if not np.isnan(px[0]):\n                # Remove quadratic in x\n                cx = -px[1] / (2 * px[0])\n                for row in range(0, image.shape[0]):\n                    for col in range(0, image.shape[1]):\n                        image[row, col] -= px[0] * (col - cx) ** 2\n            else:\n                LOGGER.info(f\"[{self.filename}] : Quadratic polyfit returns nan, skipping quadratic removal\")\n        else:\n            LOGGER.info(f\"[{self.filename}] : Quadratic polyfit returns zero, skipping quadratic removal\")\n\n        return image\n\n    @staticmethod\n    def calc_diff(array: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Calculate the difference of an array.\"\"\"\n        return array[-1] - array[0]\n\n    def calc_gradient(self, array: np.ndarray, shape: int) -&gt; np.ndarray:\n        \"\"\"Calculate the gradient of an array.\"\"\"\n        return self.calc_diff(array) / shape\n\n    def average_background(self, image: np.ndarray, mask: np.ndarray = None) -&gt; np.ndarray:\n        \"\"\"Zero the background by subtracting the non-masked mean from all pixels.\n\n        Parameters\n        ----------\n        image: np.array\n            Numpy array representing image.\n        mask: np.array\n            Mask of the array, should have the same dimensions as image.\n\n        Returns\n        -------\n        np.ndarray\n            Numpy array of image zero averaged.\n        \"\"\"\n        if mask is None:\n            mask = np.zeros_like(image)\n        mean = np.mean(image[mask == 0])\n        LOGGER.info(f\"[{self.filename}] : Zero averaging background : {mean} nm\")\n        return image - mean\n\n    def gaussian_filter(self, image: np.ndarray, **kwargs) -&gt; np.array:\n        \"\"\"Apply Gaussian filter to an image.\n\n        Parameters\n        ----------\n        image: np.array\n            Numpy array representing image.\n\n        Returns\n        -------\n        np.array\n            Numpy array of gaussian blurred image.\n\n        \"\"\"\n        LOGGER.info(\n            f\"[{self.filename}] : Applying Gaussian filter (mode : {self.gaussian_mode};\"\n            f\" Gaussian blur (px) : {self.gaussian_size}).\"\n        )\n        return gaussian(\n            image,\n            sigma=(self.gaussian_size),\n            mode=self.gaussian_mode,\n            **kwargs,\n        )\n\n    def filter_image(self) -&gt; None:\n        \"\"\"Process a single image, filtering, finding grains and calculating their statistics.\n\n        Example\n        -------\n        from topostats.io import LoadScan\n        from topostats.topotracing import Filter, process_scan\n\n        filter = Filter(image=load_scan.image,\n        ...             pixel_to_nm_scaling=load_scan.pixel_to_nm_scaling,\n        ...             filename=load_scan.filename,\n        ...             threshold_method='otsu')\n        filter.filter_image()\n\n        \"\"\"\n        self.images[\"initial_median_flatten\"] = self.median_flatten(\n            self.images[\"pixels\"], mask=None, row_alignment_quantile=self.row_alignment_quantile\n        )\n        self.images[\"initial_tilt_removal\"] = self.remove_tilt(self.images[\"initial_median_flatten\"], mask=None)\n        self.images[\"initial_quadratic_removal\"] = self.remove_quadratic(self.images[\"initial_tilt_removal\"], mask=None)\n        self.images[\"initial_nonlinear_polynomial_removal\"] = self.remove_nonlinear_polynomial(\n            self.images[\"initial_quadratic_removal\"], mask=None\n        )\n\n        # Remove scars\n        run_scar_removal = self.remove_scars_config.pop(\"run\")\n        if run_scar_removal:\n            LOGGER.info(f\"[{self.filename}] : Initial scar removal\")\n            self.images[\"initial_scar_removal\"], _ = scars.remove_scars(\n                self.images[\"initial_nonlinear_polynomial_removal\"],\n                filename=self.filename,\n                **self.remove_scars_config,\n            )\n        else:\n            LOGGER.info(f\"[{self.filename}] : Skipping scar removal as requested from config\")\n            self.images[\"initial_scar_removal\"] = self.images[\"initial_nonlinear_polynomial_removal\"]\n\n        # Zero the data before thresholding, helps with absolute thresholding\n        self.images[\"initial_zero_average_background\"] = self.average_background(\n            self.images[\"initial_scar_removal\"], mask=None\n        )\n\n        # Get the thresholds\n        try:\n            self.thresholds = get_thresholds(\n                image=self.images[\"initial_zero_average_background\"],\n                threshold_method=self.threshold_method,\n                otsu_threshold_multiplier=self.otsu_threshold_multiplier,\n                threshold_std_dev=self.threshold_std_dev,\n                absolute=self.threshold_absolute,\n            )\n        except TypeError as type_error:\n            raise type_error\n        self.images[\"mask\"] = get_mask(\n            image=self.images[\"initial_zero_average_background\"],\n            thresholds=self.thresholds,\n            img_name=self.filename,\n        )\n        self.images[\"masked_median_flatten\"] = self.median_flatten(\n            self.images[\"initial_tilt_removal\"],\n            self.images[\"mask\"],\n            row_alignment_quantile=self.row_alignment_quantile,\n        )\n        self.images[\"masked_tilt_removal\"] = self.remove_tilt(self.images[\"masked_median_flatten\"], self.images[\"mask\"])\n        self.images[\"masked_quadratic_removal\"] = self.remove_quadratic(\n            self.images[\"masked_tilt_removal\"], self.images[\"mask\"]\n        )\n        self.images[\"masked_nonlinear_polynomial_removal\"] = self.remove_nonlinear_polynomial(\n            self.images[\"masked_quadratic_removal\"], self.images[\"mask\"]\n        )\n        # Remove scars\n        if run_scar_removal:\n            LOGGER.info(f\"[{self.filename}] : Secondary scar removal\")\n            self.images[\"secondary_scar_removal\"], scar_mask = scars.remove_scars(\n                self.images[\"masked_nonlinear_polynomial_removal\"],\n                filename=self.filename,\n                **self.remove_scars_config,\n            )\n            self.images[\"scar_mask\"] = scar_mask\n        else:\n            LOGGER.info(f\"[{self.filename}] : Skipping scar removal as requested from config\")\n            self.images[\"secondary_scar_removal\"] = self.images[\"masked_nonlinear_polynomial_removal\"]\n        self.images[\"final_zero_average_background\"] = self.average_background(\n            self.images[\"secondary_scar_removal\"], self.images[\"mask\"]\n        )\n        self.images[\"gaussian_filtered\"] = self.gaussian_filter(self.images[\"final_zero_average_background\"])\n</code></pre>"},{"location":"api/filters/#topostats.filters.Filters.__init__","title":"<code>__init__(image: np.ndarray, filename: str, pixel_to_nm_scaling: float, row_alignment_quantile: float = 0.5, threshold_method: str = 'otsu', otsu_threshold_multiplier: float = 1.7, threshold_std_dev: dict = None, threshold_absolute: dict = None, gaussian_size: float = None, gaussian_mode: str = 'nearest', remove_scars: dict = None)</code>","text":"<p>Initialise the class.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The raw image from the AFM.</p> required <code>filename</code> <code>str</code> <p>The filename (used for logging outputs only).</p> required <code>pixel_to_nm_scaling</code> <code>float</code> <p>Value for converting pixels to nanometers.</p> required <code>row_alignment_quantile</code> <code>float</code> <p>Quantile (0.0 to 1.0) to be used to determine the average background for the image. below values may improve flattening of large features.</p> <code>0.5</code> <code>threshold_method</code> <code>str</code> <p>Method for thresholding, default 'otsu', valid options 'otsu', 'std_dev' and 'absolute'.</p> <code>'otsu'</code> <code>otsu_threshold_multiplier</code> <code>float</code> <p>Value for scaling the derived Otsu threshold (optional).</p> <code>1.7</code> <code>threshold_std_dev</code> <code>dict</code> <p>If using the 'std_dev' threshold method. Dictionary that contains above and below threshold values for the number of standard deviations from the mean to threshold.</p> <code>None</code> <code>threshold_absolute</code> <code>dict</code> <p>If using the 'absolute' threshold method. Dictionary that contains above and below absolute threshold values for flattening.</p> <code>None</code> <code>remove_scars</code> <code>dict</code> <p>Dictionary containing configuration parameters for the scar removal function.</p> <code>None</code> Source code in <code>topostats\\filters.py</code> <pre><code>def __init__(\n    self,\n    image: np.ndarray,\n    filename: str,\n    pixel_to_nm_scaling: float,\n    row_alignment_quantile: float = 0.5,\n    threshold_method: str = \"otsu\",\n    otsu_threshold_multiplier: float = 1.7,\n    threshold_std_dev: dict = None,\n    threshold_absolute: dict = None,\n    gaussian_size: float = None,\n    gaussian_mode: str = \"nearest\",\n    remove_scars: dict = None,\n):\n    \"\"\"Initialise the class.\n\n    Parameters\n    ----------\n    image: np.ndarray\n        The raw image from the AFM.\n    filename: str\n        The filename (used for logging outputs only).\n    pixel_to_nm_scaling: float\n        Value for converting pixels to nanometers.\n    row_alignment_quantile: float\n        Quantile (0.0 to 1.0) to be used to determine the average background for the image.\n        below values may improve flattening of large features.\n    threshold_method: str\n        Method for thresholding, default 'otsu', valid options 'otsu', 'std_dev' and 'absolute'.\n    otsu_threshold_multiplier: float\n        Value for scaling the derived Otsu threshold (optional).\n    threshold_std_dev: dict\n        If using the 'std_dev' threshold method. Dictionary that contains above and below\n        threshold values for the number of standard deviations from the mean to threshold.\n    threshold_absolute: dict\n        If using the 'absolute' threshold method. Dictionary that contains above and below\n        absolute threshold values for flattening.\n    remove_scars: dict\n        Dictionary containing configuration parameters for the scar removal function.\n    \"\"\"\n    self.filename = filename\n    self.pixel_to_nm_scaling = pixel_to_nm_scaling\n    self.gaussian_size = gaussian_size\n    self.gaussian_mode = gaussian_mode\n    self.row_alignment_quantile = row_alignment_quantile\n    self.threshold_method = threshold_method\n    self.otsu_threshold_multiplier = otsu_threshold_multiplier\n    self.threshold_std_dev = threshold_std_dev\n    self.threshold_absolute = threshold_absolute\n    self.remove_scars_config = remove_scars\n    self.images = {\n        \"pixels\": image,\n        \"initial_median_flatten\": None,\n        \"initial_tilt_removal\": None,\n        \"initial_quadratic_removal\": None,\n        \"initial_scar_removal\": None,\n        \"initial_zero_average_background\": None,\n        \"masked_median_flatten\": None,\n        \"masked_tilt_removal\": None,\n        \"masked_quadratic_removal\": None,\n        \"secondary_scar_removal\": None,\n        \"scar_mask\": None,\n        \"mask\": None,\n        \"final_zero_average_background\": None,\n        \"gaussian_filtered\": None,\n    }\n    self.thresholds = None\n    self.medians = {\"rows\": None, \"cols\": None}\n    self.results = {\n        \"diff\": None,\n        \"median_row_height\": None,\n        \"x_gradient\": None,\n        \"y_gradient\": None,\n        \"threshold\": None,\n    }\n</code></pre>"},{"location":"api/filters/#topostats.filters.Filters.average_background","title":"<code>average_background(image: np.ndarray, mask: np.ndarray = None) -&gt; np.ndarray</code>","text":"<p>Zero the background by subtracting the non-masked mean from all pixels.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Numpy array representing image.</p> required <code>mask</code> <code>ndarray</code> <p>Mask of the array, should have the same dimensions as image.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Numpy array of image zero averaged.</p> Source code in <code>topostats\\filters.py</code> <pre><code>def average_background(self, image: np.ndarray, mask: np.ndarray = None) -&gt; np.ndarray:\n    \"\"\"Zero the background by subtracting the non-masked mean from all pixels.\n\n    Parameters\n    ----------\n    image: np.array\n        Numpy array representing image.\n    mask: np.array\n        Mask of the array, should have the same dimensions as image.\n\n    Returns\n    -------\n    np.ndarray\n        Numpy array of image zero averaged.\n    \"\"\"\n    if mask is None:\n        mask = np.zeros_like(image)\n    mean = np.mean(image[mask == 0])\n    LOGGER.info(f\"[{self.filename}] : Zero averaging background : {mean} nm\")\n    return image - mean\n</code></pre>"},{"location":"api/filters/#topostats.filters.Filters.calc_diff","title":"<code>calc_diff(array: np.ndarray) -&gt; np.ndarray</code>  <code>staticmethod</code>","text":"<p>Calculate the difference of an array.</p> Source code in <code>topostats\\filters.py</code> <pre><code>@staticmethod\ndef calc_diff(array: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Calculate the difference of an array.\"\"\"\n    return array[-1] - array[0]\n</code></pre>"},{"location":"api/filters/#topostats.filters.Filters.calc_gradient","title":"<code>calc_gradient(array: np.ndarray, shape: int) -&gt; np.ndarray</code>","text":"<p>Calculate the gradient of an array.</p> Source code in <code>topostats\\filters.py</code> <pre><code>def calc_gradient(self, array: np.ndarray, shape: int) -&gt; np.ndarray:\n    \"\"\"Calculate the gradient of an array.\"\"\"\n    return self.calc_diff(array) / shape\n</code></pre>"},{"location":"api/filters/#topostats.filters.Filters.filter_image","title":"<code>filter_image() -&gt; None</code>","text":"<p>Process a single image, filtering, finding grains and calculating their statistics.</p> Example <p>from topostats.io import LoadScan from topostats.topotracing import Filter, process_scan</p> <p>filter = Filter(image=load_scan.image, ...             pixel_to_nm_scaling=load_scan.pixel_to_nm_scaling, ...             filename=load_scan.filename, ...             threshold_method='otsu') filter.filter_image()</p> Source code in <code>topostats\\filters.py</code> <pre><code>def filter_image(self) -&gt; None:\n    \"\"\"Process a single image, filtering, finding grains and calculating their statistics.\n\n    Example\n    -------\n    from topostats.io import LoadScan\n    from topostats.topotracing import Filter, process_scan\n\n    filter = Filter(image=load_scan.image,\n    ...             pixel_to_nm_scaling=load_scan.pixel_to_nm_scaling,\n    ...             filename=load_scan.filename,\n    ...             threshold_method='otsu')\n    filter.filter_image()\n\n    \"\"\"\n    self.images[\"initial_median_flatten\"] = self.median_flatten(\n        self.images[\"pixels\"], mask=None, row_alignment_quantile=self.row_alignment_quantile\n    )\n    self.images[\"initial_tilt_removal\"] = self.remove_tilt(self.images[\"initial_median_flatten\"], mask=None)\n    self.images[\"initial_quadratic_removal\"] = self.remove_quadratic(self.images[\"initial_tilt_removal\"], mask=None)\n    self.images[\"initial_nonlinear_polynomial_removal\"] = self.remove_nonlinear_polynomial(\n        self.images[\"initial_quadratic_removal\"], mask=None\n    )\n\n    # Remove scars\n    run_scar_removal = self.remove_scars_config.pop(\"run\")\n    if run_scar_removal:\n        LOGGER.info(f\"[{self.filename}] : Initial scar removal\")\n        self.images[\"initial_scar_removal\"], _ = scars.remove_scars(\n            self.images[\"initial_nonlinear_polynomial_removal\"],\n            filename=self.filename,\n            **self.remove_scars_config,\n        )\n    else:\n        LOGGER.info(f\"[{self.filename}] : Skipping scar removal as requested from config\")\n        self.images[\"initial_scar_removal\"] = self.images[\"initial_nonlinear_polynomial_removal\"]\n\n    # Zero the data before thresholding, helps with absolute thresholding\n    self.images[\"initial_zero_average_background\"] = self.average_background(\n        self.images[\"initial_scar_removal\"], mask=None\n    )\n\n    # Get the thresholds\n    try:\n        self.thresholds = get_thresholds(\n            image=self.images[\"initial_zero_average_background\"],\n            threshold_method=self.threshold_method,\n            otsu_threshold_multiplier=self.otsu_threshold_multiplier,\n            threshold_std_dev=self.threshold_std_dev,\n            absolute=self.threshold_absolute,\n        )\n    except TypeError as type_error:\n        raise type_error\n    self.images[\"mask\"] = get_mask(\n        image=self.images[\"initial_zero_average_background\"],\n        thresholds=self.thresholds,\n        img_name=self.filename,\n    )\n    self.images[\"masked_median_flatten\"] = self.median_flatten(\n        self.images[\"initial_tilt_removal\"],\n        self.images[\"mask\"],\n        row_alignment_quantile=self.row_alignment_quantile,\n    )\n    self.images[\"masked_tilt_removal\"] = self.remove_tilt(self.images[\"masked_median_flatten\"], self.images[\"mask\"])\n    self.images[\"masked_quadratic_removal\"] = self.remove_quadratic(\n        self.images[\"masked_tilt_removal\"], self.images[\"mask\"]\n    )\n    self.images[\"masked_nonlinear_polynomial_removal\"] = self.remove_nonlinear_polynomial(\n        self.images[\"masked_quadratic_removal\"], self.images[\"mask\"]\n    )\n    # Remove scars\n    if run_scar_removal:\n        LOGGER.info(f\"[{self.filename}] : Secondary scar removal\")\n        self.images[\"secondary_scar_removal\"], scar_mask = scars.remove_scars(\n            self.images[\"masked_nonlinear_polynomial_removal\"],\n            filename=self.filename,\n            **self.remove_scars_config,\n        )\n        self.images[\"scar_mask\"] = scar_mask\n    else:\n        LOGGER.info(f\"[{self.filename}] : Skipping scar removal as requested from config\")\n        self.images[\"secondary_scar_removal\"] = self.images[\"masked_nonlinear_polynomial_removal\"]\n    self.images[\"final_zero_average_background\"] = self.average_background(\n        self.images[\"secondary_scar_removal\"], self.images[\"mask\"]\n    )\n    self.images[\"gaussian_filtered\"] = self.gaussian_filter(self.images[\"final_zero_average_background\"])\n</code></pre>"},{"location":"api/filters/#topostats.filters.Filters.gaussian_filter","title":"<code>gaussian_filter(image: np.ndarray, **kwargs) -&gt; np.array</code>","text":"<p>Apply Gaussian filter to an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Numpy array representing image.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Numpy array of gaussian blurred image.</p> Source code in <code>topostats\\filters.py</code> <pre><code>def gaussian_filter(self, image: np.ndarray, **kwargs) -&gt; np.array:\n    \"\"\"Apply Gaussian filter to an image.\n\n    Parameters\n    ----------\n    image: np.array\n        Numpy array representing image.\n\n    Returns\n    -------\n    np.array\n        Numpy array of gaussian blurred image.\n\n    \"\"\"\n    LOGGER.info(\n        f\"[{self.filename}] : Applying Gaussian filter (mode : {self.gaussian_mode};\"\n        f\" Gaussian blur (px) : {self.gaussian_size}).\"\n    )\n    return gaussian(\n        image,\n        sigma=(self.gaussian_size),\n        mode=self.gaussian_mode,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/filters/#topostats.filters.Filters.median_flatten","title":"<code>median_flatten(image: np.ndarray, mask: np.ndarray = None, row_alignment_quantile: float = 0.5) -&gt; np.ndarray</code>","text":"<p>Flatten images using median differences.</p> <p>Flatten the rows of an image, aligning the rows and centering the median around zero. When used with a mask, this has the effect of centering the background data on zero.</p> <p>Note this function does not handle scars.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>2-D image of the data to align the rows of.</p> required <code>mask</code> <code>ndarray</code> <p>Boolean array of points to mask out (ignore).</p> <code>None</code> <code>row_alignment_quantile</code> <code>float</code> <p>Quantile (0.0 to 1.0) used for defining the average background.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Returns a copy of the input image with rows aligned</p> Source code in <code>topostats\\filters.py</code> <pre><code>    def median_flatten(\n        self, image: np.ndarray, mask: np.ndarray = None, row_alignment_quantile: float = 0.5\n    ) -&gt; np.ndarray:\n        \"\"\"Flatten images using median differences.\n\n        Flatten the rows of an image, aligning the rows and centering the median around zero. When used with a mask,\n        this has the effect of centering the background data on zero.\n\n        Note this function does not handle scars.\n\n        Parameters\n        ----------\n        image: np.ndarray\n            2-D image of the data to align the rows of.\n        mask: np.ndarray\n            Boolean array of points to mask out (ignore).\n        row_alignment_quantile: float\n            Quantile (0.0 to 1.0) used for defining the average background.\n\n        Returns\n        -------\n        np.ndarray\n            Returns a copy of the input image with rows aligned\n        \"\"\"\n        image = image.copy()\n        if mask is not None:\n            read_matrix = np.ma.masked_array(image, mask=mask, fill_value=np.nan).filled()\n            LOGGER.info(f\"[{self.filename}] : Median flattening with mask\")\n        else:\n            read_matrix = image\n            LOGGER.info(f\"[{self.filename}] : Median flattening without mask\")\n\n        for row in range(image.shape[0]):\n            # Get the median of the row\n            m = np.nanquantile(read_matrix[row, :], row_alignment_quantile)\n            if not np.isnan(m):\n                image[row, :] -= m\n            else:\n                LOGGER.warning(\n                    \"\"\"f[{self.filename}] Large grain detected image can not be\nprocessed, please refer to &lt;url to page where we document common problems&gt; for more information.\"\"\"\n                )\n\n        return image\n</code></pre>"},{"location":"api/filters/#topostats.filters.Filters.remove_nonlinear_polynomial","title":"<code>remove_nonlinear_polynomial(image: np.ndarray, mask: np.ndarray | None = None) -&gt; np.ndarray</code>","text":"<p>Fit and remove a \"saddle\" shaped nonlinear polynomial from the image.</p> <p>\"Saddles\" with the form a + b * x * y - c * x - d * y from the supplied image. AFM images sometimes contain a \"saddle\" shape trend to their background, and so to remove them we fit a nonlinear polynomial of x and y and then subtract the fit from the image.</p> <p>If these trends are not removed, then the image will not flatten properly and will leave opposite diagonal corners raised or lowered.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>2D numpy heightmap array of floats with a polynomial trend to remove.</p> required <code>mask</code> <code>ndarray | None</code> <p>2D numpy boolean array used to mask out any points in the image that are deemed not to be part of the heightmap's background data. This argument is optional.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Copy of the supplied image with the polynomial trend subtracted.</p> Source code in <code>topostats\\filters.py</code> <pre><code>def remove_nonlinear_polynomial(self, image: np.ndarray, mask: np.ndarray | None = None) -&gt; np.ndarray:\n    # Script has a lot of locals but I feel this is necessary for readability?\n    # pylint: disable=too-many-locals\n    \"\"\"Fit and remove a \"saddle\" shaped nonlinear polynomial from the image.\n\n    \"Saddles\" with the form a + b * x * y - c * x - d * y from the supplied image. AFM images sometimes contain a\n    \"saddle\" shape trend to their background, and so to remove them we fit a nonlinear polynomial of x and y and\n    then subtract the fit from the image.\n\n    If these trends are not removed, then the image will not flatten properly and will leave opposite diagonal\n    corners raised or lowered.\n\n    Parameters\n    ----------\n    image: np.ndarray\n        2D numpy heightmap array of floats with a polynomial trend to remove.\n    mask: np.ndarray\n        2D numpy boolean array used to mask out any points in the image that are deemed not to be part of the\n        heightmap's background data. This argument is optional.\n\n    Returns\n    -------\n    np.ndarray\n        Copy of the supplied image with the polynomial trend subtracted.\n    \"\"\"\n\n    # Define the polynomial function to fit to the image\n    def model_func(x, y, a, b, c, d):\n        return a + b * x * y - c * x - d * y\n\n    image = image.copy()\n    if mask is not None:\n        read_matrix = np.ma.masked_array(image, mask=mask, fill_value=np.nan).filled()\n    else:\n        read_matrix = image\n\n    # Construct a meshgrid of x and y points for fitting to the z heights\n    xdata, ydata = np.meshgrid(np.arange(read_matrix.shape[1]), np.arange(read_matrix.shape[0]))\n    zdata = read_matrix\n\n    # Only use data that is not nan. Nans may be in the image from the\n    # masked array. Curve fitting cannot handle nans.\n    nan_mask = ~np.isnan(zdata)\n    xdata_nans_removed = xdata[nan_mask]\n    ydata_nans_removed = ydata[nan_mask]\n    zdata_nans_removed = zdata[nan_mask]\n\n    # Convert the z data to a 1D array\n    zdata = zdata.ravel()\n    zdata_nans_removed = zdata_nans_removed.ravel()\n\n    # Stack the x, y meshgrid data after converting them to 1D\n    xy_data_stacked = np.vstack((xdata_nans_removed.ravel(), ydata_nans_removed.ravel()))\n\n    # Fit the model to the data\n    # Note: pylint is flagging the tuple unpacking regarding an internal line of scipy.optimize._minpack_py : 910.\n    # This isn't actually an issue though as the extended tuple output is only provided if the 'full_output' flag is\n    # provided as a kwarg in curve_fit.\n    popt, _pcov = curve_fit(  # pylint: disable=unbalanced-tuple-unpacking\n        lambda x, a, b, c, d: model_func(x[0], x[1], a, b, c, d),\n        xy_data_stacked,\n        zdata_nans_removed,\n    )\n\n    # Unpack the optimised parameters\n    a, b, c, d = popt\n    LOGGER.info(\n        f\"[{self.filename}] : Nonlinear polynomial removal optimal params: const: {a} xy: {b} x: {c} y: {d}\"\n    )\n\n    # Use the optimised parameters to construct a prediction of the underlying surface\n    z_pred = model_func(xdata, ydata, a, b, c, d)\n    # Subtract the fitted nonlinear polynomial from the image\n    image -= z_pred\n\n    return image\n</code></pre>"},{"location":"api/filters/#topostats.filters.Filters.remove_quadratic","title":"<code>remove_quadratic(image: np.ndarray, mask: np.ndarray = None) -&gt; np.ndarray</code>","text":"<p>Remove the quadratic bowing that can be seen in some large-scale AFM images.</p> <p>Use a simple quadratic fit on the medians of the columns of the image and then subtracts the calculated quadratic from the columns.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>2-D image of the data to remove the quadratic from.</p> required <code>mask</code> <code>ndarray</code> <p>Boolean array of points to mask out (ignore).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Returns a copy of the input image with the quadratic bowing removed</p> Source code in <code>topostats\\filters.py</code> <pre><code>def remove_quadratic(self, image: np.ndarray, mask: np.ndarray = None) -&gt; np.ndarray:\n    \"\"\"\n    Remove the quadratic bowing that can be seen in some large-scale AFM images.\n\n    Use a simple quadratic fit on the medians of the columns of the image and then subtracts the calculated\n    quadratic from the columns.\n\n    Parameters\n    ----------\n    image: np.ndarray\n        2-D image of the data to remove the quadratic from.\n    mask: np.ndarray\n        Boolean array of points to mask out (ignore).\n\n    Returns\n    -------\n    np.ndarray\n        Returns a copy of the input image with the quadratic bowing removed\n    \"\"\"\n    image = image.copy()\n    if mask is not None:\n        read_matrix = np.ma.masked_array(image, mask=mask, fill_value=np.nan).filled()\n        LOGGER.info(f\"[{self.filename}] : Remove quadratic bow with mask\")\n    else:\n        read_matrix = image\n        LOGGER.info(f\"[{self.filename}] : Remove quadratic bow without mask\")\n\n    # Calculate medians\n    medians_x = [np.nanmedian(read_matrix[:, i]) for i in range(read_matrix.shape[1])]\n\n    # Fit quadratic x\n    px = np.polyfit(range(0, len(medians_x)), medians_x, 2)\n    LOGGER.info(f\"[{self.filename}] : x polyfit 2nd order: {px}\")\n\n    # Handle divide by zero\n    if px[0] != 0:\n        if not np.isnan(px[0]):\n            # Remove quadratic in x\n            cx = -px[1] / (2 * px[0])\n            for row in range(0, image.shape[0]):\n                for col in range(0, image.shape[1]):\n                    image[row, col] -= px[0] * (col - cx) ** 2\n        else:\n            LOGGER.info(f\"[{self.filename}] : Quadratic polyfit returns nan, skipping quadratic removal\")\n    else:\n        LOGGER.info(f\"[{self.filename}] : Quadratic polyfit returns zero, skipping quadratic removal\")\n\n    return image\n</code></pre>"},{"location":"api/filters/#topostats.filters.Filters.remove_tilt","title":"<code>remove_tilt(image: np.ndarray, mask: np.ndarray = None)</code>","text":"<p>Remove planar tilt from an image (linear in 2D space).</p> <p>Uses a linear fit of the medians of the rows and columns to determine the linear slants in x and y directions and then subtracts the fit from the columns.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>2-D image of the data to remove the planar tilt from.</p> required <code>mask</code> <code>ndarray</code> <p>Boolean array of points to mask out (ignore).</p> <code>None</code> <code>img_name</code> <p>Name of the image (to be able to print information in the console).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Returns a copy of the input image with the planar tilt removed</p> Source code in <code>topostats\\filters.py</code> <pre><code>def remove_tilt(self, image: np.ndarray, mask: np.ndarray = None):\n    \"\"\"\n    Remove planar tilt from an image (linear in 2D space).\n\n    Uses a linear fit of the medians of the rows and columns to determine the linear slants in x and y directions\n    and then subtracts the fit from the columns.\n\n    Parameters\n    ----------\n    image: np.ndarray\n        2-D image of the data to remove the planar tilt from.\n    mask: np.ndarray\n        Boolean array of points to mask out (ignore).\n    img_name: str\n        Name of the image (to be able to print information in the console).\n\n    Returns\n    -------\n    np.ndarray\n        Returns a copy of the input image with the planar tilt removed\n    \"\"\"\n    image = image.copy()\n    if mask is not None:\n        read_matrix = np.ma.masked_array(image, mask=mask, fill_value=np.nan).filled()\n        LOGGER.info(f\"[{self.filename}] : Plane tilt removal with mask\")\n    else:\n        read_matrix = image\n        LOGGER.info(f\"[{self.filename}] : Plane tilt removal without mask\")\n\n    # Line of best fit\n    # Calculate medians\n    medians_x = [np.nanmedian(read_matrix[:, i]) for i in range(read_matrix.shape[1])]\n    medians_y = [np.nanmedian(read_matrix[j, :]) for j in range(read_matrix.shape[0])]\n    LOGGER.debug(f\"[{self.filename}] [remove_tilt] medians_x   : {medians_x}\")\n    LOGGER.debug(f\"[{self.filename}] [remove_tilt] medians_y   : {medians_y}\")\n\n    # Fit linear x\n    px = np.polyfit(range(0, len(medians_x)), medians_x, 1)\n    LOGGER.info(f\"[{self.filename}] : x-polyfit 1st order: {px}\")\n    py = np.polyfit(range(0, len(medians_y)), medians_y, 1)\n    LOGGER.info(f\"[{self.filename}] : y-polyfit 1st order: {py}\")\n\n    if px[0] != 0:\n        if not np.isnan(px[0]):\n            LOGGER.info(f\"[{self.filename}] : Removing x plane tilt\")\n            for row in range(0, image.shape[0]):\n                for col in range(0, image.shape[1]):\n                    image[row, col] -= px[0] * (col)\n        else:\n            LOGGER.info(f\"[{self.filename}] : x gradient is nan, skipping plane tilt x removal\")\n    else:\n        LOGGER.info(\"[{self.filename}] : x gradient is zero, skipping plane tilt x removal\")\n\n    if py[0] != 0:\n        if not np.isnan(py[0]):\n            LOGGER.info(f\"[{self.filename}] : removing y plane tilt\")\n            for row in range(0, image.shape[0]):\n                for col in range(0, image.shape[1]):\n                    image[row, col] -= py[0] * (row)\n        else:\n            LOGGER.info(\"[{self.filename}] : y gradient is nan, skipping plane tilt y removal\")\n    else:\n        LOGGER.info(\"[{self.filename}] : y gradient is zero, skipping plane tilt y removal\")\n\n    return image\n</code></pre>"},{"location":"api/grains/","title":"Grains Modules","text":"<p>Find grains in an image.</p>"},{"location":"api/grains/#topostats.grains.Grains","title":"<code>Grains</code>","text":"<p>Find grains in an image.</p> Source code in <code>topostats\\grains.py</code> <pre><code>class Grains:\n    \"\"\"Find grains in an image.\"\"\"\n\n    def __init__(\n        self,\n        image: np.ndarray,\n        filename: str,\n        pixel_to_nm_scaling: float,\n        threshold_method: str = None,\n        otsu_threshold_multiplier: float = None,\n        threshold_std_dev: dict = None,\n        threshold_absolute: dict = None,\n        absolute_area_threshold: dict = None,\n        direction: str = None,\n        smallest_grain_size_nm2: float = None,\n        remove_edge_intersecting_grains: bool = True,\n    ):\n        \"\"\"Initialise the class.\n\n        Parameters\n        ----------\n        image: np.ndarray\n            2D Numpy array of image\n        filename: str\n            File being processed\n        pixel_to_nm_scaling: float\n            Scaling of pixels to nanometre.\n        threshold_multiplier : Union[int, float]\n            Factor by which below threshold is to be scaled prior to masking.\n        threshold_method: str\n            Method for determining threshold to mask values, default is 'otsu'.\n        threshold_std_dev: dict\n            Dictionary of 'below' and 'above' factors by which standard deviation is multiplied to derive the threshold if threshold_method is 'std_dev'.\n        threshold_absolute: dict\n            Dictionary of absolute 'below' and 'above' thresholds for grain finding.\n        absolute_area_threshold: dict\n            Dictionary of above and below grain's area thresholds\n        direction: str\n            Direction for which grains are to be detected, valid values are above, below and both.\n        remove_edge_intersecting_grains: bool\n            Whether or not to remove grains that intersect the edge of the image.\n        \"\"\"\n        if absolute_area_threshold is None:\n            absolute_area_threshold = {\"above\": [None, None], \"below\": [None, None]}\n        self.image = image\n        self.filename = filename\n        self.pixel_to_nm_scaling = pixel_to_nm_scaling\n        self.threshold_method = threshold_method\n        self.otsu_threshold_multiplier = otsu_threshold_multiplier\n        self.threshold_std_dev = threshold_std_dev\n        self.threshold_absolute = threshold_absolute\n        self.absolute_area_threshold = absolute_area_threshold\n        # Only detect grains for the desired direction\n        self.direction = [direction] if direction != \"both\" else [\"above\", \"below\"]\n        self.smallest_grain_size_nm2 = smallest_grain_size_nm2\n        self.remove_edge_intersecting_grains = remove_edge_intersecting_grains\n        self.thresholds = None\n        self.images = {\n            \"mask_grains\": None,\n            \"tidied_border\": None,\n            \"tiny_objects_removed\": None,\n            \"objects_removed\": None,\n            # \"labelled_regions\": None,\n            # \"coloured_regions\": None,\n        }\n        self.directions = defaultdict()\n        self.minimum_grain_size = None\n        self.region_properties = defaultdict()\n        self.bounding_boxes = defaultdict()\n        self.grainstats = None\n\n    def tidy_border(self, image: np.array, **kwargs) -&gt; np.array:\n        \"\"\"Remove grains touching the border.\n\n        Parameters\n        ----------\n        image: np.array\n            Numpy array representing image.\n\n        Returns\n        -------\n        np.array\n            Numpy array of image with borders tidied.\n        \"\"\"\n        LOGGER.info(f\"[{self.filename}] : Tidying borders\")\n        return clear_border(image, **kwargs)\n\n    def label_regions(self, image: np.array) -&gt; np.array:\n        \"\"\"Label regions.\n\n        This method is used twice, once prior to removal of small regions, and again afterwards, hence requiring an\n        argument of what image to label.\n\n        Parameters\n        ----------\n        image: np.array\n            Numpy array representing image.\n\n        Returns\n        -------\n        np.array\n            Numpy array of image with objects coloured.\n        \"\"\"\n        LOGGER.info(f\"[{self.filename}] : Labelling Regions\")\n        return morphology.label(image, background=0)\n\n    def calc_minimum_grain_size(self, image: np.ndarray) -&gt; float:\n        \"\"\"Calculate the minimum grain size in pixels squared.\n\n        Very small objects are first removed via thresholding before calculating the below extreme.\n        \"\"\"\n        region_properties = self.get_region_properties(image)\n        grain_areas = np.array([grain.area for grain in region_properties])\n        if len(grain_areas &gt; 0):\n            # Exclude small objects less than a given threshold first\n            grain_areas = grain_areas[\n                grain_areas &gt;= threshold(grain_areas, method=\"otsu\", otsu_threshold_multiplier=1.0)\n            ]\n            self.minimum_grain_size = np.median(grain_areas) - (\n                1.5 * (np.quantile(grain_areas, 0.75) - np.quantile(grain_areas, 0.25))\n            )\n        else:\n            self.minimum_grain_size = -1\n\n    def remove_noise(self, image: np.ndarray, **kwargs) -&gt; np.ndarray:\n        \"\"\"Remove noise which are objects smaller than the 'smallest_grain_size_nm2'.\n\n        This ensures that the smallest objects ~1px are removed regardless of the size distribution of the grains.\n\n        Parameters\n        ----------\n        image: np.ndarray\n            2D Numpy image to be cleaned.\n\n        Returns\n        -------\n        np.ndarray\n            2D Numpy array of image with objects &lt; smallest_grain_size_nm2 removed.\n        \"\"\"\n        LOGGER.info(\n            f\"[{self.filename}] : Removing noise (&lt; {self.smallest_grain_size_nm2} nm^2\"\n            \"{self.smallest_grain_size_nm2 / (self.pixel_to_nm_scaling**2):.2f} px^2)\"\n        )\n        return morphology.remove_small_objects(\n            image, min_size=self.smallest_grain_size_nm2 / (self.pixel_to_nm_scaling**2), **kwargs\n        )\n\n    def remove_small_objects(self, image: np.array, **kwargs):\n        \"\"\"Remove small objects from the input image.\n\n        Threshold determined by the minimum grain size, in pixels squared, of the classes initialisation.\n\n        Parameters\n        ----------\n        image: np.ndarray\n            2D Numpy image to remove small objects from.\n\n        Returns\n        -------\n        np.ndarray\n            2D Numpy array of image with objects &lt; minimum_grain_size removed.\n        \"\"\"\n        # If self.minimum_grain_size is -1, then this means that\n        # there were no grains to calculate the minimum grian size from.\n        if self.minimum_grain_size != -1:\n            small_objects_removed = morphology.remove_small_objects(\n                image,\n                min_size=self.minimum_grain_size,  # minimum_grain_size is in pixels squared\n                **kwargs,\n            )\n            LOGGER.info(\n                f\"[{self.filename}] : Removed small objects (&lt; \\\n{self.minimum_grain_size} px^2 / {self.minimum_grain_size / (self.pixel_to_nm_scaling)**2} nm^2)\"\n            )\n            return small_objects_removed &gt; 0.0\n        return image\n\n    def area_thresholding(self, image: np.ndarray, area_thresholds: list):\n        \"\"\"Remove objects larger and smaller than the specified thresholds.\n\n        Parameters\n        ----------\n        image: np.ndarray\n            Image array where the background == 0 and grains are labelled as integers &gt; 0.\n        area_thresholds: list\n            List of area thresholds (in nanometres squared, not pixels squared), first should be\n            the lower limit for size, and the second should be the upper limit for size.\n\n        Returns\n        -------\n        np.ndarray\n            Image where grains outside the thresholds have been removed, as a re-numbered labeled image.\n\n        \"\"\"\n        image_cp = image.copy()\n        lower_size_limit, upper_size_limit = area_thresholds\n        # if one value is None adjust for comparison\n        if upper_size_limit is None:\n            upper_size_limit = image.size * self.pixel_to_nm_scaling**2\n        if lower_size_limit is None:\n            lower_size_limit = 0\n        # Get array of grain numbers (discounting zero)\n        uniq = np.delete(np.unique(image), 0)\n        grain_count = 0\n        LOGGER.info(\n            f\"[{self.filename}] : Area thresholding grains | Thresholds: L: {(lower_size_limit / self.pixel_to_nm_scaling**2):.2f},\"\n            f\"U: {(upper_size_limit / self.pixel_to_nm_scaling**2):.2f} px^2, L: {lower_size_limit:.2f}, U: {upper_size_limit:.2f} nm^2.\"\n        )\n        for grain_no in uniq:  # Calculate grian area in nm^2\n            grain_area = np.sum(image_cp == grain_no) * (self.pixel_to_nm_scaling**2)\n            # Compare area in nm^2 to area thresholds\n            if grain_area &gt; upper_size_limit or grain_area &lt; lower_size_limit:\n                image_cp[image_cp == grain_no] = 0\n            else:\n                grain_count += 1\n                image_cp[image_cp == grain_no] = grain_count\n        return image_cp\n\n    def colour_regions(self, image: np.array, **kwargs) -&gt; np.array:\n        \"\"\"Colour the regions.\n\n        Parameters\n        ----------\n        image: np.array\n            Numpy array representing image.\n\n        Returns\n        -------\n        np.array\n            Numpy array of image with objects coloured.\n        \"\"\"\n        coloured_regions = label2rgb(image, **kwargs)\n        LOGGER.info(f\"[{self.filename}] : Coloured regions\")\n        return coloured_regions\n\n    @staticmethod\n    def get_region_properties(image: np.array, **kwargs) -&gt; list:\n        \"\"\"Extract the properties of each region.\n\n        Parameters\n        ----------\n        image: np.array\n            Numpy array representing image\n\n        Returns\n        -------\n        List\n            List of region property objects.\n        \"\"\"\n        return regionprops(image, **kwargs)\n\n    def get_bounding_boxes(self, direction) -&gt; dict:\n        \"\"\"Derive a list of bounding boxes for each region from the derived region_properties.\n\n        Parameters\n        ----------\n        direction: str\n            Direction of threshold for which bounding boxes are being calculated.\n\n        Returns\n        -------\n        dict\n            Dictionary of bounding boxes indexed by region area.\n        \"\"\"\n        return {region.area: region.area_bbox for region in self.region_properties[direction]}\n\n    def find_grains(self):\n        \"\"\"Find grains.\"\"\"\n        LOGGER.info(f\"[{self.filename}] : Thresholding method (grains) : {self.threshold_method}\")\n        self.thresholds = get_thresholds(\n            image=self.image,\n            threshold_method=self.threshold_method,\n            otsu_threshold_multiplier=self.otsu_threshold_multiplier,\n            threshold_std_dev=self.threshold_std_dev,\n            absolute=self.threshold_absolute,\n        )\n        for direction in self.direction:\n            LOGGER.info(f\"[{self.filename}] : Finding {direction} grains, threshold: ({self.thresholds[direction]})\")\n            self.directions[direction] = {}\n            self.directions[direction][\"mask_grains\"] = _get_mask(\n                self.image,\n                thresh=self.thresholds[direction],\n                threshold_direction=direction,\n                img_name=self.filename,\n            )\n            self.directions[direction][\"labelled_regions_01\"] = self.label_regions(\n                self.directions[direction][\"mask_grains\"]\n            )\n\n            if self.remove_edge_intersecting_grains:\n                self.directions[direction][\"tidied_border\"] = self.tidy_border(\n                    self.directions[direction][\"labelled_regions_01\"]\n                )\n            else:\n                self.directions[direction][\"tidied_border\"] = self.directions[direction][\"labelled_regions_01\"]\n\n            LOGGER.info(f\"[{self.filename}] : Removing noise ({direction})\")\n            self.directions[direction][\"removed_noise\"] = self.area_thresholding(\n                self.directions[direction][\"tidied_border\"],\n                [self.smallest_grain_size_nm2, None],\n            )\n\n            LOGGER.info(f\"[{self.filename}] : Removing small / large grains ({direction})\")\n            # if no area thresholds specified, use otsu\n            if self.absolute_area_threshold[direction].count(None) == 2:\n                self.calc_minimum_grain_size(self.directions[direction][\"removed_noise\"])\n                self.directions[direction][\"removed_small_objects\"] = self.remove_small_objects(\n                    self.directions[direction][\"removed_noise\"]\n                )\n            else:\n                self.directions[direction][\"removed_small_objects\"] = self.area_thresholding(\n                    self.directions[direction][\"removed_noise\"],\n                    self.absolute_area_threshold[direction],\n                )\n            self.directions[direction][\"labelled_regions_02\"] = self.label_regions(\n                self.directions[direction][\"removed_small_objects\"]\n            )\n\n            self.region_properties[direction] = self.get_region_properties(\n                self.directions[direction][\"labelled_regions_02\"]\n            )\n            LOGGER.info(f\"[{self.filename}] : Region properties calculated ({direction})\")\n            self.directions[direction][\"coloured_regions\"] = self.colour_regions(\n                self.directions[direction][\"labelled_regions_02\"]\n            )\n            self.bounding_boxes[direction] = self.get_bounding_boxes(direction=direction)\n            LOGGER.info(f\"[{self.filename}] : Extracted bounding boxes ({direction})\")\n</code></pre>"},{"location":"api/grains/#topostats.grains.Grains.__init__","title":"<code>__init__(image: np.ndarray, filename: str, pixel_to_nm_scaling: float, threshold_method: str = None, otsu_threshold_multiplier: float = None, threshold_std_dev: dict = None, threshold_absolute: dict = None, absolute_area_threshold: dict = None, direction: str = None, smallest_grain_size_nm2: float = None, remove_edge_intersecting_grains: bool = True)</code>","text":"<p>Initialise the class.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>2D Numpy array of image</p> required <code>filename</code> <code>str</code> <p>File being processed</p> required <code>pixel_to_nm_scaling</code> <code>float</code> <p>Scaling of pixels to nanometre.</p> required <code>threshold_multiplier</code> <code>Union[int, float]</code> <p>Factor by which below threshold is to be scaled prior to masking.</p> required <code>threshold_method</code> <code>str</code> <p>Method for determining threshold to mask values, default is 'otsu'.</p> <code>None</code> <code>threshold_std_dev</code> <code>dict</code> <p>Dictionary of 'below' and 'above' factors by which standard deviation is multiplied to derive the threshold if threshold_method is 'std_dev'.</p> <code>None</code> <code>threshold_absolute</code> <code>dict</code> <p>Dictionary of absolute 'below' and 'above' thresholds for grain finding.</p> <code>None</code> <code>absolute_area_threshold</code> <code>dict</code> <p>Dictionary of above and below grain's area thresholds</p> <code>None</code> <code>direction</code> <code>str</code> <p>Direction for which grains are to be detected, valid values are above, below and both.</p> <code>None</code> <code>remove_edge_intersecting_grains</code> <code>bool</code> <p>Whether or not to remove grains that intersect the edge of the image.</p> <code>True</code> Source code in <code>topostats\\grains.py</code> <pre><code>def __init__(\n    self,\n    image: np.ndarray,\n    filename: str,\n    pixel_to_nm_scaling: float,\n    threshold_method: str = None,\n    otsu_threshold_multiplier: float = None,\n    threshold_std_dev: dict = None,\n    threshold_absolute: dict = None,\n    absolute_area_threshold: dict = None,\n    direction: str = None,\n    smallest_grain_size_nm2: float = None,\n    remove_edge_intersecting_grains: bool = True,\n):\n    \"\"\"Initialise the class.\n\n    Parameters\n    ----------\n    image: np.ndarray\n        2D Numpy array of image\n    filename: str\n        File being processed\n    pixel_to_nm_scaling: float\n        Scaling of pixels to nanometre.\n    threshold_multiplier : Union[int, float]\n        Factor by which below threshold is to be scaled prior to masking.\n    threshold_method: str\n        Method for determining threshold to mask values, default is 'otsu'.\n    threshold_std_dev: dict\n        Dictionary of 'below' and 'above' factors by which standard deviation is multiplied to derive the threshold if threshold_method is 'std_dev'.\n    threshold_absolute: dict\n        Dictionary of absolute 'below' and 'above' thresholds for grain finding.\n    absolute_area_threshold: dict\n        Dictionary of above and below grain's area thresholds\n    direction: str\n        Direction for which grains are to be detected, valid values are above, below and both.\n    remove_edge_intersecting_grains: bool\n        Whether or not to remove grains that intersect the edge of the image.\n    \"\"\"\n    if absolute_area_threshold is None:\n        absolute_area_threshold = {\"above\": [None, None], \"below\": [None, None]}\n    self.image = image\n    self.filename = filename\n    self.pixel_to_nm_scaling = pixel_to_nm_scaling\n    self.threshold_method = threshold_method\n    self.otsu_threshold_multiplier = otsu_threshold_multiplier\n    self.threshold_std_dev = threshold_std_dev\n    self.threshold_absolute = threshold_absolute\n    self.absolute_area_threshold = absolute_area_threshold\n    # Only detect grains for the desired direction\n    self.direction = [direction] if direction != \"both\" else [\"above\", \"below\"]\n    self.smallest_grain_size_nm2 = smallest_grain_size_nm2\n    self.remove_edge_intersecting_grains = remove_edge_intersecting_grains\n    self.thresholds = None\n    self.images = {\n        \"mask_grains\": None,\n        \"tidied_border\": None,\n        \"tiny_objects_removed\": None,\n        \"objects_removed\": None,\n        # \"labelled_regions\": None,\n        # \"coloured_regions\": None,\n    }\n    self.directions = defaultdict()\n    self.minimum_grain_size = None\n    self.region_properties = defaultdict()\n    self.bounding_boxes = defaultdict()\n    self.grainstats = None\n</code></pre>"},{"location":"api/grains/#topostats.grains.Grains.area_thresholding","title":"<code>area_thresholding(image: np.ndarray, area_thresholds: list)</code>","text":"<p>Remove objects larger and smaller than the specified thresholds.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Image array where the background == 0 and grains are labelled as integers &gt; 0.</p> required <code>area_thresholds</code> <code>list</code> <p>List of area thresholds (in nanometres squared, not pixels squared), first should be the lower limit for size, and the second should be the upper limit for size.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Image where grains outside the thresholds have been removed, as a re-numbered labeled image.</p> Source code in <code>topostats\\grains.py</code> <pre><code>def area_thresholding(self, image: np.ndarray, area_thresholds: list):\n    \"\"\"Remove objects larger and smaller than the specified thresholds.\n\n    Parameters\n    ----------\n    image: np.ndarray\n        Image array where the background == 0 and grains are labelled as integers &gt; 0.\n    area_thresholds: list\n        List of area thresholds (in nanometres squared, not pixels squared), first should be\n        the lower limit for size, and the second should be the upper limit for size.\n\n    Returns\n    -------\n    np.ndarray\n        Image where grains outside the thresholds have been removed, as a re-numbered labeled image.\n\n    \"\"\"\n    image_cp = image.copy()\n    lower_size_limit, upper_size_limit = area_thresholds\n    # if one value is None adjust for comparison\n    if upper_size_limit is None:\n        upper_size_limit = image.size * self.pixel_to_nm_scaling**2\n    if lower_size_limit is None:\n        lower_size_limit = 0\n    # Get array of grain numbers (discounting zero)\n    uniq = np.delete(np.unique(image), 0)\n    grain_count = 0\n    LOGGER.info(\n        f\"[{self.filename}] : Area thresholding grains | Thresholds: L: {(lower_size_limit / self.pixel_to_nm_scaling**2):.2f},\"\n        f\"U: {(upper_size_limit / self.pixel_to_nm_scaling**2):.2f} px^2, L: {lower_size_limit:.2f}, U: {upper_size_limit:.2f} nm^2.\"\n    )\n    for grain_no in uniq:  # Calculate grian area in nm^2\n        grain_area = np.sum(image_cp == grain_no) * (self.pixel_to_nm_scaling**2)\n        # Compare area in nm^2 to area thresholds\n        if grain_area &gt; upper_size_limit or grain_area &lt; lower_size_limit:\n            image_cp[image_cp == grain_no] = 0\n        else:\n            grain_count += 1\n            image_cp[image_cp == grain_no] = grain_count\n    return image_cp\n</code></pre>"},{"location":"api/grains/#topostats.grains.Grains.calc_minimum_grain_size","title":"<code>calc_minimum_grain_size(image: np.ndarray) -&gt; float</code>","text":"<p>Calculate the minimum grain size in pixels squared.</p> <p>Very small objects are first removed via thresholding before calculating the below extreme.</p> Source code in <code>topostats\\grains.py</code> <pre><code>def calc_minimum_grain_size(self, image: np.ndarray) -&gt; float:\n    \"\"\"Calculate the minimum grain size in pixels squared.\n\n    Very small objects are first removed via thresholding before calculating the below extreme.\n    \"\"\"\n    region_properties = self.get_region_properties(image)\n    grain_areas = np.array([grain.area for grain in region_properties])\n    if len(grain_areas &gt; 0):\n        # Exclude small objects less than a given threshold first\n        grain_areas = grain_areas[\n            grain_areas &gt;= threshold(grain_areas, method=\"otsu\", otsu_threshold_multiplier=1.0)\n        ]\n        self.minimum_grain_size = np.median(grain_areas) - (\n            1.5 * (np.quantile(grain_areas, 0.75) - np.quantile(grain_areas, 0.25))\n        )\n    else:\n        self.minimum_grain_size = -1\n</code></pre>"},{"location":"api/grains/#topostats.grains.Grains.colour_regions","title":"<code>colour_regions(image: np.array, **kwargs) -&gt; np.array</code>","text":"<p>Colour the regions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>array</code> <p>Numpy array representing image.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Numpy array of image with objects coloured.</p> Source code in <code>topostats\\grains.py</code> <pre><code>def colour_regions(self, image: np.array, **kwargs) -&gt; np.array:\n    \"\"\"Colour the regions.\n\n    Parameters\n    ----------\n    image: np.array\n        Numpy array representing image.\n\n    Returns\n    -------\n    np.array\n        Numpy array of image with objects coloured.\n    \"\"\"\n    coloured_regions = label2rgb(image, **kwargs)\n    LOGGER.info(f\"[{self.filename}] : Coloured regions\")\n    return coloured_regions\n</code></pre>"},{"location":"api/grains/#topostats.grains.Grains.find_grains","title":"<code>find_grains()</code>","text":"<p>Find grains.</p> Source code in <code>topostats\\grains.py</code> <pre><code>def find_grains(self):\n    \"\"\"Find grains.\"\"\"\n    LOGGER.info(f\"[{self.filename}] : Thresholding method (grains) : {self.threshold_method}\")\n    self.thresholds = get_thresholds(\n        image=self.image,\n        threshold_method=self.threshold_method,\n        otsu_threshold_multiplier=self.otsu_threshold_multiplier,\n        threshold_std_dev=self.threshold_std_dev,\n        absolute=self.threshold_absolute,\n    )\n    for direction in self.direction:\n        LOGGER.info(f\"[{self.filename}] : Finding {direction} grains, threshold: ({self.thresholds[direction]})\")\n        self.directions[direction] = {}\n        self.directions[direction][\"mask_grains\"] = _get_mask(\n            self.image,\n            thresh=self.thresholds[direction],\n            threshold_direction=direction,\n            img_name=self.filename,\n        )\n        self.directions[direction][\"labelled_regions_01\"] = self.label_regions(\n            self.directions[direction][\"mask_grains\"]\n        )\n\n        if self.remove_edge_intersecting_grains:\n            self.directions[direction][\"tidied_border\"] = self.tidy_border(\n                self.directions[direction][\"labelled_regions_01\"]\n            )\n        else:\n            self.directions[direction][\"tidied_border\"] = self.directions[direction][\"labelled_regions_01\"]\n\n        LOGGER.info(f\"[{self.filename}] : Removing noise ({direction})\")\n        self.directions[direction][\"removed_noise\"] = self.area_thresholding(\n            self.directions[direction][\"tidied_border\"],\n            [self.smallest_grain_size_nm2, None],\n        )\n\n        LOGGER.info(f\"[{self.filename}] : Removing small / large grains ({direction})\")\n        # if no area thresholds specified, use otsu\n        if self.absolute_area_threshold[direction].count(None) == 2:\n            self.calc_minimum_grain_size(self.directions[direction][\"removed_noise\"])\n            self.directions[direction][\"removed_small_objects\"] = self.remove_small_objects(\n                self.directions[direction][\"removed_noise\"]\n            )\n        else:\n            self.directions[direction][\"removed_small_objects\"] = self.area_thresholding(\n                self.directions[direction][\"removed_noise\"],\n                self.absolute_area_threshold[direction],\n            )\n        self.directions[direction][\"labelled_regions_02\"] = self.label_regions(\n            self.directions[direction][\"removed_small_objects\"]\n        )\n\n        self.region_properties[direction] = self.get_region_properties(\n            self.directions[direction][\"labelled_regions_02\"]\n        )\n        LOGGER.info(f\"[{self.filename}] : Region properties calculated ({direction})\")\n        self.directions[direction][\"coloured_regions\"] = self.colour_regions(\n            self.directions[direction][\"labelled_regions_02\"]\n        )\n        self.bounding_boxes[direction] = self.get_bounding_boxes(direction=direction)\n        LOGGER.info(f\"[{self.filename}] : Extracted bounding boxes ({direction})\")\n</code></pre>"},{"location":"api/grains/#topostats.grains.Grains.get_bounding_boxes","title":"<code>get_bounding_boxes(direction) -&gt; dict</code>","text":"<p>Derive a list of bounding boxes for each region from the derived region_properties.</p> <p>Parameters:</p> Name Type Description Default <code>direction</code> <p>Direction of threshold for which bounding boxes are being calculated.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of bounding boxes indexed by region area.</p> Source code in <code>topostats\\grains.py</code> <pre><code>def get_bounding_boxes(self, direction) -&gt; dict:\n    \"\"\"Derive a list of bounding boxes for each region from the derived region_properties.\n\n    Parameters\n    ----------\n    direction: str\n        Direction of threshold for which bounding boxes are being calculated.\n\n    Returns\n    -------\n    dict\n        Dictionary of bounding boxes indexed by region area.\n    \"\"\"\n    return {region.area: region.area_bbox for region in self.region_properties[direction]}\n</code></pre>"},{"location":"api/grains/#topostats.grains.Grains.get_region_properties","title":"<code>get_region_properties(image: np.array, **kwargs) -&gt; list</code>  <code>staticmethod</code>","text":"<p>Extract the properties of each region.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>array</code> <p>Numpy array representing image</p> required <p>Returns:</p> Type Description <code>List</code> <p>List of region property objects.</p> Source code in <code>topostats\\grains.py</code> <pre><code>@staticmethod\ndef get_region_properties(image: np.array, **kwargs) -&gt; list:\n    \"\"\"Extract the properties of each region.\n\n    Parameters\n    ----------\n    image: np.array\n        Numpy array representing image\n\n    Returns\n    -------\n    List\n        List of region property objects.\n    \"\"\"\n    return regionprops(image, **kwargs)\n</code></pre>"},{"location":"api/grains/#topostats.grains.Grains.label_regions","title":"<code>label_regions(image: np.array) -&gt; np.array</code>","text":"<p>Label regions.</p> <p>This method is used twice, once prior to removal of small regions, and again afterwards, hence requiring an argument of what image to label.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>array</code> <p>Numpy array representing image.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Numpy array of image with objects coloured.</p> Source code in <code>topostats\\grains.py</code> <pre><code>def label_regions(self, image: np.array) -&gt; np.array:\n    \"\"\"Label regions.\n\n    This method is used twice, once prior to removal of small regions, and again afterwards, hence requiring an\n    argument of what image to label.\n\n    Parameters\n    ----------\n    image: np.array\n        Numpy array representing image.\n\n    Returns\n    -------\n    np.array\n        Numpy array of image with objects coloured.\n    \"\"\"\n    LOGGER.info(f\"[{self.filename}] : Labelling Regions\")\n    return morphology.label(image, background=0)\n</code></pre>"},{"location":"api/grains/#topostats.grains.Grains.remove_noise","title":"<code>remove_noise(image: np.ndarray, **kwargs) -&gt; np.ndarray</code>","text":"<p>Remove noise which are objects smaller than the 'smallest_grain_size_nm2'.</p> <p>This ensures that the smallest objects ~1px are removed regardless of the size distribution of the grains.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>2D Numpy image to be cleaned.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>2D Numpy array of image with objects &lt; smallest_grain_size_nm2 removed.</p> Source code in <code>topostats\\grains.py</code> <pre><code>def remove_noise(self, image: np.ndarray, **kwargs) -&gt; np.ndarray:\n    \"\"\"Remove noise which are objects smaller than the 'smallest_grain_size_nm2'.\n\n    This ensures that the smallest objects ~1px are removed regardless of the size distribution of the grains.\n\n    Parameters\n    ----------\n    image: np.ndarray\n        2D Numpy image to be cleaned.\n\n    Returns\n    -------\n    np.ndarray\n        2D Numpy array of image with objects &lt; smallest_grain_size_nm2 removed.\n    \"\"\"\n    LOGGER.info(\n        f\"[{self.filename}] : Removing noise (&lt; {self.smallest_grain_size_nm2} nm^2\"\n        \"{self.smallest_grain_size_nm2 / (self.pixel_to_nm_scaling**2):.2f} px^2)\"\n    )\n    return morphology.remove_small_objects(\n        image, min_size=self.smallest_grain_size_nm2 / (self.pixel_to_nm_scaling**2), **kwargs\n    )\n</code></pre>"},{"location":"api/grains/#topostats.grains.Grains.remove_small_objects","title":"<code>remove_small_objects(image: np.array, **kwargs)</code>","text":"<p>Remove small objects from the input image.</p> <p>Threshold determined by the minimum grain size, in pixels squared, of the classes initialisation.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>array</code> <p>2D Numpy image to remove small objects from.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>2D Numpy array of image with objects &lt; minimum_grain_size removed.</p> Source code in <code>topostats\\grains.py</code> <pre><code>    def remove_small_objects(self, image: np.array, **kwargs):\n        \"\"\"Remove small objects from the input image.\n\n        Threshold determined by the minimum grain size, in pixels squared, of the classes initialisation.\n\n        Parameters\n        ----------\n        image: np.ndarray\n            2D Numpy image to remove small objects from.\n\n        Returns\n        -------\n        np.ndarray\n            2D Numpy array of image with objects &lt; minimum_grain_size removed.\n        \"\"\"\n        # If self.minimum_grain_size is -1, then this means that\n        # there were no grains to calculate the minimum grian size from.\n        if self.minimum_grain_size != -1:\n            small_objects_removed = morphology.remove_small_objects(\n                image,\n                min_size=self.minimum_grain_size,  # minimum_grain_size is in pixels squared\n                **kwargs,\n            )\n            LOGGER.info(\n                f\"[{self.filename}] : Removed small objects (&lt; \\\n{self.minimum_grain_size} px^2 / {self.minimum_grain_size / (self.pixel_to_nm_scaling)**2} nm^2)\"\n            )\n            return small_objects_removed &gt; 0.0\n        return image\n</code></pre>"},{"location":"api/grains/#topostats.grains.Grains.tidy_border","title":"<code>tidy_border(image: np.array, **kwargs) -&gt; np.array</code>","text":"<p>Remove grains touching the border.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>array</code> <p>Numpy array representing image.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Numpy array of image with borders tidied.</p> Source code in <code>topostats\\grains.py</code> <pre><code>def tidy_border(self, image: np.array, **kwargs) -&gt; np.array:\n    \"\"\"Remove grains touching the border.\n\n    Parameters\n    ----------\n    image: np.array\n        Numpy array representing image.\n\n    Returns\n    -------\n    np.array\n        Numpy array of image with borders tidied.\n    \"\"\"\n    LOGGER.info(f\"[{self.filename}] : Tidying borders\")\n    return clear_border(image, **kwargs)\n</code></pre>"},{"location":"api/grainstats/","title":"Grainstats Modules","text":"<p>Contains class for calculating the statistics of grains - 2d raster images.</p>"},{"location":"api/grainstats/#topostats.grainstats.GrainStats","title":"<code>GrainStats</code>","text":"<p>Class for calculating grain stats.</p> Source code in <code>topostats\\grainstats.py</code> <pre><code>class GrainStats:\n    \"\"\"Class for calculating grain stats.\"\"\"\n\n    def __init__(\n        self,\n        data: np.ndarray,\n        labelled_data: np.ndarray,\n        pixel_to_nanometre_scaling: float,\n        direction: str,\n        base_output_dir: str | Path,\n        image_name: str = None,\n        edge_detection_method: str = \"binary_erosion\",\n        cropped_size: float = -1,\n        plot_opts: dict = None,\n        metre_scaling_factor: float = 1e-9,\n    ):\n        \"\"\"Initialise the class.\n\n        Parameters\n        ----------\n        data : np.ndarray\n            2D Numpy array containing the flattened afm image. Data in this 2D array is floating point.\n        labelled_data : np.ndarray\n            2D Numpy array containing all the grain masks in the image. Data in this 2D array is boolean.\n        pixel_to_nanometre_scaling : float\n            Floating point value that defines the scaling factor between nanometres and pixels.\n        direction: str\n            Direction for which grains have been detected (\"above\" or \"below\").\n        base_output_dir : Path\n            Path to the folder that will store the grain stats output images and data.\n        image_name : str\n            The name of the file being processed.\n        edge_detection_method : str\n            Method used for detecting the edges of grain masks before calculating statistics on them.\n            Do not change unless you know exactly what this is doing. Options: \"binary_erosion\", \"canny\".\n        cropped_size : float\n            Length of square side (in nm) to crop grains to.\n        plot_opts : dict\n            Plotting options dictionary for the cropped grains\n        metre_scaling_factor : float\n            Multiplier to convert the current length scale to metres. Default: 1e-9 for the\n            usual AFM length scale of nanometres.\n        \"\"\"\n        self.data = data\n        self.labelled_data = labelled_data\n        self.pixel_to_nanometre_scaling = pixel_to_nanometre_scaling\n        self.direction = direction\n        self.base_output_dir = Path(base_output_dir)\n        self.start_point = None\n        self.image_name = image_name\n        self.edge_detection_method = edge_detection_method\n        self.cropped_size = cropped_size\n        self.plot_opts = plot_opts\n        self.metre_scaling_factor = metre_scaling_factor\n\n    @staticmethod\n    def get_angle(point_1: tuple, point_2: tuple) -&gt; float:\n        \"\"\"Calculate the angle in radians between two points.\n\n        Parameters\n        ----------\n        point1: tuple\n            Coordinate vectors for the first point to find the angle between.\n        point2: tuple\n            Coordinate vectors for the second point to find the angle between.\n\n        Returns\n        -------\n        angle : float\n            The angle in radians between the two input vectors.\n        \"\"\"\n        return np.arctan2(point_1[1] - point_2[1], point_1[0] - point_2[0])\n\n    @staticmethod\n    def is_clockwise(p_1: tuple, p_2: tuple, p_3: tuple) -&gt; bool:\n        \"\"\"Determine if three points make a clockwise or counter-clockwise turn.\n\n        Parameters\n        ----------\n        p_1: tuple\n            First point to be used to calculate turn.\n        p_2: tuple\n            Second point to be used to calculate turn.\n        p_3: tuple\n            Third point to be used to calculate turn.\n\n        Returns\n        -------\n        boolean\n            Indicator of whether turn is clockwise.\n        \"\"\"\n        # Determine if three points form a clockwise or counter-clockwise turn.\n        # I use the method of calculating the determinant of the following rotation matrix here. If the determinant\n        # is &gt; 0 then the rotation is counter-clockwise.\n        rotation_matrix = np.array(((p_1[0], p_1[1], 1), (p_2[0], p_2[1], 1), (p_3[0], p_3[1], 1)))\n        return not np.linalg.det(rotation_matrix) &gt; 0\n\n    def calculate_stats(self) -&gt; dict:\n        \"\"\"Calculate the stats of grains in the labelled image.\n\n        Returns\n        -------\n        grainstats: pd.DataFrame\n            A DataFrame containing all the grain stats that have been calculated for the labelled image.\n        grains_plot_data:\n            A list of dictionaries containing grain data to be plotted.\n        \"\"\"\n        grains_plot_data = []\n        if self.labelled_data is None:\n            LOGGER.info(\n                f\"[{self.image_name}] : No labelled regions for this image, grain statistics can not be calculated.\"\n            )\n            return pd.DataFrame(columns=GRAIN_STATS_COLUMNS), grains_plot_data\n\n        # Calculate region properties\n        region_properties = skimage_measure.regionprops(self.labelled_data)\n\n        # Iterate over all the grains in the image\n        stats_array = []\n        # List to hold all the plot data for all the grains. Each entry is a dictionary of plotting data.\n        # There are multiple entries for each grain.\n        for index, region in enumerate(region_properties):\n            LOGGER.info(f\"[{self.image_name}] : Processing grain: {index}\")\n\n            # Skip grain if too small to calculate stats for\n            LOGGER.debug(f\"[{self.image_name}] : Grain size: {region.image.size}\")\n            if min(region.image.shape) &lt; 5:\n                LOGGER.info(\n                    f\"[{self.image_name}] : Skipping grain due to being too small (size: {region.image.shape}) to calculate stats for.\"\n                )\n                continue\n\n            # Create directory for each grain's plots\n            output_grain = self.base_output_dir / self.direction\n            # Obtain cropped grain mask and image\n            minr, minc, maxr, maxc = region.bbox\n            grain_mask = np.array(region.image)\n            grain_image = self.data[minr:maxr, minc:maxc]\n            grain_mask_image = np.ma.masked_array(grain_image, mask=np.invert(grain_mask), fill_value=np.nan).filled()\n\n            if self.cropped_size == -1:\n                for name, image in {\n                    \"grain_image\": grain_image,\n                    \"grain_mask\": grain_mask,\n                    \"grain_mask_image\": grain_mask_image,\n                }.items():\n                    grains_plot_data.append(\n                        {\n                            \"data\": image,\n                            \"output_dir\": output_grain,\n                            \"filename\": f\"{self.image_name}_{name}_{index}\",\n                            \"name\": name,\n                        }\n                    )\n\n            else:\n                # Get cropped image and mask\n                grain_centre = int((minr + maxr) / 2), int((minc + maxc) / 2)\n                length = int(self.cropped_size / (2 * self.pixel_to_nanometre_scaling))\n                solo_mask = self.labelled_data.copy()\n                solo_mask[solo_mask != index + 1] = 0\n                solo_mask[solo_mask == index + 1] = 1\n                cropped_grain_image = self.get_cropped_region(self.data, length, np.asarray(grain_centre))\n                cropped_grain_mask = self.get_cropped_region(solo_mask, length, np.asarray(grain_centre)).astype(bool)\n                cropped_grain_mask_image = np.ma.masked_array(\n                    grain_image, mask=np.invert(grain_mask), fill_value=np.nan\n                ).filled()\n                for name, image in {\n                    \"grain_image\": cropped_grain_image,\n                    \"grain_mask\": cropped_grain_mask,\n                    \"grain_mask_image\": cropped_grain_mask_image,\n                }.items():\n                    grains_plot_data.append(\n                        {\n                            \"data\": image,\n                            \"output_dir\": output_grain,\n                            \"filename\": f\"{self.image_name}_{name}_{index}\",\n                            \"name\": name,\n                        }\n                    )\n\n            points = self.calculate_points(grain_mask)\n            edges = self.calculate_edges(grain_mask, edge_detection_method=self.edge_detection_method)\n            radius_stats = self.calculate_radius_stats(edges, points)\n            # hull, hull_indices, hull_simplexes = self.convex_hull(edges, output_grain)\n            _, _, hull_simplexes = self.convex_hull(edges, output_grain)\n            centroid = self._calculate_centroid(points)\n            # Centroids for the grains (minc and minr added because centroid returns values local to the cropped grain images)\n            centre_x = centroid[0] + minc\n            centre_y = centroid[1] + minr\n            (\n                smallest_bounding_width,\n                smallest_bounding_length,\n                aspect_ratio,\n            ) = self.calculate_aspect_ratio(\n                edges=edges,\n                hull_simplices=hull_simplexes,\n                path=output_grain,\n            )\n\n            # Calculate minimum and maximum feret diameters\n            min_feret, max_feret = self.get_max_min_ferets(edge_points=edges)\n\n            # Save the stats to dictionary. Note that many of the stats are multiplied by a scaling factor to convert\n            # from pixel units to nanometres.\n            # Removed formatting, better to keep accurate until the end, including in CSV, then shorten display\n            length_scaling_factor = self.pixel_to_nanometre_scaling * self.metre_scaling_factor\n            area_scaling_factor = length_scaling_factor**2\n            stats = {\n                \"centre_x\": centre_x * length_scaling_factor,\n                \"centre_y\": centre_y * length_scaling_factor,\n                \"radius_min\": radius_stats[\"min\"] * length_scaling_factor,\n                \"radius_max\": radius_stats[\"max\"] * length_scaling_factor,\n                \"radius_mean\": radius_stats[\"mean\"] * length_scaling_factor,\n                \"radius_median\": radius_stats[\"median\"] * length_scaling_factor,\n                \"height_min\": np.nanmin(grain_mask_image) * self.metre_scaling_factor,\n                \"height_max\": np.nanmax(grain_mask_image) * self.metre_scaling_factor,\n                \"height_median\": np.nanmedian(grain_mask_image) * self.metre_scaling_factor,\n                \"height_mean\": np.nanmean(grain_mask_image) * self.metre_scaling_factor,\n                # [volume] = [pixel] * [pixel] * [height] = px * px * nm.\n                # To turn into m^3, multiply by pixel_to_nanometre_scaling^2 and metre_scaling_factor^3.\n                \"volume\": np.nansum(grain_mask_image)\n                * self.pixel_to_nanometre_scaling**2\n                * (self.metre_scaling_factor**3),\n                \"area\": region.area * area_scaling_factor,\n                \"area_cartesian_bbox\": region.area_bbox * area_scaling_factor,\n                \"smallest_bounding_width\": smallest_bounding_width * length_scaling_factor,\n                \"smallest_bounding_length\": smallest_bounding_length * length_scaling_factor,\n                \"smallest_bounding_area\": smallest_bounding_length * smallest_bounding_width * area_scaling_factor,\n                \"aspect_ratio\": aspect_ratio,\n                \"threshold\": self.direction,\n                \"max_feret\": max_feret * length_scaling_factor,\n                \"min_feret\": min_feret * length_scaling_factor,\n            }\n            stats_array.append(stats)\n        if len(stats_array) &gt; 0:\n            grainstats_df = pd.DataFrame(data=stats_array)\n        else:\n            grainstats_df = create_empty_dataframe()\n        grainstats_df.index.name = \"molecule_number\"\n        grainstats_df[\"image\"] = self.image_name\n\n        return grainstats_df, grains_plot_data\n\n    @staticmethod\n    def calculate_points(grain_mask: np.ndarray):\n        \"\"\"Convert a 2D boolean array to a list of coordinates.\n\n        Parameters\n        ----------\n        grain_mask : np.ndarray\n            A 2D numpy array image of a grain. Data in the array must be boolean.\n\n        Returns\n        -------\n        points : list\n        A python list containing the coordinates of the pixels in the grain.\n        \"\"\"\n        nonzero_coordinates = grain_mask.nonzero()\n        points = []\n        for point in np.transpose(nonzero_coordinates):\n            points.append(list(point))\n\n        return points\n\n    @staticmethod\n    def calculate_edges(grain_mask: np.ndarray, edge_detection_method: str):\n        \"\"\"Convert 2D boolean array to list of the coordinates of the edges of the grain.\n\n        Parameters\n        ----------\n        grain_mask : np.ndarray\n            A 2D numpy array image of a grain. Data in the array must be boolean.\n        edge_detection_method : str\n            Method used for detecting the edges of grain masks before calculating statistics on them.\n            Do not change unless you know exactly what this is doing. Options: \"binary_erosion\", \"canny\".\n\n        Returns\n        -------\n        edges : list\n            List containing the coordinates of the edges of the grain.\n        \"\"\"\n        # Fill any holes\n        filled_grain_mask = scipy.ndimage.binary_fill_holes(grain_mask)\n\n        if edge_detection_method == \"binary_erosion\":\n            # Add padding (needed for erosion)\n            padded = np.pad(filled_grain_mask, 1)\n            # Erode by 1 pixel\n            eroded = skimage_morphology.binary_erosion(padded)\n            # Remove padding\n            eroded = eroded[1:-1, 1:-1]\n\n            # Edges is equal to the difference between the\n            # original image and the eroded image.\n            edges = filled_grain_mask.astype(int) - eroded.astype(int)\n        else:\n            # Get outer edge using canny filtering\n            edges = skimage_feature.canny(filled_grain_mask, sigma=3)\n\n        nonzero_coordinates = edges.nonzero()\n        # Get vector representation of the points\n        # FIXME : Switched to list comprehension but should be unnecessary to create this as a list as we can use\n        # np.stack() to combine the arrays and use that...\n        # return np.stack(nonzero_coordinates, axis=1)\n        # edges = []\n        # for vector in np.transpose(nonzero_coordinates):\n        #     edges.append(list(vector))\n        # return edges\n        return [list(vector) for vector in np.transpose(nonzero_coordinates)]\n\n    def calculate_radius_stats(self, edges: list, points: list) -&gt; tuple:\n        \"\"\"Calculate the radius of grains.\n\n        The radius in this context is the distance from the centroid to points on the edge of the grain.\n\n        Parameters\n        ----------\n        edges: list\n            A 2D python list containing the coordinates of the edges of a grain.\n        points: list\n            A 2D python list containing the coordinates of the points in a grain.\n\n        Returns\n        -------\n        Tuple[float]\n            A tuple of the minimum, maximum, mean and median radius of the grain\n        \"\"\"\n        # Calculate the centroid of the grain\n        centroid = self._calculate_centroid(points)\n        # Calculate the displacement\n        displacements = self._calculate_displacement(edges, centroid)\n        # Calculate the radius of each point\n        radii = self._calculate_radius(displacements)\n        return {\n            \"min\": np.min(radii),\n            \"max\": np.max(radii),\n            \"mean\": np.mean(radii),\n            \"median\": np.median(radii),\n        }\n\n    @staticmethod\n    def _calculate_centroid(points: np.array) -&gt; tuple:\n        \"\"\"Calculate the centroid of a bounding box.\n\n        Parameters\n        ----------\n        points: list\n            A 2D python list containing the coordinates of the points in a grain.\n\n        Returns\n        -------\n        tuple\n            The coordinates of the centroid.\n        \"\"\"\n        # FIXME : Remove once we have a numpy array returned by calculate_edges\n        points = np.array(points)\n        return (np.mean(points[:, 0]), np.mean(points[:, 1]))\n\n    @staticmethod\n    def _calculate_displacement(edges: np.array, centroid: tuple) -&gt; np.array:\n        \"\"\"Calculate the displacement between the centroid and edges.\"\"\"\n        # FIXME : Remove once we have a numpy array returned by calculate_edges\n        return np.array(edges) - centroid\n\n    @staticmethod\n    def _calculate_radius(displacements) -&gt; np.array:\n        \"\"\"Calculate the radius of each point from the centroid.\n\n        Parameters\n        ----------\n        displacements: List[list]\n\n        Retrurns\n        --------\n        np.array\n        \"\"\"\n        return np.array([np.sqrt(radius[0] ** 2 + radius[1] ** 2) for radius in displacements])\n\n    def convex_hull(self, edges: list, base_output_dir: Path, debug: bool = False):\n        \"\"\"Calculate a grain's convex hull.\n\n        Based off of the Graham Scan algorithm and should ideally scale in time with O(nlog(n)).\n\n        Parameters\n        ----------\n        edges : list\n            A python list containing the coordinates of the edges of the grain.\n        base_output_dir : Union[str, Path]\n            Directory to save output to.\n        debug : bool\n            Default false. If true, debug information will be displayed to the terminal and plots for the convex hulls and edges will be saved.\n\n        Returns\n        -------\n        hull : list\n            Coordinates of the points in the hull.\n        hull_indices : list\n            The hull points indices inside the edges list. In other words, this provides a way to find the points from the hull inside the edges list that was passed.\n        simplices : list\n            List of tuples, each tuple representing a simplex of the convex hull. These simplices are sorted such that they follow each other in counterclockwise order.\n        \"\"\"\n        hull, hull_indices, simplexes = self.graham_scan(edges)\n\n        # Debug information\n        if debug:\n            base_output_dir.mkdir(parents=True, exist_ok=True)\n            self.plot(edges, hull, base_output_dir / \"_points_hull.png\")\n            LOGGER.info(f\"points: {edges}\")\n            LOGGER.info(f\"hull: {hull}\")\n            LOGGER.info(f\"hull indexes: {hull_indices}\")\n            LOGGER.info(f\"simplexes: {simplexes}\")\n\n        return hull, hull_indices, simplexes\n\n    def calculate_squared_distance(self, point_2: tuple, point_1: tuple = None) -&gt; float:\n        \"\"\"Calculate the squared distance between two points.\n\n        Used for distance sorting purposes and therefore does not perform a square root in the interests of efficiency.\n\n        Parameters\n        ----------\n        point_2 : tuple\n            The point to find the squared distance to.\n        point_1 : tuple\n            Optional - defaults to the starting point defined in the graham_scan() function. The point to find the\n        squared distance from.\n\n        Returns\n        -------\n        distance_squared : float\n            The squared distance between the two points.\n        \"\"\"\n        # Get the distance squared between two points. If the second point is not provided, use the starting point.\n        point_1 = self.start_point if point_1 is None else point_1\n        delta_x = point_2[0] - point_1[0]\n        delta_y = point_2[1] - point_1[1]\n        # Don't need the sqrt since just sorting for dist\n        return float(delta_x**2 + delta_y**2)\n\n    def sort_points(self, points: list) -&gt; list:\n        #    def sort_points(self, points: np.array) -&gt; List:\n        \"\"\"Sort points in counter-clockwise order of angle made with the starting point.\n\n        Parameters\n        ----------\n        points: list\n            A python list of the coordinates to sort.\n\n        Returns\n        -------\n        sorted_points : list\n            A python list of sorted points.\n        \"\"\"\n        # Return if the list is length 1 or 0 (i.e. a single point).\n        if len(points) &lt;= 1:\n            return points\n        # Lists that allow sorting of points relative to a current comparison point\n        smaller, equal, larger = [], [], []\n        # Get a random point in the array to calculate the pivot angle from. This sorts the points relative to this point.\n        pivot_angle = self.get_angle(points[randint(0, len(points) - 1)], self.start_point)  # noqa: S311\n        for point in points:\n            point_angle = self.get_angle(point, self.start_point)\n            # If the\n            if point_angle &lt; pivot_angle:\n                smaller.append(point)\n            elif point_angle == pivot_angle:\n                equal.append(point)\n            else:\n                larger.append(point)\n        # Lets take a different approach and use arrays, we have a start point lets work out the angle of each point\n        # relative to that and _then_ sort it.\n        # pivot_angles = self.get_angle(points, self.start_point)\n        # Recursively sort the arrays until each point is sorted\n        return self.sort_points(smaller) + sorted(equal, key=self.calculate_squared_distance) + self.sort_points(larger)\n        # Return sorted array where equal angle points are sorted by distance\n\n    def get_start_point(self, edges) -&gt; None:\n        \"\"\"Determine the index of the bottom most point of the hull when sorted by x-position.\n\n        Parameters\n        ----------\n        edges: np.array\n\n        \"\"\"\n        min_y_index = np.argmin(edges[:, 1])\n        self.start_point = edges[min_y_index]\n\n    def graham_scan(self, edges: list):\n        \"\"\"Construct the convex hull using the  Graham Scan algorithm.\n\n        Ideally this algorithm will take O( n * log(n) ) time.\n\n        Parameters\n        ----------\n        edges : list\n            A python list of coordinates that make up the edges of the grain.\n\n        Returns\n        -------\n        hull : list\n            A list containing coordinates of the points in the hull.\n        hull_indices : list\n            A list containing the hull points indices inside the edges list. In other words, this provides a way to find\n            the points from the hull inside the edges list that was passed.\n        simplices : list\n            A  list of tuples, each tuple representing a simplex of the convex hull. These simplices are sorted such\n            that they follow each other in counterclockwise order.\n        \"\"\"\n        # FIXME : Make this an isolated method\n        # Find a point guaranteed to be on the hull. I find the bottom most point(s) and sort by x-position.\n        min_y_index = None\n        for index, point in enumerate(edges):\n            if min_y_index is None or point[1] &lt; edges[min_y_index][1]:\n                min_y_index = index\n            if point[1] == edges[min_y_index][1] and point[0] &lt; edges[min_y_index][0]:\n                min_y_index = index\n        self.start_point = edges[min_y_index]\n        # This does the same thing, but as a separate method and with Numpy Array rather than a list\n        # self.get_start_point(edges)\n        # Sort the points\n        points_sorted_by_angle = self.sort_points(edges)\n\n        # Remove starting point from the list so it's not added more than once to the hull\n        start_point_index = points_sorted_by_angle.index(self.start_point)\n        del points_sorted_by_angle[start_point_index]\n        # Add start point and the first point sorted by angle. Both of these points will always be on the hull.\n        hull = [self.start_point, points_sorted_by_angle[0]]\n\n        # Iterate through each point, checking if this point would cause a clockwise rotation if added to the hull, and\n        # if so, backtracking.\n        for _, point in enumerate(points_sorted_by_angle[1:]):\n            # Determine if the proposed point demands a clockwise rotation\n            while self.is_clockwise(hull[-2], hull[-1], point) is True:\n                # Delete the failed point\n                del hull[-1]\n                if len(hull) &lt; 2:\n                    break\n            # The point does not immediately cause a clockwise rotation.\n            hull.append(point)\n\n        # Get hull indices from original points array\n        hull_indices = []\n        for point in hull:\n            hull_indices.append(edges.index(point))\n\n        # Create simplices from the hull points\n        simplices = []\n        for index, value in enumerate(hull_indices):\n            simplices.append((hull_indices[index - 1], value))\n\n        return hull, hull_indices, simplices\n\n    @staticmethod\n    def plot(edges: list, convex_hull: list = None, file_path: Path = None) -&gt; None:\n        \"\"\"Plot and save the coordinates of the edges in the grain and optionally the hull.\n\n        Parameters\n        ----------\n        coordinates : list\n            A list of points to be plotted.\n        convex_hull : list\n            Optional argument. A list of points that form the convex hull. Will be plotted with the coordinates if\n            provided.\n        file_path : Path\n            Path of the file to save the plot as.\n        \"\"\"\n        _, ax = plt.subplots(1, 1, figsize=(8, 8))\n        x_s, y_s = zip(*edges)\n        ax.scatter(x_s, y_s)\n        if convex_hull is not None:\n            for index in range(1, len(convex_hull) + 1):\n                # Loop on the final simplex of the hull to join the last and first points together.\n                if len(convex_hull) == index:\n                    index = 0\n                point2 = convex_hull[index]\n                point1 = convex_hull[index - 1]\n                # Plot a line between the two points\n                plt.plot((point1[0], point2[0]), (point1[1], point2[1]), \"#994400\")\n        plt.savefig(file_path)\n        plt.close()\n\n    def calculate_aspect_ratio(self, edges: list, hull_simplices: np.ndarray, path: Path, debug: bool = False) -&gt; tuple:\n        \"\"\"Calculate the width, length and aspect ratio of the smallest bounding rectangle of a grain.\n\n        Parameters\n        ----------\n        edges : list\n            A python list of coordinates of the edge of the grain.\n        hull_simplices : np.ndarray\n            A 2D numpy array of simplices that the hull is comprised of.\n        path : Path\n            Path to the save folder for the grain.\n        debug : bool\n            If true, various plots will be saved for diagnostic purposes.\n\n        Returns\n        -------\n        smallest_bounding_width : float\n            The width in pixels (not nanometres), of the smallest bounding rectangle for the grain.\n        smallest_bounding_length : float\n            The length in pixels (not nanometres), of the smallest bounding rectangle for the grain.\n        aspect_ratio : float\n            The width divided by the length of the smallest bounding rectangle for the grain. It will always be greater\n            or equal to 1.\n        \"\"\"\n        # Ensure the edges are in the form of a numpy array.\n        edges = np.array(edges)\n\n        # Create a variable to store the smallest area in - this is to be able to compare whilst iterating\n        smallest_bounding_area = None\n        # FIXME : pylint complains that this is unused which looks like a false positive to me as it is used.\n        #         Probably does not need initiating here though (and code runs fine when doing so)\n        # smallest_bounding_rectangle = None\n\n        # Iterate through the simplices\n        for simplex_index, simplex in enumerate(hull_simplices):\n            p_1 = edges[simplex[0]]\n            p_2 = edges[simplex[1]]\n            delta = p_1 - p_2\n            angle = np.arctan2(delta[0], delta[1])\n\n            # Find the centroid of the points\n            centroid = (sum(edges[:, 0]) / len(edges), sum(edges[:, 1] / len(edges)))\n\n            # Map the coordinates such that the centroid is now centered on the origin. This is needed for the\n            # matrix rotation step coming up.\n            remapped_points = edges - centroid\n\n            # Rotate the coordinates using a rotation matrix\n            rotated_coordinates = np.array(((np.cos(angle), -np.sin(angle)), (np.sin(angle), np.cos(angle))))\n\n            # For each point in the set, rotate it using the above rotation matrix.\n            rotated_points = []\n            for _, point in enumerate(remapped_points):\n                newpoint = rotated_coordinates @ point\n                # FIXME : Can probably use np.append() here to append arrays directly, something like\n                # np.append(rotated_points, newpoint, axis=0) but doing so requires other areas to be modified\n                rotated_points.append(newpoint)\n            rotated_points = np.array(rotated_points)\n            # Find the cartesian extremities\n            extremes = self.find_cartesian_extremes(rotated_points)\n\n            if debug:\n                # Ensure directory is there\n                path.mkdir(parents=True, exist_ok=True)\n\n                # Create plot\n                # FIXME : Make this a method\n                fig = plt.figure(figsize=(8, 8))\n                ax = fig.add_subplot(111)\n\n                # Draw the points and the current simplex that is being tested\n                plt.scatter(x=remapped_points[:, 0], y=remapped_points[:, 1])\n                plt.plot(\n                    remapped_points[simplex, 0],\n                    remapped_points[simplex, 1],\n                    \"#444444\",\n                    linewidth=4,\n                )\n                plt.scatter(x=rotated_points[:, 0], y=rotated_points[:, 1])\n                plt.plot(\n                    rotated_points[simplex, 0],\n                    rotated_points[simplex, 1],\n                    \"k-\",\n                    linewidth=5,\n                )\n                LOGGER.info(rotated_points[simplex, 0], rotated_points[simplex, 1])\n\n                # Draw the convex hulls\n                for _simplex in hull_simplices:\n                    plt.plot(\n                        remapped_points[_simplex, 0],\n                        remapped_points[_simplex, 1],\n                        \"#888888\",\n                    )\n                    plt.plot(\n                        rotated_points[_simplex, 0],\n                        rotated_points[_simplex, 1],\n                        \"#555555\",\n                    )\n\n                # Draw bounding box\n                plt.plot(\n                    [\n                        extremes[\"x_min\"],\n                        extremes[\"x_min\"],\n                        extremes[\"x_max\"],\n                        extremes[\"x_max\"],\n                        extremes[\"x_min\"],\n                    ],\n                    [\n                        extremes[\"y_min\"],\n                        extremes[\"y_max\"],\n                        extremes[\"y_max\"],\n                        extremes[\"y_min\"],\n                        extremes[\"y_min\"],\n                    ],\n                    \"#994400\",\n                )\n                plt.savefig(path / (\"bounding_rectangle_construction_simplex_\" + str(simplex_index) + \".png\"))\n\n            # Calculate the area of the proposed bounding rectangle\n            bounding_area = (extremes[\"x_max\"] - extremes[\"x_min\"]) * (extremes[\"y_max\"] - extremes[\"y_min\"])\n\n            # If current bounding rectangle is the smallest so far\n            if smallest_bounding_area is None or bounding_area &lt; smallest_bounding_area:\n                smallest_bounding_area = bounding_area\n                # smallest_bounding_rectangle = (\n                #     extremes[\"x_min\"],\n                #     extremes[\"x_max\"],\n                #     extremes[\"y_min\"],\n                #     extremes[\"y_max\"],\n                # )\n                aspect_ratio = (extremes[\"x_max\"] - extremes[\"x_min\"]) / (extremes[\"y_max\"] - extremes[\"y_min\"])\n                smallest_bounding_width = min(\n                    (extremes[\"x_max\"] - extremes[\"x_min\"]),\n                    (extremes[\"y_max\"] - extremes[\"y_min\"]),\n                )\n                smallest_bounding_length = max(\n                    (extremes[\"x_max\"] - extremes[\"x_min\"]),\n                    (extremes[\"y_max\"] - extremes[\"y_min\"]),\n                )\n                # Enforce &gt;= 1 aspect ratio\n                if aspect_ratio &lt; 1.0:\n                    aspect_ratio = 1 / aspect_ratio\n\n        # Unrotate the bounding box vertices\n        r_inverse = rotated_coordinates.T\n        translated_rotated_bounding_rectangle_vertices = np.array(\n            (\n                [extremes[\"x_min\"], extremes[\"y_min\"]],\n                [extremes[\"x_max\"], extremes[\"y_min\"]],\n                [extremes[\"x_max\"], extremes[\"y_max\"]],\n                [extremes[\"x_min\"], extremes[\"y_max\"]],\n            )\n        )\n        translated_bounding_rectangle_vertices = []\n        for _, point in enumerate(translated_rotated_bounding_rectangle_vertices):\n            newpoint = r_inverse @ point\n            # FIXME : As above can likely use np.append(, axis=0) here\n            translated_bounding_rectangle_vertices.append(newpoint)\n        translated_bounding_rectangle_vertices = np.array(translated_bounding_rectangle_vertices)\n\n        if debug:\n            # Create plot\n            # FIXME : Make this a private method\n            fig = plt.figure(figsize=(8, 8))\n            ax = fig.add_subplot(111)\n            plt.scatter(x=edges[:, 0], y=edges[:, 1])\n            ax.plot(\n                np.append(\n                    translated_rotated_bounding_rectangle_vertices[:, 0],\n                    translated_rotated_bounding_rectangle_vertices[0, 0],\n                ),\n                np.append(\n                    translated_rotated_bounding_rectangle_vertices[:, 1],\n                    translated_rotated_bounding_rectangle_vertices[0, 1],\n                ),\n                \"#994400\",\n                label=\"rotated\",\n            )\n            ax.plot(\n                np.append(\n                    translated_bounding_rectangle_vertices[:, 0],\n                    translated_bounding_rectangle_vertices[0, 0],\n                ),\n                np.append(\n                    translated_bounding_rectangle_vertices[:, 1],\n                    translated_bounding_rectangle_vertices[0, 1],\n                ),\n                \"#004499\",\n                label=\"unrotated\",\n            )\n            ax.scatter(\n                x=remapped_points[:, 0],\n                y=remapped_points[:, 1],\n                color=\"#004499\",\n                label=\"translated\",\n            )\n            ax.scatter(x=rotated_points[:, 0], y=rotated_points[:, 1], label=\"rotated\")\n            ax.legend()\n            plt.savefig(path / \"hull_bounding_rectangle_extra\")\n\n        fig = plt.figure(figsize=(8, 8))\n        ax = fig.add_subplot(111)\n        bounding_rectangle_vertices = translated_bounding_rectangle_vertices + centroid\n        ax.plot(\n            np.append(bounding_rectangle_vertices[:, 0], bounding_rectangle_vertices[0, 0]),\n            np.append(bounding_rectangle_vertices[:, 1], bounding_rectangle_vertices[0, 1]),\n            \"#994400\",\n            label=\"unrotated\",\n        )\n        ax.scatter(x=edges[:, 0], y=edges[:, 1], label=\"original points\")\n        ax.set_aspect(1)\n        ax.legend()\n        plt.xlabel(\"Grain Length (nm)\")\n        plt.ylabel(\"Grain Width (nm)\")\n        # plt.savefig(path / \"minimum_bbox.png\")\n        plt.close()\n\n        return smallest_bounding_width, smallest_bounding_length, aspect_ratio\n\n    @staticmethod\n    def find_cartesian_extremes(rotated_points: np.ndarray) -&gt; dict:\n        \"\"\"Find the limits of x and y of rotated points.\n\n        Parameters\n        ----------\n        rotated_points: np.ndarray\n            2-D array of rotated points.\n\n        Returns\n        -------\n        Dict\n            Dictionary of the x and y min and max.__annotations__\n        \"\"\"\n        extremes = {}\n        extremes[\"x_min\"] = np.min(rotated_points[:, 0])\n        extremes[\"x_max\"] = np.max(rotated_points[:, 0])\n        extremes[\"y_min\"] = np.min(rotated_points[:, 1])\n        extremes[\"y_max\"] = np.max(rotated_points[:, 1])\n        return extremes\n\n    @staticmethod\n    def get_shift(coords: np.ndarray, shape: np.ndarray) -&gt; int:\n        \"\"\"Obtain the coordinate shift to reflect the cropped image box for molecules near the edges of the image.\n\n        Parameters\n        ----------\n        coords: np.ndarray\n            Value representing integer coordinates which may be outside of the image.\n        shape: np.ndarray\n            Array of the shape of an image.\n\n        Returns\n        -------\n        np.int64\n            Max value of the shift to reflect the croped region so it stays within the image.\n        \"\"\"\n        shift = shape - coords[np.where(coords &gt; shape)]\n        shift = np.hstack((shift, -coords[np.where(coords &lt; 0)]))\n        if len(shift) == 0:\n            return 0\n        max_index = np.argmax(abs(shift))\n        return shift[max_index]\n\n    def get_cropped_region(self, image: np.ndarray, length: int, centre: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Crops the image with respect to a given pixel length around the centre coordinates.\n\n        Parameters\n        ----------\n        image: np.ndarray\n            The image array.\n        length: int\n            The length (in pixels) of the resultant cropped image.\n        centre: np.ndarray\n            The centre of the object to crop.\n\n        Returns\n        -------\n        np.ndarray\n            Cropped array of the image.\n        \"\"\"\n        shape = image.shape\n        xy1 = shape - (centre + length + 1)\n        xy2 = shape - (centre - length)\n        xy = np.stack((xy1, xy2))\n        shiftx = self.get_shift(xy[:, 0], shape[0])\n        shifty = self.get_shift(xy[:, 1], shape[1])\n        return image.copy()[\n            centre[0] - length - shiftx : centre[0] + length + 1 - shiftx,  # noqa: E203\n            centre[1] - length - shifty : centre[1] + length + 1 - shifty,  # noqa: E203\n        ]\n\n    @staticmethod\n    def get_triangle_height(base_point_1: np.array, base_point_2: np.array, top_point: np.array) -&gt; float:\n        \"\"\"Return the height of a triangle defined by the input point vectors.\n\n        Parameters\n        ----------\n        base_point_1: np.ndarray\n            a base point of the triangle, eg: [5, 3].\n\n        base_point_2: np.ndarray\n            a base point of the triangle, eg: [8, 3].\n\n        top_point: np.ndarray\n            the top point of the triangle, defining the height from the line between the two base points, eg: [6,10].\n\n        Returns\n        -------\n        Float\n            The height of the triangle - ie the shortest distance between the top point and the line between the two\n        base points.\n        \"\"\"\n        # Height of triangle = A/b = ||AB X AC|| / ||AB||\n        a_b = base_point_1 - base_point_2\n        a_c = base_point_1 - top_point\n        return np.linalg.norm(np.cross(a_b, a_c)) / np.linalg.norm(a_b)\n\n    @staticmethod\n    def get_max_min_ferets(edge_points: list):  # noqa: C901\n        \"\"\"Return the minimum and maximum feret diameters for a grain.\n\n        These are defined as the smallest and greatest distances between a pair of callipers that are rotating around a\n        2d object, maintaining contact at all times.\n\n        Parameters\n        ----------\n        edge_points: list\n            a list of the vector positions of the pixels comprising the edge of the\n            grain. Eg: [[0, 0], [1, 0], [2, 1]]\n\n        Returns\n        -------\n        min_feret: float\n            the minimum feret diameter of the grain\n        max_feret: float\n            the maximum feret diameter of the grain\n\n        Notes\n        -----\n        The method starts out by calculating the upper and lower convex hulls using  an algorithm based on the Graham\n        Scan Algorithm [1]. Using these upper and lower hulls, the callipers are simulated as rotating clockwise around\n        the grain. We determine the order in which vertices are encountered by comparing the gradients of the slopes\n        between vertices. An array of pairs of points that are in contact with either calliper at a given time is\n        created in order to be able to calculate the maximum feret diameter. The minimum diameter is a little tricky,\n        since it won't simply be the shortest distance between two contact points, but it will occur somewhere during\n        the rotation around a pair of contact points. It turns out that the point will always be such that two points\n        are in contact with one calliper while the other calliper is in contact with another point. We can use this fact\n        to be sure of finding the smallest feret diameter, simply by testing each triangle of 3 contact points as we\n        iterate, finding the height of the triangle that is formed between the three aforementioned points, as this will\n        be the perpendicular distance between the callipers.\n\n        References\n        ----------\n        [1] Graham, R.L. (1972).\n            \"An Efficient Algorithm for Determining the Convex Hull of a Finite Planar Set\".\n            Information Processing Letters. 1 (4): 132-133.\n            doi:10.1016/0020-0190(72)90045-2.\n        \"\"\"\n        # Sort the vectors by their x coordinate and then by their y coordinate.\n        # The conversion between list and numpy array can be removed, though it would be harder\n        # to read.\n        edge_points.sort()\n        edge_points = np.array(edge_points)\n\n        # Construct upper and lower hulls for the edge points. Sadly we can't just use the standard hull\n        # that graham_scan() returns, since we need to separate the upper and lower hulls. I might streamline\n        # these two into one method later.\n        upper_hull = []\n        lower_hull = []\n        for point in edge_points:\n            while len(lower_hull) &gt; 1 and GrainStats.is_clockwise(lower_hull[-2], lower_hull[-1], point):\n                lower_hull.pop()\n            lower_hull.append(point)\n            while len(upper_hull) &gt; 1 and not GrainStats.is_clockwise(upper_hull[-2], upper_hull[-1], point):\n                upper_hull.pop()\n            upper_hull.append(point)\n\n        upper_hull = np.array(upper_hull)\n        lower_hull = np.array(lower_hull)\n\n        # Create list of contact vertices for calipers on the antipodal hulls\n        contact_points = []\n        upper_index = 0\n        lower_index = len(lower_hull) - 1\n        min_feret = None\n        while upper_index &lt; len(upper_hull) - 1 or lower_index &gt; 0:\n            contact_points.append([lower_hull[lower_index, :], upper_hull[upper_index, :]])\n            # If we have reached the end of the upper hull, continue iterating over the lower hull\n            if upper_index == len(upper_hull) - 1:\n                lower_index -= 1\n                small_feret = GrainStats.get_triangle_height(\n                    np.array(lower_hull[lower_index + 1, :]),\n                    np.array(lower_hull[lower_index, :]),\n                    np.array(upper_hull[upper_index, :]),\n                )\n                if min_feret is None or small_feret &lt; min_feret:\n                    min_feret = small_feret\n            # If we have reached the end of the lower hull, continue iterating over the upper hull\n            elif lower_index == 0:\n                upper_index += 1\n                small_feret = GrainStats.get_triangle_height(\n                    np.array(upper_hull[upper_index - 1, :]),\n                    np.array(upper_hull[upper_index, :]),\n                    np.array(lower_hull[lower_index, :]),\n                )\n                if min_feret is None or small_feret &lt; min_feret:\n                    min_feret = small_feret\n            # Check if the gradient of the last point and the proposed next point in the upper hull is greater than the gradient\n            # of the two corresponding points in the lower hull, if so, this means that the next point in the upper hull\n            # will be encountered before the next point in the lower hull and vice versa.\n            # Note that the calculation here for gradients is the simple delta upper_y / delta upper_x &gt; delta lower_y / delta lower_x\n            # however I have multiplied through the denominators such that there are no instances of division by zero. The\n            # inequality still holds and provides what is needed.\n            elif (upper_hull[upper_index + 1, 1] - upper_hull[upper_index, 1]) * (\n                lower_hull[lower_index, 0] - lower_hull[lower_index - 1, 0]\n            ) &gt; (lower_hull[lower_index, 1] - lower_hull[lower_index - 1, 1]) * (\n                upper_hull[upper_index + 1, 0] - upper_hull[upper_index, 0]\n            ):\n                # If the upper hull is encountered first, increment the iteration index for the upper hull\n                # Also consider the triangle that is made as the two upper hull vertices are colinear with the caliper\n                upper_index += 1\n                small_feret = GrainStats.get_triangle_height(\n                    np.array(upper_hull[upper_index - 1, :]),\n                    np.array(upper_hull[upper_index, :]),\n                    np.array(lower_hull[lower_index, :]),\n                )\n                if min_feret is None or small_feret &lt; min_feret:\n                    min_feret = small_feret\n            else:\n                # The next point in the lower hull will be encountered first, so increment the lower hull iteration index.\n                lower_index -= 1\n                small_feret = GrainStats.get_triangle_height(\n                    np.array(lower_hull[lower_index + 1, :]),\n                    np.array(lower_hull[lower_index, :]),\n                    np.array(upper_hull[upper_index, :]),\n                )\n\n                if min_feret is None or small_feret &lt; min_feret:\n                    min_feret = small_feret\n\n        contact_points = np.array(contact_points)\n\n        # Find the minimum and maximum distance in the contact points\n        max_feret = None\n        for point_pair in contact_points:\n            dist = np.sqrt((point_pair[0, 0] - point_pair[1, 0]) ** 2 + (point_pair[0, 1] - point_pair[1, 1]) ** 2)\n            if max_feret is None or max_feret &lt; dist:\n                max_feret = dist\n\n        return min_feret, max_feret\n</code></pre>"},{"location":"api/grainstats/#topostats.grainstats.GrainStats.__init__","title":"<code>__init__(data: np.ndarray, labelled_data: np.ndarray, pixel_to_nanometre_scaling: float, direction: str, base_output_dir: str | Path, image_name: str = None, edge_detection_method: str = 'binary_erosion', cropped_size: float = -1, plot_opts: dict = None, metre_scaling_factor: float = 1e-09)</code>","text":"<p>Initialise the class.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>2D Numpy array containing the flattened afm image. Data in this 2D array is floating point.</p> required <code>labelled_data</code> <code>ndarray</code> <p>2D Numpy array containing all the grain masks in the image. Data in this 2D array is boolean.</p> required <code>pixel_to_nanometre_scaling</code> <code>float</code> <p>Floating point value that defines the scaling factor between nanometres and pixels.</p> required <code>direction</code> <code>str</code> <p>Direction for which grains have been detected (\"above\" or \"below\").</p> required <code>base_output_dir</code> <code>Path</code> <p>Path to the folder that will store the grain stats output images and data.</p> required <code>image_name</code> <code>str</code> <p>The name of the file being processed.</p> <code>None</code> <code>edge_detection_method</code> <code>str</code> <p>Method used for detecting the edges of grain masks before calculating statistics on them. Do not change unless you know exactly what this is doing. Options: \"binary_erosion\", \"canny\".</p> <code>'binary_erosion'</code> <code>cropped_size</code> <code>float</code> <p>Length of square side (in nm) to crop grains to.</p> <code>-1</code> <code>plot_opts</code> <code>dict</code> <p>Plotting options dictionary for the cropped grains</p> <code>None</code> <code>metre_scaling_factor</code> <code>float</code> <p>Multiplier to convert the current length scale to metres. Default: 1e-9 for the usual AFM length scale of nanometres.</p> <code>1e-09</code> Source code in <code>topostats\\grainstats.py</code> <pre><code>def __init__(\n    self,\n    data: np.ndarray,\n    labelled_data: np.ndarray,\n    pixel_to_nanometre_scaling: float,\n    direction: str,\n    base_output_dir: str | Path,\n    image_name: str = None,\n    edge_detection_method: str = \"binary_erosion\",\n    cropped_size: float = -1,\n    plot_opts: dict = None,\n    metre_scaling_factor: float = 1e-9,\n):\n    \"\"\"Initialise the class.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2D Numpy array containing the flattened afm image. Data in this 2D array is floating point.\n    labelled_data : np.ndarray\n        2D Numpy array containing all the grain masks in the image. Data in this 2D array is boolean.\n    pixel_to_nanometre_scaling : float\n        Floating point value that defines the scaling factor between nanometres and pixels.\n    direction: str\n        Direction for which grains have been detected (\"above\" or \"below\").\n    base_output_dir : Path\n        Path to the folder that will store the grain stats output images and data.\n    image_name : str\n        The name of the file being processed.\n    edge_detection_method : str\n        Method used for detecting the edges of grain masks before calculating statistics on them.\n        Do not change unless you know exactly what this is doing. Options: \"binary_erosion\", \"canny\".\n    cropped_size : float\n        Length of square side (in nm) to crop grains to.\n    plot_opts : dict\n        Plotting options dictionary for the cropped grains\n    metre_scaling_factor : float\n        Multiplier to convert the current length scale to metres. Default: 1e-9 for the\n        usual AFM length scale of nanometres.\n    \"\"\"\n    self.data = data\n    self.labelled_data = labelled_data\n    self.pixel_to_nanometre_scaling = pixel_to_nanometre_scaling\n    self.direction = direction\n    self.base_output_dir = Path(base_output_dir)\n    self.start_point = None\n    self.image_name = image_name\n    self.edge_detection_method = edge_detection_method\n    self.cropped_size = cropped_size\n    self.plot_opts = plot_opts\n    self.metre_scaling_factor = metre_scaling_factor\n</code></pre>"},{"location":"api/grainstats/#topostats.grainstats.GrainStats.calculate_aspect_ratio","title":"<code>calculate_aspect_ratio(edges: list, hull_simplices: np.ndarray, path: Path, debug: bool = False) -&gt; tuple</code>","text":"<p>Calculate the width, length and aspect ratio of the smallest bounding rectangle of a grain.</p> <p>Parameters:</p> Name Type Description Default <code>edges</code> <code>list</code> <p>A python list of coordinates of the edge of the grain.</p> required <code>hull_simplices</code> <code>ndarray</code> <p>A 2D numpy array of simplices that the hull is comprised of.</p> required <code>path</code> <code>Path</code> <p>Path to the save folder for the grain.</p> required <code>debug</code> <code>bool</code> <p>If true, various plots will be saved for diagnostic purposes.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>smallest_bounding_width</code> <code>float</code> <p>The width in pixels (not nanometres), of the smallest bounding rectangle for the grain.</p> <code>smallest_bounding_length</code> <code>float</code> <p>The length in pixels (not nanometres), of the smallest bounding rectangle for the grain.</p> <code>aspect_ratio</code> <code>float</code> <p>The width divided by the length of the smallest bounding rectangle for the grain. It will always be greater or equal to 1.</p> Source code in <code>topostats\\grainstats.py</code> <pre><code>def calculate_aspect_ratio(self, edges: list, hull_simplices: np.ndarray, path: Path, debug: bool = False) -&gt; tuple:\n    \"\"\"Calculate the width, length and aspect ratio of the smallest bounding rectangle of a grain.\n\n    Parameters\n    ----------\n    edges : list\n        A python list of coordinates of the edge of the grain.\n    hull_simplices : np.ndarray\n        A 2D numpy array of simplices that the hull is comprised of.\n    path : Path\n        Path to the save folder for the grain.\n    debug : bool\n        If true, various plots will be saved for diagnostic purposes.\n\n    Returns\n    -------\n    smallest_bounding_width : float\n        The width in pixels (not nanometres), of the smallest bounding rectangle for the grain.\n    smallest_bounding_length : float\n        The length in pixels (not nanometres), of the smallest bounding rectangle for the grain.\n    aspect_ratio : float\n        The width divided by the length of the smallest bounding rectangle for the grain. It will always be greater\n        or equal to 1.\n    \"\"\"\n    # Ensure the edges are in the form of a numpy array.\n    edges = np.array(edges)\n\n    # Create a variable to store the smallest area in - this is to be able to compare whilst iterating\n    smallest_bounding_area = None\n    # FIXME : pylint complains that this is unused which looks like a false positive to me as it is used.\n    #         Probably does not need initiating here though (and code runs fine when doing so)\n    # smallest_bounding_rectangle = None\n\n    # Iterate through the simplices\n    for simplex_index, simplex in enumerate(hull_simplices):\n        p_1 = edges[simplex[0]]\n        p_2 = edges[simplex[1]]\n        delta = p_1 - p_2\n        angle = np.arctan2(delta[0], delta[1])\n\n        # Find the centroid of the points\n        centroid = (sum(edges[:, 0]) / len(edges), sum(edges[:, 1] / len(edges)))\n\n        # Map the coordinates such that the centroid is now centered on the origin. This is needed for the\n        # matrix rotation step coming up.\n        remapped_points = edges - centroid\n\n        # Rotate the coordinates using a rotation matrix\n        rotated_coordinates = np.array(((np.cos(angle), -np.sin(angle)), (np.sin(angle), np.cos(angle))))\n\n        # For each point in the set, rotate it using the above rotation matrix.\n        rotated_points = []\n        for _, point in enumerate(remapped_points):\n            newpoint = rotated_coordinates @ point\n            # FIXME : Can probably use np.append() here to append arrays directly, something like\n            # np.append(rotated_points, newpoint, axis=0) but doing so requires other areas to be modified\n            rotated_points.append(newpoint)\n        rotated_points = np.array(rotated_points)\n        # Find the cartesian extremities\n        extremes = self.find_cartesian_extremes(rotated_points)\n\n        if debug:\n            # Ensure directory is there\n            path.mkdir(parents=True, exist_ok=True)\n\n            # Create plot\n            # FIXME : Make this a method\n            fig = plt.figure(figsize=(8, 8))\n            ax = fig.add_subplot(111)\n\n            # Draw the points and the current simplex that is being tested\n            plt.scatter(x=remapped_points[:, 0], y=remapped_points[:, 1])\n            plt.plot(\n                remapped_points[simplex, 0],\n                remapped_points[simplex, 1],\n                \"#444444\",\n                linewidth=4,\n            )\n            plt.scatter(x=rotated_points[:, 0], y=rotated_points[:, 1])\n            plt.plot(\n                rotated_points[simplex, 0],\n                rotated_points[simplex, 1],\n                \"k-\",\n                linewidth=5,\n            )\n            LOGGER.info(rotated_points[simplex, 0], rotated_points[simplex, 1])\n\n            # Draw the convex hulls\n            for _simplex in hull_simplices:\n                plt.plot(\n                    remapped_points[_simplex, 0],\n                    remapped_points[_simplex, 1],\n                    \"#888888\",\n                )\n                plt.plot(\n                    rotated_points[_simplex, 0],\n                    rotated_points[_simplex, 1],\n                    \"#555555\",\n                )\n\n            # Draw bounding box\n            plt.plot(\n                [\n                    extremes[\"x_min\"],\n                    extremes[\"x_min\"],\n                    extremes[\"x_max\"],\n                    extremes[\"x_max\"],\n                    extremes[\"x_min\"],\n                ],\n                [\n                    extremes[\"y_min\"],\n                    extremes[\"y_max\"],\n                    extremes[\"y_max\"],\n                    extremes[\"y_min\"],\n                    extremes[\"y_min\"],\n                ],\n                \"#994400\",\n            )\n            plt.savefig(path / (\"bounding_rectangle_construction_simplex_\" + str(simplex_index) + \".png\"))\n\n        # Calculate the area of the proposed bounding rectangle\n        bounding_area = (extremes[\"x_max\"] - extremes[\"x_min\"]) * (extremes[\"y_max\"] - extremes[\"y_min\"])\n\n        # If current bounding rectangle is the smallest so far\n        if smallest_bounding_area is None or bounding_area &lt; smallest_bounding_area:\n            smallest_bounding_area = bounding_area\n            # smallest_bounding_rectangle = (\n            #     extremes[\"x_min\"],\n            #     extremes[\"x_max\"],\n            #     extremes[\"y_min\"],\n            #     extremes[\"y_max\"],\n            # )\n            aspect_ratio = (extremes[\"x_max\"] - extremes[\"x_min\"]) / (extremes[\"y_max\"] - extremes[\"y_min\"])\n            smallest_bounding_width = min(\n                (extremes[\"x_max\"] - extremes[\"x_min\"]),\n                (extremes[\"y_max\"] - extremes[\"y_min\"]),\n            )\n            smallest_bounding_length = max(\n                (extremes[\"x_max\"] - extremes[\"x_min\"]),\n                (extremes[\"y_max\"] - extremes[\"y_min\"]),\n            )\n            # Enforce &gt;= 1 aspect ratio\n            if aspect_ratio &lt; 1.0:\n                aspect_ratio = 1 / aspect_ratio\n\n    # Unrotate the bounding box vertices\n    r_inverse = rotated_coordinates.T\n    translated_rotated_bounding_rectangle_vertices = np.array(\n        (\n            [extremes[\"x_min\"], extremes[\"y_min\"]],\n            [extremes[\"x_max\"], extremes[\"y_min\"]],\n            [extremes[\"x_max\"], extremes[\"y_max\"]],\n            [extremes[\"x_min\"], extremes[\"y_max\"]],\n        )\n    )\n    translated_bounding_rectangle_vertices = []\n    for _, point in enumerate(translated_rotated_bounding_rectangle_vertices):\n        newpoint = r_inverse @ point\n        # FIXME : As above can likely use np.append(, axis=0) here\n        translated_bounding_rectangle_vertices.append(newpoint)\n    translated_bounding_rectangle_vertices = np.array(translated_bounding_rectangle_vertices)\n\n    if debug:\n        # Create plot\n        # FIXME : Make this a private method\n        fig = plt.figure(figsize=(8, 8))\n        ax = fig.add_subplot(111)\n        plt.scatter(x=edges[:, 0], y=edges[:, 1])\n        ax.plot(\n            np.append(\n                translated_rotated_bounding_rectangle_vertices[:, 0],\n                translated_rotated_bounding_rectangle_vertices[0, 0],\n            ),\n            np.append(\n                translated_rotated_bounding_rectangle_vertices[:, 1],\n                translated_rotated_bounding_rectangle_vertices[0, 1],\n            ),\n            \"#994400\",\n            label=\"rotated\",\n        )\n        ax.plot(\n            np.append(\n                translated_bounding_rectangle_vertices[:, 0],\n                translated_bounding_rectangle_vertices[0, 0],\n            ),\n            np.append(\n                translated_bounding_rectangle_vertices[:, 1],\n                translated_bounding_rectangle_vertices[0, 1],\n            ),\n            \"#004499\",\n            label=\"unrotated\",\n        )\n        ax.scatter(\n            x=remapped_points[:, 0],\n            y=remapped_points[:, 1],\n            color=\"#004499\",\n            label=\"translated\",\n        )\n        ax.scatter(x=rotated_points[:, 0], y=rotated_points[:, 1], label=\"rotated\")\n        ax.legend()\n        plt.savefig(path / \"hull_bounding_rectangle_extra\")\n\n    fig = plt.figure(figsize=(8, 8))\n    ax = fig.add_subplot(111)\n    bounding_rectangle_vertices = translated_bounding_rectangle_vertices + centroid\n    ax.plot(\n        np.append(bounding_rectangle_vertices[:, 0], bounding_rectangle_vertices[0, 0]),\n        np.append(bounding_rectangle_vertices[:, 1], bounding_rectangle_vertices[0, 1]),\n        \"#994400\",\n        label=\"unrotated\",\n    )\n    ax.scatter(x=edges[:, 0], y=edges[:, 1], label=\"original points\")\n    ax.set_aspect(1)\n    ax.legend()\n    plt.xlabel(\"Grain Length (nm)\")\n    plt.ylabel(\"Grain Width (nm)\")\n    # plt.savefig(path / \"minimum_bbox.png\")\n    plt.close()\n\n    return smallest_bounding_width, smallest_bounding_length, aspect_ratio\n</code></pre>"},{"location":"api/grainstats/#topostats.grainstats.GrainStats.calculate_edges","title":"<code>calculate_edges(grain_mask: np.ndarray, edge_detection_method: str)</code>  <code>staticmethod</code>","text":"<p>Convert 2D boolean array to list of the coordinates of the edges of the grain.</p> <p>Parameters:</p> Name Type Description Default <code>grain_mask</code> <code>ndarray</code> <p>A 2D numpy array image of a grain. Data in the array must be boolean.</p> required <code>edge_detection_method</code> <code>str</code> <p>Method used for detecting the edges of grain masks before calculating statistics on them. Do not change unless you know exactly what this is doing. Options: \"binary_erosion\", \"canny\".</p> required <p>Returns:</p> Name Type Description <code>edges</code> <code>list</code> <p>List containing the coordinates of the edges of the grain.</p> Source code in <code>topostats\\grainstats.py</code> <pre><code>@staticmethod\ndef calculate_edges(grain_mask: np.ndarray, edge_detection_method: str):\n    \"\"\"Convert 2D boolean array to list of the coordinates of the edges of the grain.\n\n    Parameters\n    ----------\n    grain_mask : np.ndarray\n        A 2D numpy array image of a grain. Data in the array must be boolean.\n    edge_detection_method : str\n        Method used for detecting the edges of grain masks before calculating statistics on them.\n        Do not change unless you know exactly what this is doing. Options: \"binary_erosion\", \"canny\".\n\n    Returns\n    -------\n    edges : list\n        List containing the coordinates of the edges of the grain.\n    \"\"\"\n    # Fill any holes\n    filled_grain_mask = scipy.ndimage.binary_fill_holes(grain_mask)\n\n    if edge_detection_method == \"binary_erosion\":\n        # Add padding (needed for erosion)\n        padded = np.pad(filled_grain_mask, 1)\n        # Erode by 1 pixel\n        eroded = skimage_morphology.binary_erosion(padded)\n        # Remove padding\n        eroded = eroded[1:-1, 1:-1]\n\n        # Edges is equal to the difference between the\n        # original image and the eroded image.\n        edges = filled_grain_mask.astype(int) - eroded.astype(int)\n    else:\n        # Get outer edge using canny filtering\n        edges = skimage_feature.canny(filled_grain_mask, sigma=3)\n\n    nonzero_coordinates = edges.nonzero()\n    # Get vector representation of the points\n    # FIXME : Switched to list comprehension but should be unnecessary to create this as a list as we can use\n    # np.stack() to combine the arrays and use that...\n    # return np.stack(nonzero_coordinates, axis=1)\n    # edges = []\n    # for vector in np.transpose(nonzero_coordinates):\n    #     edges.append(list(vector))\n    # return edges\n    return [list(vector) for vector in np.transpose(nonzero_coordinates)]\n</code></pre>"},{"location":"api/grainstats/#topostats.grainstats.GrainStats.calculate_points","title":"<code>calculate_points(grain_mask: np.ndarray)</code>  <code>staticmethod</code>","text":"<p>Convert a 2D boolean array to a list of coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>grain_mask</code> <code>ndarray</code> <p>A 2D numpy array image of a grain. Data in the array must be boolean.</p> required <p>Returns:</p> Name Type Description <code>points</code> <code>list</code> <code>A python list containing the coordinates of the pixels in the grain.</code> Source code in <code>topostats\\grainstats.py</code> <pre><code>@staticmethod\ndef calculate_points(grain_mask: np.ndarray):\n    \"\"\"Convert a 2D boolean array to a list of coordinates.\n\n    Parameters\n    ----------\n    grain_mask : np.ndarray\n        A 2D numpy array image of a grain. Data in the array must be boolean.\n\n    Returns\n    -------\n    points : list\n    A python list containing the coordinates of the pixels in the grain.\n    \"\"\"\n    nonzero_coordinates = grain_mask.nonzero()\n    points = []\n    for point in np.transpose(nonzero_coordinates):\n        points.append(list(point))\n\n    return points\n</code></pre>"},{"location":"api/grainstats/#topostats.grainstats.GrainStats.calculate_radius_stats","title":"<code>calculate_radius_stats(edges: list, points: list) -&gt; tuple</code>","text":"<p>Calculate the radius of grains.</p> <p>The radius in this context is the distance from the centroid to points on the edge of the grain.</p> <p>Parameters:</p> Name Type Description Default <code>edges</code> <code>list</code> <p>A 2D python list containing the coordinates of the edges of a grain.</p> required <code>points</code> <code>list</code> <p>A 2D python list containing the coordinates of the points in a grain.</p> required <p>Returns:</p> Type Description <code>Tuple[float]</code> <p>A tuple of the minimum, maximum, mean and median radius of the grain</p> Source code in <code>topostats\\grainstats.py</code> <pre><code>def calculate_radius_stats(self, edges: list, points: list) -&gt; tuple:\n    \"\"\"Calculate the radius of grains.\n\n    The radius in this context is the distance from the centroid to points on the edge of the grain.\n\n    Parameters\n    ----------\n    edges: list\n        A 2D python list containing the coordinates of the edges of a grain.\n    points: list\n        A 2D python list containing the coordinates of the points in a grain.\n\n    Returns\n    -------\n    Tuple[float]\n        A tuple of the minimum, maximum, mean and median radius of the grain\n    \"\"\"\n    # Calculate the centroid of the grain\n    centroid = self._calculate_centroid(points)\n    # Calculate the displacement\n    displacements = self._calculate_displacement(edges, centroid)\n    # Calculate the radius of each point\n    radii = self._calculate_radius(displacements)\n    return {\n        \"min\": np.min(radii),\n        \"max\": np.max(radii),\n        \"mean\": np.mean(radii),\n        \"median\": np.median(radii),\n    }\n</code></pre>"},{"location":"api/grainstats/#topostats.grainstats.GrainStats.calculate_squared_distance","title":"<code>calculate_squared_distance(point_2: tuple, point_1: tuple = None) -&gt; float</code>","text":"<p>Calculate the squared distance between two points.</p> <p>Used for distance sorting purposes and therefore does not perform a square root in the interests of efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>point_2</code> <code>tuple</code> <p>The point to find the squared distance to.</p> required <code>point_1</code> <code>tuple</code> <p>Optional - defaults to the starting point defined in the graham_scan() function. The point to find the</p> <code>None</code> <code>squared</code> required <p>Returns:</p> Name Type Description <code>distance_squared</code> <code>float</code> <p>The squared distance between the two points.</p> Source code in <code>topostats\\grainstats.py</code> <pre><code>def calculate_squared_distance(self, point_2: tuple, point_1: tuple = None) -&gt; float:\n    \"\"\"Calculate the squared distance between two points.\n\n    Used for distance sorting purposes and therefore does not perform a square root in the interests of efficiency.\n\n    Parameters\n    ----------\n    point_2 : tuple\n        The point to find the squared distance to.\n    point_1 : tuple\n        Optional - defaults to the starting point defined in the graham_scan() function. The point to find the\n    squared distance from.\n\n    Returns\n    -------\n    distance_squared : float\n        The squared distance between the two points.\n    \"\"\"\n    # Get the distance squared between two points. If the second point is not provided, use the starting point.\n    point_1 = self.start_point if point_1 is None else point_1\n    delta_x = point_2[0] - point_1[0]\n    delta_y = point_2[1] - point_1[1]\n    # Don't need the sqrt since just sorting for dist\n    return float(delta_x**2 + delta_y**2)\n</code></pre>"},{"location":"api/grainstats/#topostats.grainstats.GrainStats.calculate_stats","title":"<code>calculate_stats() -&gt; dict</code>","text":"<p>Calculate the stats of grains in the labelled image.</p> <p>Returns:</p> Name Type Description <code>grainstats</code> <code>DataFrame</code> <p>A DataFrame containing all the grain stats that have been calculated for the labelled image.</p> <code>grains_plot_data</code> <code>dict</code> <p>A list of dictionaries containing grain data to be plotted.</p> Source code in <code>topostats\\grainstats.py</code> <pre><code>def calculate_stats(self) -&gt; dict:\n    \"\"\"Calculate the stats of grains in the labelled image.\n\n    Returns\n    -------\n    grainstats: pd.DataFrame\n        A DataFrame containing all the grain stats that have been calculated for the labelled image.\n    grains_plot_data:\n        A list of dictionaries containing grain data to be plotted.\n    \"\"\"\n    grains_plot_data = []\n    if self.labelled_data is None:\n        LOGGER.info(\n            f\"[{self.image_name}] : No labelled regions for this image, grain statistics can not be calculated.\"\n        )\n        return pd.DataFrame(columns=GRAIN_STATS_COLUMNS), grains_plot_data\n\n    # Calculate region properties\n    region_properties = skimage_measure.regionprops(self.labelled_data)\n\n    # Iterate over all the grains in the image\n    stats_array = []\n    # List to hold all the plot data for all the grains. Each entry is a dictionary of plotting data.\n    # There are multiple entries for each grain.\n    for index, region in enumerate(region_properties):\n        LOGGER.info(f\"[{self.image_name}] : Processing grain: {index}\")\n\n        # Skip grain if too small to calculate stats for\n        LOGGER.debug(f\"[{self.image_name}] : Grain size: {region.image.size}\")\n        if min(region.image.shape) &lt; 5:\n            LOGGER.info(\n                f\"[{self.image_name}] : Skipping grain due to being too small (size: {region.image.shape}) to calculate stats for.\"\n            )\n            continue\n\n        # Create directory for each grain's plots\n        output_grain = self.base_output_dir / self.direction\n        # Obtain cropped grain mask and image\n        minr, minc, maxr, maxc = region.bbox\n        grain_mask = np.array(region.image)\n        grain_image = self.data[minr:maxr, minc:maxc]\n        grain_mask_image = np.ma.masked_array(grain_image, mask=np.invert(grain_mask), fill_value=np.nan).filled()\n\n        if self.cropped_size == -1:\n            for name, image in {\n                \"grain_image\": grain_image,\n                \"grain_mask\": grain_mask,\n                \"grain_mask_image\": grain_mask_image,\n            }.items():\n                grains_plot_data.append(\n                    {\n                        \"data\": image,\n                        \"output_dir\": output_grain,\n                        \"filename\": f\"{self.image_name}_{name}_{index}\",\n                        \"name\": name,\n                    }\n                )\n\n        else:\n            # Get cropped image and mask\n            grain_centre = int((minr + maxr) / 2), int((minc + maxc) / 2)\n            length = int(self.cropped_size / (2 * self.pixel_to_nanometre_scaling))\n            solo_mask = self.labelled_data.copy()\n            solo_mask[solo_mask != index + 1] = 0\n            solo_mask[solo_mask == index + 1] = 1\n            cropped_grain_image = self.get_cropped_region(self.data, length, np.asarray(grain_centre))\n            cropped_grain_mask = self.get_cropped_region(solo_mask, length, np.asarray(grain_centre)).astype(bool)\n            cropped_grain_mask_image = np.ma.masked_array(\n                grain_image, mask=np.invert(grain_mask), fill_value=np.nan\n            ).filled()\n            for name, image in {\n                \"grain_image\": cropped_grain_image,\n                \"grain_mask\": cropped_grain_mask,\n                \"grain_mask_image\": cropped_grain_mask_image,\n            }.items():\n                grains_plot_data.append(\n                    {\n                        \"data\": image,\n                        \"output_dir\": output_grain,\n                        \"filename\": f\"{self.image_name}_{name}_{index}\",\n                        \"name\": name,\n                    }\n                )\n\n        points = self.calculate_points(grain_mask)\n        edges = self.calculate_edges(grain_mask, edge_detection_method=self.edge_detection_method)\n        radius_stats = self.calculate_radius_stats(edges, points)\n        # hull, hull_indices, hull_simplexes = self.convex_hull(edges, output_grain)\n        _, _, hull_simplexes = self.convex_hull(edges, output_grain)\n        centroid = self._calculate_centroid(points)\n        # Centroids for the grains (minc and minr added because centroid returns values local to the cropped grain images)\n        centre_x = centroid[0] + minc\n        centre_y = centroid[1] + minr\n        (\n            smallest_bounding_width,\n            smallest_bounding_length,\n            aspect_ratio,\n        ) = self.calculate_aspect_ratio(\n            edges=edges,\n            hull_simplices=hull_simplexes,\n            path=output_grain,\n        )\n\n        # Calculate minimum and maximum feret diameters\n        min_feret, max_feret = self.get_max_min_ferets(edge_points=edges)\n\n        # Save the stats to dictionary. Note that many of the stats are multiplied by a scaling factor to convert\n        # from pixel units to nanometres.\n        # Removed formatting, better to keep accurate until the end, including in CSV, then shorten display\n        length_scaling_factor = self.pixel_to_nanometre_scaling * self.metre_scaling_factor\n        area_scaling_factor = length_scaling_factor**2\n        stats = {\n            \"centre_x\": centre_x * length_scaling_factor,\n            \"centre_y\": centre_y * length_scaling_factor,\n            \"radius_min\": radius_stats[\"min\"] * length_scaling_factor,\n            \"radius_max\": radius_stats[\"max\"] * length_scaling_factor,\n            \"radius_mean\": radius_stats[\"mean\"] * length_scaling_factor,\n            \"radius_median\": radius_stats[\"median\"] * length_scaling_factor,\n            \"height_min\": np.nanmin(grain_mask_image) * self.metre_scaling_factor,\n            \"height_max\": np.nanmax(grain_mask_image) * self.metre_scaling_factor,\n            \"height_median\": np.nanmedian(grain_mask_image) * self.metre_scaling_factor,\n            \"height_mean\": np.nanmean(grain_mask_image) * self.metre_scaling_factor,\n            # [volume] = [pixel] * [pixel] * [height] = px * px * nm.\n            # To turn into m^3, multiply by pixel_to_nanometre_scaling^2 and metre_scaling_factor^3.\n            \"volume\": np.nansum(grain_mask_image)\n            * self.pixel_to_nanometre_scaling**2\n            * (self.metre_scaling_factor**3),\n            \"area\": region.area * area_scaling_factor,\n            \"area_cartesian_bbox\": region.area_bbox * area_scaling_factor,\n            \"smallest_bounding_width\": smallest_bounding_width * length_scaling_factor,\n            \"smallest_bounding_length\": smallest_bounding_length * length_scaling_factor,\n            \"smallest_bounding_area\": smallest_bounding_length * smallest_bounding_width * area_scaling_factor,\n            \"aspect_ratio\": aspect_ratio,\n            \"threshold\": self.direction,\n            \"max_feret\": max_feret * length_scaling_factor,\n            \"min_feret\": min_feret * length_scaling_factor,\n        }\n        stats_array.append(stats)\n    if len(stats_array) &gt; 0:\n        grainstats_df = pd.DataFrame(data=stats_array)\n    else:\n        grainstats_df = create_empty_dataframe()\n    grainstats_df.index.name = \"molecule_number\"\n    grainstats_df[\"image\"] = self.image_name\n\n    return grainstats_df, grains_plot_data\n</code></pre>"},{"location":"api/grainstats/#topostats.grainstats.GrainStats.convex_hull","title":"<code>convex_hull(edges: list, base_output_dir: Path, debug: bool = False)</code>","text":"<p>Calculate a grain's convex hull.</p> <p>Based off of the Graham Scan algorithm and should ideally scale in time with O(nlog(n)).</p> <p>Parameters:</p> Name Type Description Default <code>edges</code> <code>list</code> <p>A python list containing the coordinates of the edges of the grain.</p> required <code>base_output_dir</code> <code>Union[str, Path]</code> <p>Directory to save output to.</p> required <code>debug</code> <code>bool</code> <p>Default false. If true, debug information will be displayed to the terminal and plots for the convex hulls and edges will be saved.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>hull</code> <code>list</code> <p>Coordinates of the points in the hull.</p> <code>hull_indices</code> <code>list</code> <p>The hull points indices inside the edges list. In other words, this provides a way to find the points from the hull inside the edges list that was passed.</p> <code>simplices</code> <code>list</code> <p>List of tuples, each tuple representing a simplex of the convex hull. These simplices are sorted such that they follow each other in counterclockwise order.</p> Source code in <code>topostats\\grainstats.py</code> <pre><code>def convex_hull(self, edges: list, base_output_dir: Path, debug: bool = False):\n    \"\"\"Calculate a grain's convex hull.\n\n    Based off of the Graham Scan algorithm and should ideally scale in time with O(nlog(n)).\n\n    Parameters\n    ----------\n    edges : list\n        A python list containing the coordinates of the edges of the grain.\n    base_output_dir : Union[str, Path]\n        Directory to save output to.\n    debug : bool\n        Default false. If true, debug information will be displayed to the terminal and plots for the convex hulls and edges will be saved.\n\n    Returns\n    -------\n    hull : list\n        Coordinates of the points in the hull.\n    hull_indices : list\n        The hull points indices inside the edges list. In other words, this provides a way to find the points from the hull inside the edges list that was passed.\n    simplices : list\n        List of tuples, each tuple representing a simplex of the convex hull. These simplices are sorted such that they follow each other in counterclockwise order.\n    \"\"\"\n    hull, hull_indices, simplexes = self.graham_scan(edges)\n\n    # Debug information\n    if debug:\n        base_output_dir.mkdir(parents=True, exist_ok=True)\n        self.plot(edges, hull, base_output_dir / \"_points_hull.png\")\n        LOGGER.info(f\"points: {edges}\")\n        LOGGER.info(f\"hull: {hull}\")\n        LOGGER.info(f\"hull indexes: {hull_indices}\")\n        LOGGER.info(f\"simplexes: {simplexes}\")\n\n    return hull, hull_indices, simplexes\n</code></pre>"},{"location":"api/grainstats/#topostats.grainstats.GrainStats.find_cartesian_extremes","title":"<code>find_cartesian_extremes(rotated_points: np.ndarray) -&gt; dict</code>  <code>staticmethod</code>","text":"<p>Find the limits of x and y of rotated points.</p> <p>Parameters:</p> Name Type Description Default <code>rotated_points</code> <code>ndarray</code> <p>2-D array of rotated points.</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>Dictionary of the x and y min and max.annotations</p> Source code in <code>topostats\\grainstats.py</code> <pre><code>@staticmethod\ndef find_cartesian_extremes(rotated_points: np.ndarray) -&gt; dict:\n    \"\"\"Find the limits of x and y of rotated points.\n\n    Parameters\n    ----------\n    rotated_points: np.ndarray\n        2-D array of rotated points.\n\n    Returns\n    -------\n    Dict\n        Dictionary of the x and y min and max.__annotations__\n    \"\"\"\n    extremes = {}\n    extremes[\"x_min\"] = np.min(rotated_points[:, 0])\n    extremes[\"x_max\"] = np.max(rotated_points[:, 0])\n    extremes[\"y_min\"] = np.min(rotated_points[:, 1])\n    extremes[\"y_max\"] = np.max(rotated_points[:, 1])\n    return extremes\n</code></pre>"},{"location":"api/grainstats/#topostats.grainstats.GrainStats.get_angle","title":"<code>get_angle(point_1: tuple, point_2: tuple) -&gt; float</code>  <code>staticmethod</code>","text":"<p>Calculate the angle in radians between two points.</p> <p>Parameters:</p> Name Type Description Default <code>point1</code> <p>Coordinate vectors for the first point to find the angle between.</p> required <code>point2</code> <p>Coordinate vectors for the second point to find the angle between.</p> required <p>Returns:</p> Name Type Description <code>angle</code> <code>float</code> <p>The angle in radians between the two input vectors.</p> Source code in <code>topostats\\grainstats.py</code> <pre><code>@staticmethod\ndef get_angle(point_1: tuple, point_2: tuple) -&gt; float:\n    \"\"\"Calculate the angle in radians between two points.\n\n    Parameters\n    ----------\n    point1: tuple\n        Coordinate vectors for the first point to find the angle between.\n    point2: tuple\n        Coordinate vectors for the second point to find the angle between.\n\n    Returns\n    -------\n    angle : float\n        The angle in radians between the two input vectors.\n    \"\"\"\n    return np.arctan2(point_1[1] - point_2[1], point_1[0] - point_2[0])\n</code></pre>"},{"location":"api/grainstats/#topostats.grainstats.GrainStats.get_cropped_region","title":"<code>get_cropped_region(image: np.ndarray, length: int, centre: np.ndarray) -&gt; np.ndarray</code>","text":"<p>Crops the image with respect to a given pixel length around the centre coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The image array.</p> required <code>length</code> <code>int</code> <p>The length (in pixels) of the resultant cropped image.</p> required <code>centre</code> <code>ndarray</code> <p>The centre of the object to crop.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Cropped array of the image.</p> Source code in <code>topostats\\grainstats.py</code> <pre><code>def get_cropped_region(self, image: np.ndarray, length: int, centre: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Crops the image with respect to a given pixel length around the centre coordinates.\n\n    Parameters\n    ----------\n    image: np.ndarray\n        The image array.\n    length: int\n        The length (in pixels) of the resultant cropped image.\n    centre: np.ndarray\n        The centre of the object to crop.\n\n    Returns\n    -------\n    np.ndarray\n        Cropped array of the image.\n    \"\"\"\n    shape = image.shape\n    xy1 = shape - (centre + length + 1)\n    xy2 = shape - (centre - length)\n    xy = np.stack((xy1, xy2))\n    shiftx = self.get_shift(xy[:, 0], shape[0])\n    shifty = self.get_shift(xy[:, 1], shape[1])\n    return image.copy()[\n        centre[0] - length - shiftx : centre[0] + length + 1 - shiftx,  # noqa: E203\n        centre[1] - length - shifty : centre[1] + length + 1 - shifty,  # noqa: E203\n    ]\n</code></pre>"},{"location":"api/grainstats/#topostats.grainstats.GrainStats.get_max_min_ferets","title":"<code>get_max_min_ferets(edge_points: list)</code>  <code>staticmethod</code>","text":"<p>Return the minimum and maximum feret diameters for a grain.</p> <p>These are defined as the smallest and greatest distances between a pair of callipers that are rotating around a 2d object, maintaining contact at all times.</p> <p>Parameters:</p> Name Type Description Default <code>edge_points</code> <code>list</code> <p>a list of the vector positions of the pixels comprising the edge of the grain. Eg: [[0, 0], [1, 0], [2, 1]]</p> required <p>Returns:</p> Name Type Description <code>min_feret</code> <code>float</code> <p>the minimum feret diameter of the grain</p> <code>max_feret</code> <code>float</code> <p>the maximum feret diameter of the grain</p> Notes <p>The method starts out by calculating the upper and lower convex hulls using  an algorithm based on the Graham Scan Algorithm [1]. Using these upper and lower hulls, the callipers are simulated as rotating clockwise around the grain. We determine the order in which vertices are encountered by comparing the gradients of the slopes between vertices. An array of pairs of points that are in contact with either calliper at a given time is created in order to be able to calculate the maximum feret diameter. The minimum diameter is a little tricky, since it won't simply be the shortest distance between two contact points, but it will occur somewhere during the rotation around a pair of contact points. It turns out that the point will always be such that two points are in contact with one calliper while the other calliper is in contact with another point. We can use this fact to be sure of finding the smallest feret diameter, simply by testing each triangle of 3 contact points as we iterate, finding the height of the triangle that is formed between the three aforementioned points, as this will be the perpendicular distance between the callipers.</p> References <p>[1] Graham, R.L. (1972).     \"An Efficient Algorithm for Determining the Convex Hull of a Finite Planar Set\".     Information Processing Letters. 1 (4): 132-133.     doi:10.1016/0020-0190(72)90045-2.</p> Source code in <code>topostats\\grainstats.py</code> <pre><code>@staticmethod\ndef get_max_min_ferets(edge_points: list):  # noqa: C901\n    \"\"\"Return the minimum and maximum feret diameters for a grain.\n\n    These are defined as the smallest and greatest distances between a pair of callipers that are rotating around a\n    2d object, maintaining contact at all times.\n\n    Parameters\n    ----------\n    edge_points: list\n        a list of the vector positions of the pixels comprising the edge of the\n        grain. Eg: [[0, 0], [1, 0], [2, 1]]\n\n    Returns\n    -------\n    min_feret: float\n        the minimum feret diameter of the grain\n    max_feret: float\n        the maximum feret diameter of the grain\n\n    Notes\n    -----\n    The method starts out by calculating the upper and lower convex hulls using  an algorithm based on the Graham\n    Scan Algorithm [1]. Using these upper and lower hulls, the callipers are simulated as rotating clockwise around\n    the grain. We determine the order in which vertices are encountered by comparing the gradients of the slopes\n    between vertices. An array of pairs of points that are in contact with either calliper at a given time is\n    created in order to be able to calculate the maximum feret diameter. The minimum diameter is a little tricky,\n    since it won't simply be the shortest distance between two contact points, but it will occur somewhere during\n    the rotation around a pair of contact points. It turns out that the point will always be such that two points\n    are in contact with one calliper while the other calliper is in contact with another point. We can use this fact\n    to be sure of finding the smallest feret diameter, simply by testing each triangle of 3 contact points as we\n    iterate, finding the height of the triangle that is formed between the three aforementioned points, as this will\n    be the perpendicular distance between the callipers.\n\n    References\n    ----------\n    [1] Graham, R.L. (1972).\n        \"An Efficient Algorithm for Determining the Convex Hull of a Finite Planar Set\".\n        Information Processing Letters. 1 (4): 132-133.\n        doi:10.1016/0020-0190(72)90045-2.\n    \"\"\"\n    # Sort the vectors by their x coordinate and then by their y coordinate.\n    # The conversion between list and numpy array can be removed, though it would be harder\n    # to read.\n    edge_points.sort()\n    edge_points = np.array(edge_points)\n\n    # Construct upper and lower hulls for the edge points. Sadly we can't just use the standard hull\n    # that graham_scan() returns, since we need to separate the upper and lower hulls. I might streamline\n    # these two into one method later.\n    upper_hull = []\n    lower_hull = []\n    for point in edge_points:\n        while len(lower_hull) &gt; 1 and GrainStats.is_clockwise(lower_hull[-2], lower_hull[-1], point):\n            lower_hull.pop()\n        lower_hull.append(point)\n        while len(upper_hull) &gt; 1 and not GrainStats.is_clockwise(upper_hull[-2], upper_hull[-1], point):\n            upper_hull.pop()\n        upper_hull.append(point)\n\n    upper_hull = np.array(upper_hull)\n    lower_hull = np.array(lower_hull)\n\n    # Create list of contact vertices for calipers on the antipodal hulls\n    contact_points = []\n    upper_index = 0\n    lower_index = len(lower_hull) - 1\n    min_feret = None\n    while upper_index &lt; len(upper_hull) - 1 or lower_index &gt; 0:\n        contact_points.append([lower_hull[lower_index, :], upper_hull[upper_index, :]])\n        # If we have reached the end of the upper hull, continue iterating over the lower hull\n        if upper_index == len(upper_hull) - 1:\n            lower_index -= 1\n            small_feret = GrainStats.get_triangle_height(\n                np.array(lower_hull[lower_index + 1, :]),\n                np.array(lower_hull[lower_index, :]),\n                np.array(upper_hull[upper_index, :]),\n            )\n            if min_feret is None or small_feret &lt; min_feret:\n                min_feret = small_feret\n        # If we have reached the end of the lower hull, continue iterating over the upper hull\n        elif lower_index == 0:\n            upper_index += 1\n            small_feret = GrainStats.get_triangle_height(\n                np.array(upper_hull[upper_index - 1, :]),\n                np.array(upper_hull[upper_index, :]),\n                np.array(lower_hull[lower_index, :]),\n            )\n            if min_feret is None or small_feret &lt; min_feret:\n                min_feret = small_feret\n        # Check if the gradient of the last point and the proposed next point in the upper hull is greater than the gradient\n        # of the two corresponding points in the lower hull, if so, this means that the next point in the upper hull\n        # will be encountered before the next point in the lower hull and vice versa.\n        # Note that the calculation here for gradients is the simple delta upper_y / delta upper_x &gt; delta lower_y / delta lower_x\n        # however I have multiplied through the denominators such that there are no instances of division by zero. The\n        # inequality still holds and provides what is needed.\n        elif (upper_hull[upper_index + 1, 1] - upper_hull[upper_index, 1]) * (\n            lower_hull[lower_index, 0] - lower_hull[lower_index - 1, 0]\n        ) &gt; (lower_hull[lower_index, 1] - lower_hull[lower_index - 1, 1]) * (\n            upper_hull[upper_index + 1, 0] - upper_hull[upper_index, 0]\n        ):\n            # If the upper hull is encountered first, increment the iteration index for the upper hull\n            # Also consider the triangle that is made as the two upper hull vertices are colinear with the caliper\n            upper_index += 1\n            small_feret = GrainStats.get_triangle_height(\n                np.array(upper_hull[upper_index - 1, :]),\n                np.array(upper_hull[upper_index, :]),\n                np.array(lower_hull[lower_index, :]),\n            )\n            if min_feret is None or small_feret &lt; min_feret:\n                min_feret = small_feret\n        else:\n            # The next point in the lower hull will be encountered first, so increment the lower hull iteration index.\n            lower_index -= 1\n            small_feret = GrainStats.get_triangle_height(\n                np.array(lower_hull[lower_index + 1, :]),\n                np.array(lower_hull[lower_index, :]),\n                np.array(upper_hull[upper_index, :]),\n            )\n\n            if min_feret is None or small_feret &lt; min_feret:\n                min_feret = small_feret\n\n    contact_points = np.array(contact_points)\n\n    # Find the minimum and maximum distance in the contact points\n    max_feret = None\n    for point_pair in contact_points:\n        dist = np.sqrt((point_pair[0, 0] - point_pair[1, 0]) ** 2 + (point_pair[0, 1] - point_pair[1, 1]) ** 2)\n        if max_feret is None or max_feret &lt; dist:\n            max_feret = dist\n\n    return min_feret, max_feret\n</code></pre>"},{"location":"api/grainstats/#topostats.grainstats.GrainStats.get_shift","title":"<code>get_shift(coords: np.ndarray, shape: np.ndarray) -&gt; int</code>  <code>staticmethod</code>","text":"<p>Obtain the coordinate shift to reflect the cropped image box for molecules near the edges of the image.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>ndarray</code> <p>Value representing integer coordinates which may be outside of the image.</p> required <code>shape</code> <code>ndarray</code> <p>Array of the shape of an image.</p> required <p>Returns:</p> Type Description <code>int64</code> <p>Max value of the shift to reflect the croped region so it stays within the image.</p> Source code in <code>topostats\\grainstats.py</code> <pre><code>@staticmethod\ndef get_shift(coords: np.ndarray, shape: np.ndarray) -&gt; int:\n    \"\"\"Obtain the coordinate shift to reflect the cropped image box for molecules near the edges of the image.\n\n    Parameters\n    ----------\n    coords: np.ndarray\n        Value representing integer coordinates which may be outside of the image.\n    shape: np.ndarray\n        Array of the shape of an image.\n\n    Returns\n    -------\n    np.int64\n        Max value of the shift to reflect the croped region so it stays within the image.\n    \"\"\"\n    shift = shape - coords[np.where(coords &gt; shape)]\n    shift = np.hstack((shift, -coords[np.where(coords &lt; 0)]))\n    if len(shift) == 0:\n        return 0\n    max_index = np.argmax(abs(shift))\n    return shift[max_index]\n</code></pre>"},{"location":"api/grainstats/#topostats.grainstats.GrainStats.get_start_point","title":"<code>get_start_point(edges) -&gt; None</code>","text":"<p>Determine the index of the bottom most point of the hull when sorted by x-position.</p> <p>Parameters:</p> Name Type Description Default <code>edges</code> required Source code in <code>topostats\\grainstats.py</code> <pre><code>def get_start_point(self, edges) -&gt; None:\n    \"\"\"Determine the index of the bottom most point of the hull when sorted by x-position.\n\n    Parameters\n    ----------\n    edges: np.array\n\n    \"\"\"\n    min_y_index = np.argmin(edges[:, 1])\n    self.start_point = edges[min_y_index]\n</code></pre>"},{"location":"api/grainstats/#topostats.grainstats.GrainStats.get_triangle_height","title":"<code>get_triangle_height(base_point_1: np.array, base_point_2: np.array, top_point: np.array) -&gt; float</code>  <code>staticmethod</code>","text":"<p>Return the height of a triangle defined by the input point vectors.</p> <p>Parameters:</p> Name Type Description Default <code>base_point_1</code> <code>array</code> <p>a base point of the triangle, eg: [5, 3].</p> required <code>base_point_2</code> <code>array</code> <p>a base point of the triangle, eg: [8, 3].</p> required <code>top_point</code> <code>array</code> <p>the top point of the triangle, defining the height from the line between the two base points, eg: [6,10].</p> required <p>Returns:</p> Type Description <code>Float</code> <p>The height of the triangle - ie the shortest distance between the top point and the line between the two</p> <code>base points.</code> Source code in <code>topostats\\grainstats.py</code> <pre><code>@staticmethod\ndef get_triangle_height(base_point_1: np.array, base_point_2: np.array, top_point: np.array) -&gt; float:\n    \"\"\"Return the height of a triangle defined by the input point vectors.\n\n    Parameters\n    ----------\n    base_point_1: np.ndarray\n        a base point of the triangle, eg: [5, 3].\n\n    base_point_2: np.ndarray\n        a base point of the triangle, eg: [8, 3].\n\n    top_point: np.ndarray\n        the top point of the triangle, defining the height from the line between the two base points, eg: [6,10].\n\n    Returns\n    -------\n    Float\n        The height of the triangle - ie the shortest distance between the top point and the line between the two\n    base points.\n    \"\"\"\n    # Height of triangle = A/b = ||AB X AC|| / ||AB||\n    a_b = base_point_1 - base_point_2\n    a_c = base_point_1 - top_point\n    return np.linalg.norm(np.cross(a_b, a_c)) / np.linalg.norm(a_b)\n</code></pre>"},{"location":"api/grainstats/#topostats.grainstats.GrainStats.graham_scan","title":"<code>graham_scan(edges: list)</code>","text":"<p>Construct the convex hull using the  Graham Scan algorithm.</p> <p>Ideally this algorithm will take O( n * log(n) ) time.</p> <p>Parameters:</p> Name Type Description Default <code>edges</code> <code>list</code> <p>A python list of coordinates that make up the edges of the grain.</p> required <p>Returns:</p> Name Type Description <code>hull</code> <code>list</code> <p>A list containing coordinates of the points in the hull.</p> <code>hull_indices</code> <code>list</code> <p>A list containing the hull points indices inside the edges list. In other words, this provides a way to find the points from the hull inside the edges list that was passed.</p> <code>simplices</code> <code>list</code> <p>A  list of tuples, each tuple representing a simplex of the convex hull. These simplices are sorted such that they follow each other in counterclockwise order.</p> Source code in <code>topostats\\grainstats.py</code> <pre><code>def graham_scan(self, edges: list):\n    \"\"\"Construct the convex hull using the  Graham Scan algorithm.\n\n    Ideally this algorithm will take O( n * log(n) ) time.\n\n    Parameters\n    ----------\n    edges : list\n        A python list of coordinates that make up the edges of the grain.\n\n    Returns\n    -------\n    hull : list\n        A list containing coordinates of the points in the hull.\n    hull_indices : list\n        A list containing the hull points indices inside the edges list. In other words, this provides a way to find\n        the points from the hull inside the edges list that was passed.\n    simplices : list\n        A  list of tuples, each tuple representing a simplex of the convex hull. These simplices are sorted such\n        that they follow each other in counterclockwise order.\n    \"\"\"\n    # FIXME : Make this an isolated method\n    # Find a point guaranteed to be on the hull. I find the bottom most point(s) and sort by x-position.\n    min_y_index = None\n    for index, point in enumerate(edges):\n        if min_y_index is None or point[1] &lt; edges[min_y_index][1]:\n            min_y_index = index\n        if point[1] == edges[min_y_index][1] and point[0] &lt; edges[min_y_index][0]:\n            min_y_index = index\n    self.start_point = edges[min_y_index]\n    # This does the same thing, but as a separate method and with Numpy Array rather than a list\n    # self.get_start_point(edges)\n    # Sort the points\n    points_sorted_by_angle = self.sort_points(edges)\n\n    # Remove starting point from the list so it's not added more than once to the hull\n    start_point_index = points_sorted_by_angle.index(self.start_point)\n    del points_sorted_by_angle[start_point_index]\n    # Add start point and the first point sorted by angle. Both of these points will always be on the hull.\n    hull = [self.start_point, points_sorted_by_angle[0]]\n\n    # Iterate through each point, checking if this point would cause a clockwise rotation if added to the hull, and\n    # if so, backtracking.\n    for _, point in enumerate(points_sorted_by_angle[1:]):\n        # Determine if the proposed point demands a clockwise rotation\n        while self.is_clockwise(hull[-2], hull[-1], point) is True:\n            # Delete the failed point\n            del hull[-1]\n            if len(hull) &lt; 2:\n                break\n        # The point does not immediately cause a clockwise rotation.\n        hull.append(point)\n\n    # Get hull indices from original points array\n    hull_indices = []\n    for point in hull:\n        hull_indices.append(edges.index(point))\n\n    # Create simplices from the hull points\n    simplices = []\n    for index, value in enumerate(hull_indices):\n        simplices.append((hull_indices[index - 1], value))\n\n    return hull, hull_indices, simplices\n</code></pre>"},{"location":"api/grainstats/#topostats.grainstats.GrainStats.is_clockwise","title":"<code>is_clockwise(p_1: tuple, p_2: tuple, p_3: tuple) -&gt; bool</code>  <code>staticmethod</code>","text":"<p>Determine if three points make a clockwise or counter-clockwise turn.</p> <p>Parameters:</p> Name Type Description Default <code>p_1</code> <code>tuple</code> <p>First point to be used to calculate turn.</p> required <code>p_2</code> <code>tuple</code> <p>Second point to be used to calculate turn.</p> required <code>p_3</code> <code>tuple</code> <p>Third point to be used to calculate turn.</p> required <p>Returns:</p> Type Description <code>boolean</code> <p>Indicator of whether turn is clockwise.</p> Source code in <code>topostats\\grainstats.py</code> <pre><code>@staticmethod\ndef is_clockwise(p_1: tuple, p_2: tuple, p_3: tuple) -&gt; bool:\n    \"\"\"Determine if three points make a clockwise or counter-clockwise turn.\n\n    Parameters\n    ----------\n    p_1: tuple\n        First point to be used to calculate turn.\n    p_2: tuple\n        Second point to be used to calculate turn.\n    p_3: tuple\n        Third point to be used to calculate turn.\n\n    Returns\n    -------\n    boolean\n        Indicator of whether turn is clockwise.\n    \"\"\"\n    # Determine if three points form a clockwise or counter-clockwise turn.\n    # I use the method of calculating the determinant of the following rotation matrix here. If the determinant\n    # is &gt; 0 then the rotation is counter-clockwise.\n    rotation_matrix = np.array(((p_1[0], p_1[1], 1), (p_2[0], p_2[1], 1), (p_3[0], p_3[1], 1)))\n    return not np.linalg.det(rotation_matrix) &gt; 0\n</code></pre>"},{"location":"api/grainstats/#topostats.grainstats.GrainStats.plot","title":"<code>plot(edges: list, convex_hull: list = None, file_path: Path = None) -&gt; None</code>  <code>staticmethod</code>","text":"<p>Plot and save the coordinates of the edges in the grain and optionally the hull.</p> <p>Parameters:</p> Name Type Description Default <code>coordinates</code> <code>list</code> <p>A list of points to be plotted.</p> required <code>convex_hull</code> <code>list</code> <p>Optional argument. A list of points that form the convex hull. Will be plotted with the coordinates if provided.</p> <code>None</code> <code>file_path</code> <code>Path</code> <p>Path of the file to save the plot as.</p> <code>None</code> Source code in <code>topostats\\grainstats.py</code> <pre><code>@staticmethod\ndef plot(edges: list, convex_hull: list = None, file_path: Path = None) -&gt; None:\n    \"\"\"Plot and save the coordinates of the edges in the grain and optionally the hull.\n\n    Parameters\n    ----------\n    coordinates : list\n        A list of points to be plotted.\n    convex_hull : list\n        Optional argument. A list of points that form the convex hull. Will be plotted with the coordinates if\n        provided.\n    file_path : Path\n        Path of the file to save the plot as.\n    \"\"\"\n    _, ax = plt.subplots(1, 1, figsize=(8, 8))\n    x_s, y_s = zip(*edges)\n    ax.scatter(x_s, y_s)\n    if convex_hull is not None:\n        for index in range(1, len(convex_hull) + 1):\n            # Loop on the final simplex of the hull to join the last and first points together.\n            if len(convex_hull) == index:\n                index = 0\n            point2 = convex_hull[index]\n            point1 = convex_hull[index - 1]\n            # Plot a line between the two points\n            plt.plot((point1[0], point2[0]), (point1[1], point2[1]), \"#994400\")\n    plt.savefig(file_path)\n    plt.close()\n</code></pre>"},{"location":"api/grainstats/#topostats.grainstats.GrainStats.sort_points","title":"<code>sort_points(points: list) -&gt; list</code>","text":"<p>Sort points in counter-clockwise order of angle made with the starting point.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>list</code> <p>A python list of the coordinates to sort.</p> required <p>Returns:</p> Name Type Description <code>sorted_points</code> <code>list</code> <p>A python list of sorted points.</p> Source code in <code>topostats\\grainstats.py</code> <pre><code>def sort_points(self, points: list) -&gt; list:\n    #    def sort_points(self, points: np.array) -&gt; List:\n    \"\"\"Sort points in counter-clockwise order of angle made with the starting point.\n\n    Parameters\n    ----------\n    points: list\n        A python list of the coordinates to sort.\n\n    Returns\n    -------\n    sorted_points : list\n        A python list of sorted points.\n    \"\"\"\n    # Return if the list is length 1 or 0 (i.e. a single point).\n    if len(points) &lt;= 1:\n        return points\n    # Lists that allow sorting of points relative to a current comparison point\n    smaller, equal, larger = [], [], []\n    # Get a random point in the array to calculate the pivot angle from. This sorts the points relative to this point.\n    pivot_angle = self.get_angle(points[randint(0, len(points) - 1)], self.start_point)  # noqa: S311\n    for point in points:\n        point_angle = self.get_angle(point, self.start_point)\n        # If the\n        if point_angle &lt; pivot_angle:\n            smaller.append(point)\n        elif point_angle == pivot_angle:\n            equal.append(point)\n        else:\n            larger.append(point)\n    # Lets take a different approach and use arrays, we have a start point lets work out the angle of each point\n    # relative to that and _then_ sort it.\n    # pivot_angles = self.get_angle(points, self.start_point)\n    # Recursively sort the arrays until each point is sorted\n    return self.sort_points(smaller) + sorted(equal, key=self.calculate_squared_distance) + self.sort_points(larger)\n</code></pre>"},{"location":"api/io/","title":"IO Modules","text":"<p>Functions for reading and writing data.</p>"},{"location":"api/io/#topostats.io.LoadScans","title":"<code>LoadScans</code>","text":"<p>Load the image and image parameters from a file path.</p> Source code in <code>topostats\\io.py</code> <pre><code>class LoadScans:\n    \"\"\"Load the image and image parameters from a file path.\"\"\"\n\n    def __init__(\n        self,\n        img_paths: list,\n        channel: str,\n    ):\n        \"\"\"Initialise the class.\n\n        Parameters\n        ----------\n        img_path: Union[str, Path]\n            Path to a valid AFM scan to load.\n        channel: str\n            Image channel to extract from the scan.\n        \"\"\"\n        self.img_paths = img_paths\n        self.img_path = None\n        self.channel = channel\n        self.channel_data = None\n        self.filename = None\n        self.image = None\n        self.pixel_to_nm_scaling = None\n        self.grain_masks = {}\n        self.img_dict = {}\n        self.MINIMUM_IMAGE_SIZE = 10\n\n    def load_spm(self) -&gt; tuple:\n        \"\"\"Extract image and pixel to nm scaling from the Bruker .spm file.\n\n        Returns\n        -------\n        tuple(np.ndarray, float)\n            A tuple containing the image and its pixel to nanometre scaling value.\n        \"\"\"\n        LOGGER.info(f\"Loading image from : {self.img_path}\")\n        try:\n            scan = pySPM.Bruker(self.img_path)\n            LOGGER.info(f\"[{self.filename}] : Loaded image from : {self.img_path}\")\n            self.channel_data = scan.get_channel(self.channel)\n            LOGGER.info(f\"[{self.filename}] : Extracted channel {self.channel}\")\n            image = np.flipud(np.array(self.channel_data.pixels))\n        except FileNotFoundError:\n            LOGGER.info(f\"[{self.filename}] File not found : {self.img_path}\")\n            raise\n        except Exception as e:\n            # trying to return the error with options of possible channel values\n            labels = []\n            for channel in [layer[b\"@2:Image Data\"][0] for layer in scan.layers]:\n                channel_description = channel.decode(\"latin1\").split('\"')[1]  # in case blank field raises questions?\n                labels.append(channel_description)\n            LOGGER.error(f\"[{self.filename}] : {self.channel} not in {self.img_path.suffix} channel list: {labels}\")\n            raise e\n\n        return (image, self._spm_pixel_to_nm_scaling(self.channel_data))\n\n    def _spm_pixel_to_nm_scaling(self, channel_data: pySPM.SPM.SPM_image) -&gt; float:\n        \"\"\"Extract pixel to nm scaling from the SPM image metadata.\n\n        Parameters\n        ----------\n        channel_data: pySPM.SPM.SPM_image\n            Channel data from PySPM.\n\n        Returns\n        -------\n        float\n            Pixel to nm scaling factor.\n        \"\"\"\n        unit_dict = {\n            \"nm\": 1,\n            \"um\": 1e3,\n        }\n        px_to_real = channel_data.pxs()\n        # Has potential for non-square pixels but not yet implemented\n        pixel_to_nm_scaling = (\n            px_to_real[0][0] * unit_dict[px_to_real[0][1]],\n            px_to_real[1][0] * unit_dict[px_to_real[1][1]],\n        )[0]\n        if px_to_real[0][0] == 0 and px_to_real[1][0] == 0:\n            pixel_to_nm_scaling = 1\n            LOGGER.warning(f\"[{self.filename}] : Pixel size not found in metadata, defaulting to 1nm\")\n        LOGGER.info(f\"[{self.filename}] : Pixel to nm scaling : {pixel_to_nm_scaling}\")\n        return pixel_to_nm_scaling\n\n    def load_topostats(self) -&gt; tuple:\n        \"\"\"Load a .topostats file (hdf5 format).\n\n        Loads and extracts the image, pixel to nanometre scaling factor and any grain masks.\n\n        Note that grain masks are stored via self.grain_masks rather than returned due to how we extract information for\n        all other file loading functions.\n\n        Returns\n        -------\n        tuple(np.ndarray, float)\n            A tuple containing the image and its pixel to nanometre scaling value.\n        \"\"\"\n        LOGGER.info(f\"Loading image from : {self.img_path}\")\n        try:\n            with h5py.File(self.img_path, \"r\") as f:\n                keys = f.keys()\n                file_version = f[\"topostats_file_version\"][()]\n                LOGGER.info(f\"TopoStats file version: {file_version}\")\n                image = f[\"image\"][:]\n                pixel_to_nm_scaling = f[\"pixel_to_nm_scaling\"][()]\n                if \"grain_masks\" in keys:\n                    grain_masks_keys = f[\"grain_masks\"].keys()\n                    if \"above\" in grain_masks_keys:\n                        LOGGER.info(f\"[{self.filename}] : Found grain mask for above direction\")\n                        self.grain_masks[\"above\"] = f[\"grain_masks\"][\"above\"][:]\n                    if \"below\" in grain_masks_keys:\n                        LOGGER.info(f\"[{self.filename}] : Found grain mask for below direction\")\n                        self.grain_masks[\"below\"] = f[\"grain_masks\"][\"below\"][:]\n        except OSError as e:\n            if \"Unable to open file\" in str(e):\n                LOGGER.info(f\"[{self.filename}] File not found: {self.img_path}\")\n            raise e\n\n        return (image, pixel_to_nm_scaling)\n\n    def load_asd(self) -&gt; tuple:\n        \"\"\"Extract image and pixel to nm scaling from .asd files.\n\n        Returns\n        -------\n        tuple: (np.ndarray, float)\n            A tuple containing the image and its pixel to nanometre scaling value.\n        \"\"\"\n        try:\n            frames: np.ndarray\n            pixel_to_nm_scaling: float\n            _: dict\n            frames, pixel_to_nm_scaling, _ = asd.load_asd(file_path=self.img_path, channel=self.channel)\n            LOGGER.info(f\"[{self.filename}] : Loaded image from : {self.img_path}\")\n        except FileNotFoundError:\n            LOGGER.info(f\"[{self.filename}] : File not found. Path: {self.img_path}\")\n            raise\n\n        return (frames, pixel_to_nm_scaling)\n\n    def load_ibw(self) -&gt; tuple:\n        \"\"\"Load image from Asylum Research (Igor) .ibw files.\n\n        Returns\n        -------\n        tuple(np.ndarray, float)\n            A tuple containing the image and its pixel to nanometre scaling value.\n        \"\"\"\n        LOGGER.info(f\"Loading image from : {self.img_path}\")\n        try:\n            scan = binarywave.load(self.img_path)\n            LOGGER.info(f\"[{self.filename}] : Loaded image from : {self.img_path}\")\n\n            labels = []\n            for label_list in scan[\"wave\"][\"labels\"]:\n                for label in label_list:\n                    if label:\n                        labels.append(label.decode())\n            channel_idx = labels.index(self.channel)\n            image = scan[\"wave\"][\"wData\"][:, :, channel_idx].T * 1e9  # Looks to be in m\n            image = np.flipud(image)\n            LOGGER.info(f\"[{self.filename}] : Extracted channel {self.channel}\")\n        except FileNotFoundError:\n            LOGGER.info(f\"[{self.filename}] File not found : {self.img_path}\")\n        except ValueError:\n            LOGGER.error(f\"[{self.filename}] : {self.channel} not in {self.img_path.suffix} channel list: {labels}\")\n            raise\n        except Exception as exception:\n            LOGGER.error(f\"[{self.filename}] : {exception}\")\n\n        return (image, self._ibw_pixel_to_nm_scaling(scan))\n\n    def _ibw_pixel_to_nm_scaling(self, scan: dict) -&gt; float:\n        \"\"\"Extract pixel to nm scaling from the IBW image metadata.\n\n        Parameters\n        ----------\n        scan: dict\n            The loaded binary wave object.\n\n        Returns\n        -------\n        float\n            A value corresponding to the real length of a single pixel.\n        \"\"\"\n        # Get metadata\n        notes = {}\n        for line in str(scan[\"wave\"][\"note\"]).split(\"\\\\r\"):\n            if line.count(\":\"):\n                key, val = line.split(\":\", 1)\n                notes[key] = val.strip()\n        # Has potential for non-square pixels but not yet implemented\n        pixel_to_nm_scaling = (\n            float(notes[\"SlowScanSize\"]) / scan[\"wave\"][\"wData\"].shape[0] * 1e9,  # as in m\n            float(notes[\"FastScanSize\"]) / scan[\"wave\"][\"wData\"].shape[1] * 1e9,  # as in m\n        )[0]\n        LOGGER.info(f\"[{self.filename}] : Pixel to nm scaling : {pixel_to_nm_scaling}\")\n        return pixel_to_nm_scaling\n\n    def load_jpk(self) -&gt; tuple:\n        \"\"\"Load image from JPK Instruments .jpk files.\n\n        Returns\n        -------\n        tuple(np.ndarray, float)\n            A tuple containing the image and its pixel to nanometre scaling value.\n        \"\"\"\n        # Load the file\n        img_path = str(self.img_path)\n        try:\n            tif = tifffile.TiffFile(img_path)\n        except FileNotFoundError:\n            LOGGER.info(f\"[{self.filename}] File not found : {self.img_path}\")\n            raise\n        # Obtain channel list for all channels in file\n        channel_list = {}\n        for i, page in enumerate(tif.pages[1:]):  # [0] is thumbnail\n            available_channel = page.tags[\"32848\"].value  # keys are hexadecimal values\n            if page.tags[\"32849\"].value == 0:  # whether img is trace or retrace\n                tr_rt = \"trace\"\n            else:\n                tr_rt = \"retrace\"\n            channel_list[f\"{available_channel}_{tr_rt}\"] = i + 1\n        try:\n            channel_idx = channel_list[self.channel]\n        except KeyError:\n            LOGGER.error(f\"{self.channel} not in channel list: {channel_list}\")\n            raise\n        # Get image and if applicable, scale it\n        channel_page = tif.pages[channel_idx]\n        image = channel_page.asarray()\n        scaling_type = channel_page.tags[\"33027\"].value\n        if scaling_type == \"LinearScaling\":\n            scaling = channel_page.tags[\"33028\"].value\n            offset = channel_page.tags[\"33029\"].value\n            image = (image * scaling) + offset\n        elif scaling_type == \"NullScaling\":\n            pass\n        else:\n            raise ValueError(f\"Scaling type {scaling_type} is not 'NullScaling' or 'LinearScaling'\")\n        # Get page for common metadata between scans\n        metadata_page = tif.pages[0]\n        return (image * 1e9, self._jpk_pixel_to_nm_scaling(metadata_page))\n\n    @staticmethod\n    def _jpk_pixel_to_nm_scaling(tiff_page: tifffile.tifffile.TiffPage) -&gt; float:\n        \"\"\"Extract pixel to nm scaling from the JPK image metadata.\n\n        Parameters\n        ----------\n        tiff_page: tifffile.tifffile.TiffPage\n            An image file directory (IFD) of .jpk files.\n\n        Returns\n        -------\n        float\n            A value corresponding to the real length of a single pixel.\n        \"\"\"\n        length = tiff_page.tags[\"32834\"].value  # Grid-uLength (fast)\n        width = tiff_page.tags[\"32835\"].value  # Grid-vLength (slow)\n        length_px = tiff_page.tags[\"32838\"].value  # Grid-iLength (fast)\n        width_px = tiff_page.tags[\"32839\"].value  # Grid-jLength (slow)\n\n        px_to_nm = (length / length_px, width / width_px)[0]\n\n        LOGGER.info(px_to_nm)\n\n        return px_to_nm * 1e9\n\n    @staticmethod\n    def _gwy_read_object(open_file: io.TextIOWrapper, data_dict: dict) -&gt; None:\n        \"\"\"Parse and extract data from a `.gwy` file object, starting at the current open file read position.\n\n        Parameters\n        ----------\n        open_file: io.TextIOWrapper\n            An open file object.\n        data_dict: dict\n            Dictionary of `.gwy` file image properties.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        object_name = read_null_terminated_string(open_file=open_file)\n        data_size = read_u32i(open_file)\n        LOGGER.debug(f\"OBJECT | name: {object_name} | data_size: {data_size}\")\n        # Read components\n        read_data_size = 0\n        while read_data_size &lt; data_size:\n            component_data_size = LoadScans._gwy_read_component(\n                open_file=open_file,\n                initial_byte_pos=open_file.tell(),\n                data_dict=data_dict,\n            )\n            read_data_size += component_data_size\n\n    @staticmethod\n    def _gwy_read_component(open_file: io.TextIOWrapper, initial_byte_pos: int, data_dict: dict) -&gt; int:\n        \"\"\"Parse and extract data from a `.gwy` file object, starting at the current open file read position.\n\n        Parameters\n        ----------\n        open_file: io.TextIOWrapper,\n            An open file object.\n        data_dict: dict\n            Dictionary of `.gwy` file image properties.\n\n        Returns\n        -------\n        int\n            Size of the component in bytes.\n        \"\"\"\n        component_name = read_null_terminated_string(open_file=open_file)\n        data_type = read_gwy_component_dtype(open_file=open_file)\n\n        if data_type == \"o\":\n            LOGGER.debug(f\"component name: {component_name} | dtype: {data_type} |\")\n            sub_dict = {}\n            LoadScans._gwy_read_object(open_file=open_file, data_dict=sub_dict)\n            data_dict[component_name] = sub_dict\n        elif data_type == \"c\":\n            value = read_char(open_file=open_file)\n            LOGGER.debug(f\"component name: {component_name} | dtype: {data_type} | value: {value}\")\n            data_dict[component_name] = value\n        elif data_type == \"i\":\n            value = read_u32i(open_file=open_file)\n            LOGGER.debug(f\"component name: {component_name} | dtype: {data_type} | value: {value}\")\n            data_dict[component_name] = value\n        elif data_type == \"d\":\n            value = read_64d(open_file=open_file)\n            LOGGER.debug(f\"component name: {component_name} | dtype: {data_type} | value: {value}\")\n            data_dict[component_name] = value\n        elif data_type == \"s\":\n            value = read_null_terminated_string(open_file=open_file)\n            LOGGER.debug(f\"component name: {component_name} | dtype: {data_type} | value: {value}\")\n            data_dict[component_name] = value\n        elif data_type == \"D\":\n            array_size = read_u32i(open_file=open_file)\n            LOGGER.debug(f\"component name: {component_name} | dtype: {data_type}\")\n            LOGGER.debug(f\"array size: {array_size}\")\n            data = np.zeros(array_size)\n            for index in range(array_size):\n                data[index] = read_64d(open_file=open_file)\n            if \"xres\" in data_dict and \"yres\" in data_dict:\n                data = data.reshape((data_dict[\"xres\"], data_dict[\"yres\"]))\n            data_dict[\"data\"] = data\n\n        return open_file.tell() - initial_byte_pos\n\n    @staticmethod\n    def _gwy_print_dict(gwy_file_dict: dict, pre_string: str) -&gt; None:\n        \"\"\"Print the nested object / component structure.\n\n        Can be used to find labels and values of objects / components in the `.gwy` file.\n\n        Parameters\n        ----------\n        gwy_file_dict: dict\n            Dictionary of the nested object / component structure of a `.gwy` file.\n        \"\"\"\n        for key, value in gwy_file_dict.items():\n            if isinstance(value, dict):\n                print(pre_string + f\"OBJECT: {key}\")\n                pre_string += \"  \"\n                LoadScans._gwy_print_dict(gwy_file_dict=value, pre_string=pre_string)\n                pre_string = pre_string[:-2]\n            else:\n                print(pre_string + f\"component: {key} | value: {value}\")\n\n    @staticmethod\n    def _gwy_print_dict_wrapper(gwy_file_dict: dict) -&gt; None:\n        \"\"\"Print dictionaries.\n\n        This is a wrapper for the _gwy_print_dict() method.\n\n        Parameters\n        ----------\n        gwy_file_dict: dict\n            Dictionary of the nested object / component structure of a `.gwy` file.\n        \"\"\"\n        pre_string = \"\"\n        LoadScans._gwy_print_dict(gwy_file_dict=gwy_file_dict, pre_string=pre_string)\n\n    def load_gwy(self) -&gt; tuple:\n        \"\"\"Extract image and pixel to nm scaling from the Gwyddion .gwy file.\n\n        Returns\n        -------\n        tuple(np.ndarray, float)\n            A tuple containing the image and its pixel to nanometre scaling value.\n        \"\"\"\n        LOGGER.info(f\"Loading image from : {self.img_path}\")\n        try:\n            image_data_dict = {}\n            with Path.open(self.img_path, \"rb\") as open_file:  # pylint: disable=unspecified-encoding\n                # Read header\n                header = open_file.read(4)\n                LOGGER.debug(f\"Gwy file header: {header}\")\n\n                LoadScans._gwy_read_object(open_file, data_dict=image_data_dict)\n\n            # For development - uncomment to have an indentation based nested\n            # dictionary output showing the object - component structure and\n            # available keys:\n            # LoadScans._gwy_print_dict_wrapper(gwy_file_dict=image_data_dict)\n\n            if \"/0/data\" in image_data_dict:\n                image = image_data_dict[\"/0/data\"][\"data\"]\n                units = image_data_dict[\"/0/data\"][\"si_unit_xy\"][\"unitstr\"]\n                px_to_nm = image_data_dict[\"/0/data\"][\"xreal\"] * 1e9 / image.shape[1]\n            elif \"/1/data\" in image_data_dict:\n                image = image_data_dict[\"/1/data\"][\"data\"]\n                px_to_nm = image_data_dict[\"/1/data\"][\"xreal\"] * 1e9 / image.shape[1]\n                units = image_data_dict[\"/1/data\"][\"si_unit_xy\"][\"unitstr\"]\n            else:\n                raise KeyError(\n                    \"Data location not defined in the .gwy file. Please locate it and add to the load_gwy() function.\"\n                )\n\n            # Convert image heights to nanometresQ\n            if units == \"m\":\n                image = image * 1e9\n            else:\n                raise ValueError(\n                    f\"Units '{units}' have not been added for .gwy files. Please add \\\n                    an SI to nanometre conversion factor for these units in _gwy_read_component in \\\n                    io.py.\"\n                )\n\n        except FileNotFoundError:\n            LOGGER.info(f\"[{self.filename}] File not found : {self.img_path}\")\n            raise\n\n        return (image, px_to_nm)\n\n    def get_data(self) -&gt; None:\n        \"\"\"Extract image, filepath and pixel to nm scaling value, and append these to the img_dic object.\"\"\"\n        suffix_to_loader = {\n            \".spm\": self.load_spm,\n            \".jpk\": self.load_jpk,\n            \".ibw\": self.load_ibw,\n            \".gwy\": self.load_gwy,\n            \".topostats\": self.load_topostats,\n            \".asd\": self.load_asd,\n        }\n\n        for img_path in self.img_paths:\n            self.img_path = img_path\n            self.filename = img_path.stem\n            suffix = img_path.suffix\n            LOGGER.info(f\"Extracting image from {self.img_path}\")\n            LOGGER.debug(f\"File extension : {suffix}\")\n\n            # Check that the file extension is supported\n            if suffix in suffix_to_loader:\n                try:\n                    self.image, self.pixel_to_nm_scaling = suffix_to_loader[suffix]()\n                except Exception as e:\n                    if \"Channel\" in str(e) and \"not found\" in str(e):\n                        LOGGER.warning(f\"[{self.filename}] Channel {self.channel} not found, skipping image.\")\n                    else:\n                        raise\n                else:\n                    if suffix == \".asd\":\n                        for index, frame in enumerate(self.image):\n                            self._check_image_size_and_add_to_dict(image=frame, filename=f\"{self.filename}_{index}\")\n                    else:\n                        self._check_image_size_and_add_to_dict(image=self.image, filename=self.filename)\n            else:\n                raise ValueError(\n                    f\"File type {suffix} not yet supported. Please make an issue at \\\n                https://github.com/AFM-SPM/TopoStats/issues, or email topostats@sheffield.ac.uk to request support for \\\n                this file type.\"\n                )\n\n    def _check_image_size_and_add_to_dict(self, image: np.ndarray, filename: str) -&gt; None:\n        \"\"\"Check the image is above a minimum size in both dimensions.\n\n        Images that do not meet the minimum size are not included for processing.\n\n        Parameters\n        ----------\n        image: np.ndarray\n            An array of the extracted AFM image.\n        filename: str\n            The name of the file\n        \"\"\"\n        if image.shape[0] &lt; self.MINIMUM_IMAGE_SIZE or image.shape[1] &lt; self.MINIMUM_IMAGE_SIZE:\n            LOGGER.warning(f\"[{filename}] Skipping, image too small: {image.shape}\")\n        else:\n            self.add_to_dict(image=image, filename=filename)\n            LOGGER.info(f\"[{filename}] Image added to processing.\")\n\n    def add_to_dict(self, image: np.ndarray, filename: str) -&gt; None:\n        \"\"\"Add an image and metadata to the img_dict dictionary under the key filename.\n\n        Adds the image and associated metadata such as any grain masks, and pixel to nanometere\n        scaling factor to the img_dict dictionary which is used as a place to store the image\n        information for processing.\n\n        Parameters\n        ----------\n        image: np.ndarray\n            An array of the extracted AFM image.\n        filename: str\n            The name of the file\n        \"\"\"\n        self.img_dict[filename] = {\n            \"filename\": filename,\n            \"img_path\": self.img_path.with_name(filename),\n            \"pixel_to_nm_scaling\": self.pixel_to_nm_scaling,\n            \"image_original\": image,\n            \"image_flattened\": None,\n            \"grain_masks\": self.grain_masks,\n        }\n</code></pre>"},{"location":"api/io/#topostats.io.LoadScans.__init__","title":"<code>__init__(img_paths: list, channel: str)</code>","text":"<p>Initialise the class.</p> <p>Parameters:</p> Name Type Description Default <code>img_path</code> <p>Path to a valid AFM scan to load.</p> required <code>channel</code> <code>str</code> <p>Image channel to extract from the scan.</p> required Source code in <code>topostats\\io.py</code> <pre><code>def __init__(\n    self,\n    img_paths: list,\n    channel: str,\n):\n    \"\"\"Initialise the class.\n\n    Parameters\n    ----------\n    img_path: Union[str, Path]\n        Path to a valid AFM scan to load.\n    channel: str\n        Image channel to extract from the scan.\n    \"\"\"\n    self.img_paths = img_paths\n    self.img_path = None\n    self.channel = channel\n    self.channel_data = None\n    self.filename = None\n    self.image = None\n    self.pixel_to_nm_scaling = None\n    self.grain_masks = {}\n    self.img_dict = {}\n    self.MINIMUM_IMAGE_SIZE = 10\n</code></pre>"},{"location":"api/io/#topostats.io.LoadScans.add_to_dict","title":"<code>add_to_dict(image: np.ndarray, filename: str) -&gt; None</code>","text":"<p>Add an image and metadata to the img_dict dictionary under the key filename.</p> <p>Adds the image and associated metadata such as any grain masks, and pixel to nanometere scaling factor to the img_dict dictionary which is used as a place to store the image information for processing.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>An array of the extracted AFM image.</p> required <code>filename</code> <code>str</code> <p>The name of the file</p> required Source code in <code>topostats\\io.py</code> <pre><code>def add_to_dict(self, image: np.ndarray, filename: str) -&gt; None:\n    \"\"\"Add an image and metadata to the img_dict dictionary under the key filename.\n\n    Adds the image and associated metadata such as any grain masks, and pixel to nanometere\n    scaling factor to the img_dict dictionary which is used as a place to store the image\n    information for processing.\n\n    Parameters\n    ----------\n    image: np.ndarray\n        An array of the extracted AFM image.\n    filename: str\n        The name of the file\n    \"\"\"\n    self.img_dict[filename] = {\n        \"filename\": filename,\n        \"img_path\": self.img_path.with_name(filename),\n        \"pixel_to_nm_scaling\": self.pixel_to_nm_scaling,\n        \"image_original\": image,\n        \"image_flattened\": None,\n        \"grain_masks\": self.grain_masks,\n    }\n</code></pre>"},{"location":"api/io/#topostats.io.LoadScans.get_data","title":"<code>get_data() -&gt; None</code>","text":"<p>Extract image, filepath and pixel to nm scaling value, and append these to the img_dic object.</p> Source code in <code>topostats\\io.py</code> <pre><code>def get_data(self) -&gt; None:\n    \"\"\"Extract image, filepath and pixel to nm scaling value, and append these to the img_dic object.\"\"\"\n    suffix_to_loader = {\n        \".spm\": self.load_spm,\n        \".jpk\": self.load_jpk,\n        \".ibw\": self.load_ibw,\n        \".gwy\": self.load_gwy,\n        \".topostats\": self.load_topostats,\n        \".asd\": self.load_asd,\n    }\n\n    for img_path in self.img_paths:\n        self.img_path = img_path\n        self.filename = img_path.stem\n        suffix = img_path.suffix\n        LOGGER.info(f\"Extracting image from {self.img_path}\")\n        LOGGER.debug(f\"File extension : {suffix}\")\n\n        # Check that the file extension is supported\n        if suffix in suffix_to_loader:\n            try:\n                self.image, self.pixel_to_nm_scaling = suffix_to_loader[suffix]()\n            except Exception as e:\n                if \"Channel\" in str(e) and \"not found\" in str(e):\n                    LOGGER.warning(f\"[{self.filename}] Channel {self.channel} not found, skipping image.\")\n                else:\n                    raise\n            else:\n                if suffix == \".asd\":\n                    for index, frame in enumerate(self.image):\n                        self._check_image_size_and_add_to_dict(image=frame, filename=f\"{self.filename}_{index}\")\n                else:\n                    self._check_image_size_and_add_to_dict(image=self.image, filename=self.filename)\n        else:\n            raise ValueError(\n                f\"File type {suffix} not yet supported. Please make an issue at \\\n            https://github.com/AFM-SPM/TopoStats/issues, or email topostats@sheffield.ac.uk to request support for \\\n            this file type.\"\n            )\n</code></pre>"},{"location":"api/io/#topostats.io.LoadScans.load_asd","title":"<code>load_asd() -&gt; tuple</code>","text":"<p>Extract image and pixel to nm scaling from .asd files.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <code>(ndarray, float)</code> <p>A tuple containing the image and its pixel to nanometre scaling value.</p> Source code in <code>topostats\\io.py</code> <pre><code>def load_asd(self) -&gt; tuple:\n    \"\"\"Extract image and pixel to nm scaling from .asd files.\n\n    Returns\n    -------\n    tuple: (np.ndarray, float)\n        A tuple containing the image and its pixel to nanometre scaling value.\n    \"\"\"\n    try:\n        frames: np.ndarray\n        pixel_to_nm_scaling: float\n        _: dict\n        frames, pixel_to_nm_scaling, _ = asd.load_asd(file_path=self.img_path, channel=self.channel)\n        LOGGER.info(f\"[{self.filename}] : Loaded image from : {self.img_path}\")\n    except FileNotFoundError:\n        LOGGER.info(f\"[{self.filename}] : File not found. Path: {self.img_path}\")\n        raise\n\n    return (frames, pixel_to_nm_scaling)\n</code></pre>"},{"location":"api/io/#topostats.io.LoadScans.load_gwy","title":"<code>load_gwy() -&gt; tuple</code>","text":"<p>Extract image and pixel to nm scaling from the Gwyddion .gwy file.</p> <p>Returns:</p> Type Description <code>tuple(ndarray, float)</code> <p>A tuple containing the image and its pixel to nanometre scaling value.</p> Source code in <code>topostats\\io.py</code> <pre><code>def load_gwy(self) -&gt; tuple:\n    \"\"\"Extract image and pixel to nm scaling from the Gwyddion .gwy file.\n\n    Returns\n    -------\n    tuple(np.ndarray, float)\n        A tuple containing the image and its pixel to nanometre scaling value.\n    \"\"\"\n    LOGGER.info(f\"Loading image from : {self.img_path}\")\n    try:\n        image_data_dict = {}\n        with Path.open(self.img_path, \"rb\") as open_file:  # pylint: disable=unspecified-encoding\n            # Read header\n            header = open_file.read(4)\n            LOGGER.debug(f\"Gwy file header: {header}\")\n\n            LoadScans._gwy_read_object(open_file, data_dict=image_data_dict)\n\n        # For development - uncomment to have an indentation based nested\n        # dictionary output showing the object - component structure and\n        # available keys:\n        # LoadScans._gwy_print_dict_wrapper(gwy_file_dict=image_data_dict)\n\n        if \"/0/data\" in image_data_dict:\n            image = image_data_dict[\"/0/data\"][\"data\"]\n            units = image_data_dict[\"/0/data\"][\"si_unit_xy\"][\"unitstr\"]\n            px_to_nm = image_data_dict[\"/0/data\"][\"xreal\"] * 1e9 / image.shape[1]\n        elif \"/1/data\" in image_data_dict:\n            image = image_data_dict[\"/1/data\"][\"data\"]\n            px_to_nm = image_data_dict[\"/1/data\"][\"xreal\"] * 1e9 / image.shape[1]\n            units = image_data_dict[\"/1/data\"][\"si_unit_xy\"][\"unitstr\"]\n        else:\n            raise KeyError(\n                \"Data location not defined in the .gwy file. Please locate it and add to the load_gwy() function.\"\n            )\n\n        # Convert image heights to nanometresQ\n        if units == \"m\":\n            image = image * 1e9\n        else:\n            raise ValueError(\n                f\"Units '{units}' have not been added for .gwy files. Please add \\\n                an SI to nanometre conversion factor for these units in _gwy_read_component in \\\n                io.py.\"\n            )\n\n    except FileNotFoundError:\n        LOGGER.info(f\"[{self.filename}] File not found : {self.img_path}\")\n        raise\n\n    return (image, px_to_nm)\n</code></pre>"},{"location":"api/io/#topostats.io.LoadScans.load_ibw","title":"<code>load_ibw() -&gt; tuple</code>","text":"<p>Load image from Asylum Research (Igor) .ibw files.</p> <p>Returns:</p> Type Description <code>tuple(ndarray, float)</code> <p>A tuple containing the image and its pixel to nanometre scaling value.</p> Source code in <code>topostats\\io.py</code> <pre><code>def load_ibw(self) -&gt; tuple:\n    \"\"\"Load image from Asylum Research (Igor) .ibw files.\n\n    Returns\n    -------\n    tuple(np.ndarray, float)\n        A tuple containing the image and its pixel to nanometre scaling value.\n    \"\"\"\n    LOGGER.info(f\"Loading image from : {self.img_path}\")\n    try:\n        scan = binarywave.load(self.img_path)\n        LOGGER.info(f\"[{self.filename}] : Loaded image from : {self.img_path}\")\n\n        labels = []\n        for label_list in scan[\"wave\"][\"labels\"]:\n            for label in label_list:\n                if label:\n                    labels.append(label.decode())\n        channel_idx = labels.index(self.channel)\n        image = scan[\"wave\"][\"wData\"][:, :, channel_idx].T * 1e9  # Looks to be in m\n        image = np.flipud(image)\n        LOGGER.info(f\"[{self.filename}] : Extracted channel {self.channel}\")\n    except FileNotFoundError:\n        LOGGER.info(f\"[{self.filename}] File not found : {self.img_path}\")\n    except ValueError:\n        LOGGER.error(f\"[{self.filename}] : {self.channel} not in {self.img_path.suffix} channel list: {labels}\")\n        raise\n    except Exception as exception:\n        LOGGER.error(f\"[{self.filename}] : {exception}\")\n\n    return (image, self._ibw_pixel_to_nm_scaling(scan))\n</code></pre>"},{"location":"api/io/#topostats.io.LoadScans.load_jpk","title":"<code>load_jpk() -&gt; tuple</code>","text":"<p>Load image from JPK Instruments .jpk files.</p> <p>Returns:</p> Type Description <code>tuple(ndarray, float)</code> <p>A tuple containing the image and its pixel to nanometre scaling value.</p> Source code in <code>topostats\\io.py</code> <pre><code>def load_jpk(self) -&gt; tuple:\n    \"\"\"Load image from JPK Instruments .jpk files.\n\n    Returns\n    -------\n    tuple(np.ndarray, float)\n        A tuple containing the image and its pixel to nanometre scaling value.\n    \"\"\"\n    # Load the file\n    img_path = str(self.img_path)\n    try:\n        tif = tifffile.TiffFile(img_path)\n    except FileNotFoundError:\n        LOGGER.info(f\"[{self.filename}] File not found : {self.img_path}\")\n        raise\n    # Obtain channel list for all channels in file\n    channel_list = {}\n    for i, page in enumerate(tif.pages[1:]):  # [0] is thumbnail\n        available_channel = page.tags[\"32848\"].value  # keys are hexadecimal values\n        if page.tags[\"32849\"].value == 0:  # whether img is trace or retrace\n            tr_rt = \"trace\"\n        else:\n            tr_rt = \"retrace\"\n        channel_list[f\"{available_channel}_{tr_rt}\"] = i + 1\n    try:\n        channel_idx = channel_list[self.channel]\n    except KeyError:\n        LOGGER.error(f\"{self.channel} not in channel list: {channel_list}\")\n        raise\n    # Get image and if applicable, scale it\n    channel_page = tif.pages[channel_idx]\n    image = channel_page.asarray()\n    scaling_type = channel_page.tags[\"33027\"].value\n    if scaling_type == \"LinearScaling\":\n        scaling = channel_page.tags[\"33028\"].value\n        offset = channel_page.tags[\"33029\"].value\n        image = (image * scaling) + offset\n    elif scaling_type == \"NullScaling\":\n        pass\n    else:\n        raise ValueError(f\"Scaling type {scaling_type} is not 'NullScaling' or 'LinearScaling'\")\n    # Get page for common metadata between scans\n    metadata_page = tif.pages[0]\n    return (image * 1e9, self._jpk_pixel_to_nm_scaling(metadata_page))\n</code></pre>"},{"location":"api/io/#topostats.io.LoadScans.load_spm","title":"<code>load_spm() -&gt; tuple</code>","text":"<p>Extract image and pixel to nm scaling from the Bruker .spm file.</p> <p>Returns:</p> Type Description <code>tuple(ndarray, float)</code> <p>A tuple containing the image and its pixel to nanometre scaling value.</p> Source code in <code>topostats\\io.py</code> <pre><code>def load_spm(self) -&gt; tuple:\n    \"\"\"Extract image and pixel to nm scaling from the Bruker .spm file.\n\n    Returns\n    -------\n    tuple(np.ndarray, float)\n        A tuple containing the image and its pixel to nanometre scaling value.\n    \"\"\"\n    LOGGER.info(f\"Loading image from : {self.img_path}\")\n    try:\n        scan = pySPM.Bruker(self.img_path)\n        LOGGER.info(f\"[{self.filename}] : Loaded image from : {self.img_path}\")\n        self.channel_data = scan.get_channel(self.channel)\n        LOGGER.info(f\"[{self.filename}] : Extracted channel {self.channel}\")\n        image = np.flipud(np.array(self.channel_data.pixels))\n    except FileNotFoundError:\n        LOGGER.info(f\"[{self.filename}] File not found : {self.img_path}\")\n        raise\n    except Exception as e:\n        # trying to return the error with options of possible channel values\n        labels = []\n        for channel in [layer[b\"@2:Image Data\"][0] for layer in scan.layers]:\n            channel_description = channel.decode(\"latin1\").split('\"')[1]  # in case blank field raises questions?\n            labels.append(channel_description)\n        LOGGER.error(f\"[{self.filename}] : {self.channel} not in {self.img_path.suffix} channel list: {labels}\")\n        raise e\n\n    return (image, self._spm_pixel_to_nm_scaling(self.channel_data))\n</code></pre>"},{"location":"api/io/#topostats.io.LoadScans.load_topostats","title":"<code>load_topostats() -&gt; tuple</code>","text":"<p>Load a .topostats file (hdf5 format).</p> <p>Loads and extracts the image, pixel to nanometre scaling factor and any grain masks.</p> <p>Note that grain masks are stored via self.grain_masks rather than returned due to how we extract information for all other file loading functions.</p> <p>Returns:</p> Type Description <code>tuple(ndarray, float)</code> <p>A tuple containing the image and its pixel to nanometre scaling value.</p> Source code in <code>topostats\\io.py</code> <pre><code>def load_topostats(self) -&gt; tuple:\n    \"\"\"Load a .topostats file (hdf5 format).\n\n    Loads and extracts the image, pixel to nanometre scaling factor and any grain masks.\n\n    Note that grain masks are stored via self.grain_masks rather than returned due to how we extract information for\n    all other file loading functions.\n\n    Returns\n    -------\n    tuple(np.ndarray, float)\n        A tuple containing the image and its pixel to nanometre scaling value.\n    \"\"\"\n    LOGGER.info(f\"Loading image from : {self.img_path}\")\n    try:\n        with h5py.File(self.img_path, \"r\") as f:\n            keys = f.keys()\n            file_version = f[\"topostats_file_version\"][()]\n            LOGGER.info(f\"TopoStats file version: {file_version}\")\n            image = f[\"image\"][:]\n            pixel_to_nm_scaling = f[\"pixel_to_nm_scaling\"][()]\n            if \"grain_masks\" in keys:\n                grain_masks_keys = f[\"grain_masks\"].keys()\n                if \"above\" in grain_masks_keys:\n                    LOGGER.info(f\"[{self.filename}] : Found grain mask for above direction\")\n                    self.grain_masks[\"above\"] = f[\"grain_masks\"][\"above\"][:]\n                if \"below\" in grain_masks_keys:\n                    LOGGER.info(f\"[{self.filename}] : Found grain mask for below direction\")\n                    self.grain_masks[\"below\"] = f[\"grain_masks\"][\"below\"][:]\n    except OSError as e:\n        if \"Unable to open file\" in str(e):\n            LOGGER.info(f\"[{self.filename}] File not found: {self.img_path}\")\n        raise e\n\n    return (image, pixel_to_nm_scaling)\n</code></pre>"},{"location":"api/io/#topostats.io.convert_basename_to_relative_paths","title":"<code>convert_basename_to_relative_paths(df: pd.DataFrame)</code>","text":"<p>Convert paths in the 'basename' column of a dataframe to relative paths.</p> <p>If the 'basename' column has the following paths: ['/usr/topo/data/a/b', '/usr/topo/data/c/d'], the output will be: ['a/b', 'c/d'].</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas dataframe containing a column 'basename' which contains the paths indicating the locations of the image data files.</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>A pandas dataframe where the 'basename' column has paths relative to a common parent.</p> Source code in <code>topostats\\io.py</code> <pre><code>def convert_basename_to_relative_paths(df: pd.DataFrame):\n    \"\"\"Convert paths in the 'basename' column of a dataframe to relative paths.\n\n    If the 'basename' column has the following paths: ['/usr/topo/data/a/b', '/usr/topo/data/c/d'], the output will be:\n    ['a/b', 'c/d'].\n\n    Parameters\n    ----------\n    df: pd.DataFrame\n        A pandas dataframe containing a column 'basename' which contains the paths\n        indicating the locations of the image data files.\n\n    Returns\n    -------\n    df: pd.DataFrame\n        A pandas dataframe where the 'basename' column has paths relative to a common\n        parent.\n    \"\"\"\n    paths = df[\"basename\"].tolist()\n    paths = [Path(path) for path in paths]\n    relative_paths = get_relative_paths(paths=paths)\n    df[\"basename\"] = relative_paths\n\n    return df\n</code></pre>"},{"location":"api/io/#topostats.io.find_files","title":"<code>find_files(base_dir: str | Path = None, file_ext: str = '.spm') -&gt; list</code>","text":"<p>Recursively scan the specified directory for images with the given file extension.</p> <p>Parameters:</p> Name Type Description Default <code>base_dir</code> <code>str | Path</code> <p>Directory to recursively search for files, if not specified the current directory is scanned.</p> <code>None</code> <code>file_ext</code> <code>str</code> <p>File extension to search for.</p> <code>'.spm'</code> <p>Returns:</p> Type Description <code>List</code> <p>List of files found with the extension in the given directory.</p> Source code in <code>topostats\\io.py</code> <pre><code>def find_files(base_dir: str | Path = None, file_ext: str = \".spm\") -&gt; list:\n    \"\"\"Recursively scan the specified directory for images with the given file extension.\n\n    Parameters\n    ----------\n    base_dir: Union[str, Path]\n        Directory to recursively search for files, if not specified the current directory is scanned.\n    file_ext: str\n        File extension to search for.\n\n    Returns\n    -------\n    List\n        List of files found with the extension in the given directory.\n    \"\"\"\n    base_dir = Path(\"./\") if base_dir is None else Path(base_dir)\n    return list(base_dir.glob(\"**/*\" + file_ext))\n</code></pre>"},{"location":"api/io/#topostats.io.get_date_time","title":"<code>get_date_time() -&gt; str</code>","text":"<p>Get a date and time for adding to generated files or logging.</p> <p>Parameters:</p> Name Type Description Default <code>None</code> required <p>Returns:</p> Type Description <code>str</code> <p>A string of the current date and time, formatted appropriately.</p> Source code in <code>topostats\\io.py</code> <pre><code>def get_date_time() -&gt; str:\n    \"\"\"\n    Get a date and time for adding to generated files or logging.\n\n    Parameters\n    ----------\n    None\n\n    Returns\n    -------\n    str\n        A string of the current date and time, formatted appropriately.\n    \"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"api/io/#topostats.io.get_out_path","title":"<code>get_out_path(image_path: str | Path = None, base_dir: str | Path = None, output_dir: str | Path = None) -&gt; Path</code>","text":"<p>Add the image path relative to the base directory to the output directory.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str | Path</code> <p>The path of the current image.</p> <code>None</code> <code>base_dir</code> <code>str | Path</code> <p>Directory to recursively search for files.</p> <code>None</code> <code>output_dir</code> <code>str | Path</code> <p>The output directory specified in the configuration file.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>The output path that mirrors the input path structure.</p> Source code in <code>topostats\\io.py</code> <pre><code>def get_out_path(image_path: str | Path = None, base_dir: str | Path = None, output_dir: str | Path = None) -&gt; Path:\n    \"\"\"Add the image path relative to the base directory to the output directory.\n\n    Parameters\n    ----------\n    image_path: Path\n        The path of the current image.\n    base_dir: Path\n        Directory to recursively search for files.\n    output_dir: Path\n        The output directory specified in the configuration file.\n\n    Returns\n    -------\n    Path\n        The output path that mirrors the input path structure.\n    \"\"\"\n    # If image_path is relative and doesn't include base_dir then a ValueError is raised, in which\n    # case we just want to append the image_path to the output_dir\n    try:\n        # Remove the filename if there is a suffix, not always the case as\n        # get_out_path is called from save_folder_grainstats()\n        if image_path.suffix:\n            return output_dir / image_path.relative_to(base_dir).parent / image_path.stem\n        return output_dir / image_path.relative_to(base_dir)\n    except ValueError:\n        if image_path.suffix:\n            return output_dir / image_path.parent / image_path.stem\n        return Path(str(output_dir) + \"/\" + str(image_path))\n    # AttributeError is raised if image_path is a string (since it isn't a Path() object with a .suffix)\n    except AttributeError:\n        LOGGER.error(\"A string form of a Path has been passed to 'get_out_path()' for image_path\")\n        raise\n</code></pre>"},{"location":"api/io/#topostats.io.get_relative_paths","title":"<code>get_relative_paths(paths: list[Path]) -&gt; list[str]</code>","text":"<p>Extract a list of relative paths, removing the common suffix.</p> <p>From a list of paths, create a list where each path is relative to all path's closest common parent. For example, ['a/b/c', 'a/b/d', 'a/b/e/f'] would return ['c', 'd', 'e/f'].</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list[Path]</code> <p>List of string or pathlib paths.</p> required <p>Returns:</p> Name Type Description <code>relative_paths</code> <code>list</code> <p>List of string paths, relative to the common parent.</p> Source code in <code>topostats\\io.py</code> <pre><code>def get_relative_paths(paths: list[Path]) -&gt; list[str]:\n    \"\"\"Extract a list of relative paths, removing the common suffix.\n\n    From a list of paths, create a list where each path is relative to all path's closest common parent. For\n    example, ['a/b/c', 'a/b/d', 'a/b/e/f'] would return ['c', 'd', 'e/f'].\n\n    Parameters\n    ----------\n    paths: list\n        List of string or pathlib paths.\n\n    Returns\n    -------\n    relative_paths: list\n        List of string paths, relative to the common parent.\n    \"\"\"\n    # Ensure paths are all pathlib paths, and not strings\n    paths = [Path(path) for path in paths]\n\n    # If the paths list consists of all the same path, then the relative path will\n    # be '.', which we don't want. we want the relative path to be the full path probably.\n    # len(set(my_list)) == 1 determines if all the elements in a list are the same.\n    if len(set(paths)) == 1:\n        return [str(path.as_posix()) for path in paths]\n\n    deepest_common_path = os.path.commonpath(paths)\n    # Have to convert to strings else the dataframe values will be slightly different\n    # to what is expected.\n    return [str(path.relative_to(deepest_common_path).as_posix()) for path in paths]\n</code></pre>"},{"location":"api/io/#topostats.io.load_array","title":"<code>load_array(array_path: str | Path) -&gt; np.ndarray</code>","text":"<p>Load a Numpy array from file.</p> <p>Should have been saved using save_array() or numpy.save().</p> <p>Parameters:</p> Name Type Description Default <code>array_path</code> <code>Union[str, Path]</code> <p>Path to the Numpy array on disk.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Returns the loaded Numpy array.</p> Source code in <code>topostats\\io.py</code> <pre><code>def load_array(array_path: str | Path) -&gt; np.ndarray:\n    \"\"\"Load a Numpy array from file.\n\n    Should have been saved using save_array() or numpy.save().\n\n    Parameters\n    ----------\n    array_path : Union[str, Path]\n        Path to the Numpy array on disk.\n\n    Returns\n    -------\n    np.ndarray\n        Returns the loaded Numpy array.\n    \"\"\"\n    try:\n        return np.load(Path(array_path))\n    except FileNotFoundError as e:\n        raise e\n</code></pre>"},{"location":"api/io/#topostats.io.load_pkl","title":"<code>load_pkl(infile: Path) -&gt; Any</code>","text":"<p>Load data from a pickle.</p> <p>Parameters:</p> Name Type Description Default <code>infile</code> <code>Path</code> <p>Path to a valid pickle.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Any</code> <p>Dictionary of generated images.</p> Example <p>from pathlib import Path</p> <p>from topostats.io import load_plots</p> <p>pkl_path = \"output/distribution_plots.pkl\" my_plots = load_pkl(pkl_path)</p> Source code in <code>topostats\\io.py</code> <pre><code>def load_pkl(infile: Path) -&gt; Any:\n    \"\"\"Load data from a pickle.\n\n    Parameters\n    ----------\n    infile: Path\n        Path to a valid pickle.\n\n    Returns\n    -------\n    dict:\n        Dictionary of generated images.\n\n    Example\n    -------\n\n    from pathlib import Path\n\n    from topostats.io import load_plots\n\n    pkl_path = \"output/distribution_plots.pkl\"\n    my_plots = load_pkl(pkl_path)\n    # Show the type of my_plots which is a dictionary of nested dictionaries\n    type(my_plots)\n    # Show the keys are various levels of nesting.\n    my_plots.keys()\n    my_plots[\"area\"].keys()\n    my_plots[\"area\"][\"dist\"].keys()\n    # Get the figure and axis object for a given metrics distribution plot\n    figure, axis = my_plots[\"area\"][\"dist\"].values()\n    # Get the figure and axis object for a given metrics violin plot\n    figure, axis = my_plots[\"area\"][\"violin\"].values()\n\n    \"\"\"\n    with infile.open(\"rb\", encoding=None) as f:\n        return pkl.load(f)  # noqa: S301\n</code></pre>"},{"location":"api/io/#topostats.io.load_pkl--show-the-type-of-my_plots-which-is-a-dictionary-of-nested-dictionaries","title":"Show the type of my_plots which is a dictionary of nested dictionaries","text":"<p>type(my_plots)</p>"},{"location":"api/io/#topostats.io.load_pkl--show-the-keys-are-various-levels-of-nesting","title":"Show the keys are various levels of nesting.","text":"<p>my_plots.keys() my_plots[\"area\"].keys() my_plots[\"area\"][\"dist\"].keys()</p>"},{"location":"api/io/#topostats.io.load_pkl--get-the-figure-and-axis-object-for-a-given-metrics-distribution-plot","title":"Get the figure and axis object for a given metrics distribution plot","text":"<p>figure, axis = my_plots[\"area\"][\"dist\"].values()</p>"},{"location":"api/io/#topostats.io.load_pkl--get-the-figure-and-axis-object-for-a-given-metrics-violin-plot","title":"Get the figure and axis object for a given metrics violin plot","text":"<p>figure, axis = my_plots[\"area\"][\"violin\"].values()</p>"},{"location":"api/io/#topostats.io.path_to_str","title":"<code>path_to_str(config: dict) -&gt; dict</code>","text":"<p>Recursively traverse a dictionary and convert any Path() objects to strings for writing to YAML.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Dictionary to be converted.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>dict</code> <p>The same dictionary with any Path() objects converted to string.</p> Source code in <code>topostats\\io.py</code> <pre><code>def path_to_str(config: dict) -&gt; dict:\n    \"\"\"Recursively traverse a dictionary and convert any Path() objects to strings for writing to YAML.\n\n    Parameters\n    ----------\n    config: dict\n        Dictionary to be converted.\n\n    Returns\n    -------\n    Dict:\n        The same dictionary with any Path() objects converted to string.\n    \"\"\"\n    for key, value in config.items():\n        if isinstance(value, dict):\n            path_to_str(value)\n        elif isinstance(value, Path):\n            config[key] = str(value)\n\n    return config\n</code></pre>"},{"location":"api/io/#topostats.io.read_64d","title":"<code>read_64d(open_file: io.TextIOWrapper) -&gt; str</code>","text":"<p>Read a 64-bit double from an open binary file.</p> <p>Parameters:</p> Name Type Description Default <code>open_file</code> <code>TextIOWrapper</code> <p>An open file object.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Python float type cast from the double.</p> Source code in <code>topostats\\io.py</code> <pre><code>def read_64d(open_file: io.TextIOWrapper) -&gt; str:\n    \"\"\"Read a 64-bit double from an open binary file.\n\n    Parameters\n    ----------\n    open_file:\n        An open file object.\n\n    Returns\n    -------\n    float\n        Python float type cast from the double.\n    \"\"\"\n    return float(struct.unpack(\"d\", open_file.read(8))[0])\n</code></pre>"},{"location":"api/io/#topostats.io.read_char","title":"<code>read_char(open_file: io.TextIOWrapper) -&gt; str</code>","text":"<p>Read a character from an open binary file.</p> <p>Parameters:</p> Name Type Description Default <code>open_file</code> <code>TextIOWrapper</code> <p>An open file object.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string type cast from the decoded character.</p> Source code in <code>topostats\\io.py</code> <pre><code>def read_char(open_file: io.TextIOWrapper) -&gt; str:\n    \"\"\"Read a character from an open binary file.\n\n    Parameters\n    ----------\n    open_file: io.TextIOWrapper\n        An open file object.\n\n    Returns\n    -------\n    str\n        A string type cast from the decoded character.\n    \"\"\"\n    return open_file.read(1).decode(\"ascii\")\n</code></pre>"},{"location":"api/io/#topostats.io.read_gwy_component_dtype","title":"<code>read_gwy_component_dtype(open_file: io.TextIOWrapper) -&gt; str</code>","text":"<p>Read the data type of a <code>.gwy</code> file component.</p> <p>Possible data types are as follows:</p> <ul> <li>'b': boolean</li> <li>'c': character</li> <li>'i': 32-bit integer</li> <li>'q': 64-bit integer</li> <li>'d': double</li> <li>'s': string</li> <li>'o': <code>.gwy</code> format object</li> </ul> <p>Capitalised versions of some of these data types represent arrays of values of that data type. Arrays are stored as an unsigned 32 bit integer, describing the size of the array, followed by the unseparated array values:</p> <ul> <li>'C': array of characters</li> <li>'I': array of 32-bit integers</li> <li>'Q': array of 64-bit integers</li> <li>'D': array of doubles</li> <li>'S': array of strings</li> <li>'O': array of objects.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>open_file</code> <code>TextIOWrapper</code> <p>An open file object.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Python string (one character long) of the data type of the component's value.</p> Source code in <code>topostats\\io.py</code> <pre><code>def read_gwy_component_dtype(open_file: io.TextIOWrapper) -&gt; str:\n    \"\"\"Read the data type of a `.gwy` file component.\n\n    Possible data types are as follows:\n\n    - 'b': boolean\n    - 'c': character\n    - 'i': 32-bit integer\n    - 'q': 64-bit integer\n    - 'd': double\n    - 's': string\n    - 'o': `.gwy` format object\n\n    Capitalised versions of some of these data types represent arrays of values of that data type. Arrays are stored as\n    an unsigned 32 bit integer, describing the size of the array, followed by the unseparated array values:\n\n    - 'C': array of characters\n    - 'I': array of 32-bit integers\n    - 'Q': array of 64-bit integers\n    - 'D': array of doubles\n    - 'S': array of strings\n    - 'O': array of objects.\n\n    Parameters\n    ----------\n    open_file: io.TextIOWrapper\n        An open file object.\n\n    Returns\n    -------\n    str\n        Python string (one character long) of the data type of the\n        component's value.\n    \"\"\"\n    return open_file.read(1).decode(\"ascii\")\n</code></pre>"},{"location":"api/io/#topostats.io.read_null_terminated_string","title":"<code>read_null_terminated_string(open_file: io.TextIOWrapper) -&gt; str</code>","text":"<p>Read an open file from the current position in the open binary file, until the next null value.</p> <p>Parameters:</p> Name Type Description Default <code>open_file</code> <code>TextIOWrapper</code> <p>An open file object.</p> required <p>Returns:</p> Type Description <code>str</code> <p>String of the ASCII decoded bytes before the next null byte.</p> Source code in <code>topostats\\io.py</code> <pre><code>def read_null_terminated_string(open_file: io.TextIOWrapper) -&gt; str:\n    \"\"\"Read an open file from the current position in the open binary file, until the next null value.\n\n    Parameters\n    ----------\n    open_file: io.TextIOWrapper\n        An open file object.\n\n    Returns\n    -------\n    str\n        String of the ASCII decoded bytes before the next null byte.\n    \"\"\"\n    byte = open_file.read(1)\n    value = b\"\"\n    while byte != b\"\\x00\":\n        value += byte\n        byte = open_file.read(1)\n    return str(value.decode(\"utf-8\"))\n</code></pre>"},{"location":"api/io/#topostats.io.read_u32i","title":"<code>read_u32i(open_file: io.TextIOWrapper) -&gt; str</code>","text":"<p>Read an unsigned 32 bit integer from an open binary file (in little-endian form).</p> <p>Parameters:</p> Name Type Description Default <code>open_file</code> <code>TextIOWrapper</code> <p>An open file object.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Python integer type cast from the unsigned 32 bit integer.</p> Source code in <code>topostats\\io.py</code> <pre><code>def read_u32i(open_file: io.TextIOWrapper) -&gt; str:\n    \"\"\"Read an unsigned 32 bit integer from an open binary file (in little-endian form).\n\n    Parameters\n    ----------\n    open_file: io.TextIOWrapper\n        An open file object.\n\n    Returns\n    -------\n    int\n        Python integer type cast from the unsigned 32 bit integer.\n    \"\"\"\n    return int(struct.unpack(\"&lt;i\", open_file.read(4))[0])\n</code></pre>"},{"location":"api/io/#topostats.io.read_yaml","title":"<code>read_yaml(filename: str | Path) -&gt; dict</code>","text":"<p>Read a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str | Path</code> <p>YAML file to read.</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>Dictionary of the file.</p> Source code in <code>topostats\\io.py</code> <pre><code>def read_yaml(filename: str | Path) -&gt; dict:\n    \"\"\"Read a YAML file.\n\n    Parameters\n    ----------\n    filename: Union[str, Path]\n        YAML file to read.\n\n    Returns\n    -------\n    Dict\n        Dictionary of the file.\n    \"\"\"\n    with Path(filename).open(encoding=\"utf-8\") as f:\n        try:\n            yaml_file = YAML(typ=\"safe\")\n            return yaml_file.load(f)\n        except YAMLError as exception:\n            LOGGER.error(exception)\n            return {}\n</code></pre>"},{"location":"api/io/#topostats.io.save_array","title":"<code>save_array(array: np.ndarray, outpath: Path, filename: str, array_type: str) -&gt; None</code>","text":"<p>Save a Numpy array to disk.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Numpy array to be saved.</p> required <code>outpath</code> <code>Path</code> <p>Location array should be saved</p> required <code>filename</code> <code>str</code> <p>Filename of the current image from which the array is derived.</p> required <code>array_type</code> <code>str</code> <p>Short string describing the array type e.g. z_threshold. Ideally should not have periods or spaces in (use</p> required <code>underscores</code> required Source code in <code>topostats\\io.py</code> <pre><code>def save_array(array: np.ndarray, outpath: Path, filename: str, array_type: str) -&gt; None:\n    \"\"\"Save a Numpy array to disk.\n\n    Parameters\n    ----------\n    array : np.ndarray\n        Numpy array to be saved.\n    outpath : Path\n        Location array should be saved\n    filename : str\n        Filename of the current image from which the array is derived.\n    array_type : str\n        Short string describing the array type e.g. z_threshold. Ideally should not have periods or spaces in (use\n    underscores '_' instead).\n    \"\"\"\n    np.save(outpath / f\"{filename}_{array_type}.npy\", array)\n    LOGGER.info(f\"[{filename}] Numpy array saved to : {outpath}/{filename}_{array_type}.npy\")\n</code></pre>"},{"location":"api/io/#topostats.io.save_folder_grainstats","title":"<code>save_folder_grainstats(output_dir: str | Path, base_dir: str | Path, all_stats_df: pd.DataFrame) -&gt; None</code>","text":"<p>Save a data frame of grain and tracing statistics at the folder level.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str | Path</code> <p>Path of the output directory head.</p> required <code>base_dir</code> <code>str | Path</code> <p>Path of the base directory where files were found.</p> required <code>all_stats_df</code> <code>DataFrame</code> <p>The dataframe containing all sample statistics run.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This only saves the dataframes and does not retain them.</p> Source code in <code>topostats\\io.py</code> <pre><code>def save_folder_grainstats(output_dir: str | Path, base_dir: str | Path, all_stats_df: pd.DataFrame) -&gt; None:\n    \"\"\"Save a data frame of grain and tracing statistics at the folder level.\n\n    Parameters\n    ----------\n    output_dir: Union[str, Path]\n        Path of the output directory head.\n    base_dir: Union[str, Path]\n        Path of the base directory where files were found.\n    all_stats_df: pd.DataFrame\n        The dataframe containing all sample statistics run.\n\n    Returns\n    -------\n    None\n        This only saves the dataframes and does not retain them.\n    \"\"\"\n    dirs = set(all_stats_df[\"basename\"].values)\n    LOGGER.debug(f\"Statistics :\\n{all_stats_df}\")\n    for _dir in dirs:\n        LOGGER.debug(f\"Statistics ({_dir}) :\\n{all_stats_df}\")\n        try:\n            out_path = get_out_path(Path(_dir), base_dir, output_dir)\n            # Ensure \"processed\" directory exists at the stem of out_path, creating if needed\n            if out_path.stem != \"processed\":\n                out_path_processed = out_path / \"processed\"\n                out_path_processed.mkdir(parents=True, exist_ok=True)\n            all_stats_df[all_stats_df[\"basename\"] == _dir].to_csv(\n                out_path / \"processed\" / \"folder_grainstats.csv\", index=True\n            )\n            LOGGER.info(f\"Folder-wise statistics saved to: {str(out_path)}/folder_grainstats.csv\")\n        except TypeError:\n            LOGGER.info(f\"No folder-wise statistics for directory {_dir}, no grains detected in any images.\")\n</code></pre>"},{"location":"api/io/#topostats.io.save_pkl","title":"<code>save_pkl(outfile: Path, to_pkl: dict) -&gt; None</code>","text":"<p>Pickle objects for working with later.</p> <p>Parameters:</p> Name Type Description Default <code>outfile</code> <code>Path</code> <p>Path and filename to save pickle to.</p> required <code>to_pkl</code> <code>dict</code> <p>Object to be picled.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>topostats\\io.py</code> <pre><code>def save_pkl(outfile: Path, to_pkl: dict) -&gt; None:\n    \"\"\"Pickle objects for working with later.\n\n    Parameters\n    ----------\n    outfile: Path\n        Path and filename to save pickle to.\n    to_pkl: dict\n        Object to be picled.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    with outfile.open(mode=\"wb\", encoding=None) as f:\n        pkl.dump(to_pkl, f)\n</code></pre>"},{"location":"api/io/#topostats.io.save_topostats_file","title":"<code>save_topostats_file(output_dir: Path, filename: str, topostats_object: dict) -&gt; None</code>","text":"<p>Save a topostats dictionary object to a .topostats (hdf5 format) file.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Path</code> <p>Directory to save the .topostats file in.</p> required <code>filename</code> <code>str</code> <p>File name of the .topostats file.</p> required <code>topostats_object</code> <code>dict</code> <p>Dictionary of the topostats data to save. Must include a flattened image and pixel to nanometre scaling factor. May also include grain masks.</p> required Source code in <code>topostats\\io.py</code> <pre><code>def save_topostats_file(output_dir: Path, filename: str, topostats_object: dict) -&gt; None:\n    \"\"\"Save a topostats dictionary object to a .topostats (hdf5 format) file.\n\n    Parameters\n    ----------\n    output_dir: Path\n        Directory to save the .topostats file in.\n    filename: str\n        File name of the .topostats file.\n    topostats_object: dict\n        Dictionary of the topostats data to save. Must include a flattened image and\n        pixel to nanometre scaling factor. May also include grain masks.\n    \"\"\"\n    LOGGER.info(f\"[{filename}] : Saving image to .topostats file\")\n\n    if \".topostats\" not in filename:\n        save_file_path = output_dir / f\"{filename}.topostats\"\n    else:\n        save_file_path = output_dir / filename\n\n    with h5py.File(save_file_path, \"w\") as f:\n        # It may be possible for topostats_object[\"image_flattened\"] to be None.\n        # Make sure that this is not the case.\n        if topostats_object[\"image_flattened\"] is not None:\n            f[\"topostats_file_version\"] = 0.1\n            f[\"image\"] = topostats_object[\"image_flattened\"]\n            # It should not be possible for topostats_object[\"pixel_to_nm_scaling\"] to be None\n            f[\"pixel_to_nm_scaling\"] = topostats_object[\"pixel_to_nm_scaling\"]\n            if topostats_object[\"grain_masks\"]:\n                if \"above\" in topostats_object[\"grain_masks\"].keys():\n                    if topostats_object[\"grain_masks\"][\"above\"] is not None:\n                        f[\"grain_masks/above\"] = topostats_object[\"grain_masks\"][\"above\"]\n                if \"below\" in topostats_object[\"grain_masks\"].keys():\n                    if topostats_object[\"grain_masks\"][\"below\"] is not None:\n                        f[\"grain_masks/below\"] = topostats_object[\"grain_masks\"][\"below\"]\n        else:\n            raise ValueError(\n                \"TopoStats object dictionary does not contain an 'image_flattened'. \\\n                 TopoStats objects must be saved with a flattened image.\"\n            )\n</code></pre>"},{"location":"api/io/#topostats.io.write_config_with_comments","title":"<code>write_config_with_comments(args=None) -&gt; None</code>","text":"<p>Write a sample configuration with in-line comments.</p> <p>This function is not designed to be used interactively but can be, just call it without any arguments and it will write a configuration to './config.yaml'.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>A Namespace object parsed from argparse with values for 'filename',</p> <code>None</code> Source code in <code>topostats\\io.py</code> <pre><code>def write_config_with_comments(args=None) -&gt; None:\n    \"\"\"\n    Write a sample configuration with in-line comments.\n\n    This function is not designed to be used interactively but can be, just call it without any arguments and it will\n    write a configuration to './config.yaml'.\n\n    Parameters\n    ----------\n    args: Namespace\n        A Namespace object parsed from argparse with values for 'filename',\n    \"\"\"\n    filename = \"config\" if args.filename is None else args.filename\n    output_dir = Path(\"./\") if args.output_dir is None else Path(args.output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n    logger_msg = \"A sample configuration has been written to\"\n    # If no config or default is requested we load the default_config.yaml\n    if args.config is None or args.config == \"default\":\n        config = pkg_resources.open_text(__package__, \"default_config.yaml\").read()\n    elif args.config == \"topostats.mplstyle\":\n        config = pkg_resources.open_text(__package__, \"topostats.mplstyle\").read()\n        logger_msg = \"A sample matplotlibrc parameters file has been written to\"\n    # Otherwise we have scope for loading different configs based on the argument, add future dictionaries to\n    # topostats/&lt;sample_type&gt;_config.yaml\n    else:\n        try:\n            config = pkg_resources.open_text(__package__, f\"{args.config}_config.yaml\").read()\n        except FileNotFoundError as e:\n            raise UserWarning(f\"There is no configuration for samples of type : {args.config}\") from e\n\n    if \".yaml\" not in filename and \".yml\" not in filename and \".mplstyle\" not in filename:\n        create_config_path = output_dir / f\"{filename}.yaml\"\n    else:\n        create_config_path = output_dir / filename\n\n    with create_config_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(f\"# Config file generated {get_date_time()}\\n\")\n        f.write(f\"# {CONFIG_DOCUMENTATION_REFERENCE}\")\n        f.write(config)\n    LOGGER.info(f\"{logger_msg} : {str(create_config_path)}\")\n    LOGGER.info(CONFIG_DOCUMENTATION_REFERENCE)\n</code></pre>"},{"location":"api/io/#topostats.io.write_yaml","title":"<code>write_yaml(config: dict, output_dir: str | Path, config_file: str = 'config.yaml', header_message: str = None) -&gt; None</code>","text":"<p>Write a configuration (stored as a dictionary) to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary.</p> required <code>output_dir</code> <code>str | Path</code> <p>Path to save the dictionary to as a YAML file (it will be called 'config.yaml').</p> required <code>config_file</code> <code>str</code> <p>Filename to write to.</p> <code>'config.yaml'</code> <code>header_message</code> <code>str</code> <p>String to write to the header message of the YAML file</p> <code>None</code> Source code in <code>topostats\\io.py</code> <pre><code>def write_yaml(\n    config: dict,\n    output_dir: str | Path,\n    config_file: str = \"config.yaml\",\n    header_message: str = None,\n) -&gt; None:\n    \"\"\"Write a configuration (stored as a dictionary) to a YAML file.\n\n    Parameters\n    ----------\n    config: dict\n        Configuration dictionary.\n    output_dir: Union[str, Path]\n        Path to save the dictionary to as a YAML file (it will be called 'config.yaml').\n    config_file: str\n        Filename to write to.\n    header_message: str\n        String to write to the header message of the YAML file\n    \"\"\"\n    # Save the configuration to output directory\n    output_config = Path(output_dir) / config_file\n    # Revert PosixPath items to string\n    config = path_to_str(config)\n\n    if header_message:\n        header = f\"# {header_message} : {get_date_time()}\\n\" + CONFIG_DOCUMENTATION_REFERENCE\n    else:\n        header = f\"# Configuration from TopoStats run completed : {get_date_time()}\\n\" + CONFIG_DOCUMENTATION_REFERENCE\n    output_config.write_text(header, encoding=\"utf-8\")\n\n    yaml = YAML(typ=\"safe\")\n    with output_config.open(\"a\", encoding=\"utf-8\") as f:\n        try:\n            yaml.dump(config, f)\n        except YAMLError as exception:\n            LOGGER.error(exception)\n</code></pre>"},{"location":"api/plotting/","title":"Plotting Modules","text":"<p>Plotting and summary of TopoStats output statistics.</p>"},{"location":"api/plotting/#topostats.plotting.TopoSum","title":"<code>TopoSum</code>","text":"<p>Class for summarising grain statistics in plots.</p> Source code in <code>topostats\\plotting.py</code> <pre><code>class TopoSum:\n    \"\"\"Class for summarising grain statistics in plots.\"\"\"\n\n    def __init__(\n        self,\n        df: pd.DataFrame = None,\n        base_dir: Union[str, Path] = None,\n        csv_file: Union[str, Path] = None,\n        stat_to_sum: str = None,\n        molecule_id: str = \"molecule_number\",\n        image_id: str = \"image\",\n        hist: bool = True,\n        stat: str = \"count\",\n        bins: int = 12,\n        kde: bool = True,\n        cut: float = 20,\n        figsize: tuple = (16, 9),\n        alpha: float = 0.5,\n        palette: str = \"deep\",\n        savefig_format: str = \"png\",\n        output_dir: Union[str, Path] = \".\",\n        var_to_label: dict = None,\n        hue: str = \"basename\",\n    ) -&gt; None:\n        \"\"\"Initialise the class.\n\n        Parameters\n        ==========\n        df: Union[pd.DataFrame]\n            Pandas data frame of data to be summarised.\n        base_dir: Union[str, Path]\n            Base directory from which all paths are relative to.\n        csv_file: Union[str, Path]\n            CSV file of data to be summarised.\n        stat_to_sum: str\n            Variable to summarise.\n        molecule_id: str\n            Variable that uniquely identifies molecules.\n        image_id: str\n            Variable that uniquely identifies images.\n        hist: bool\n            Whether to plot histograms.\n        stat: str\n            Statistic to plot on histogram 'count' (default), 'freq'.\n        bins: int\n            Number of bins to plot.\n        kde: bool\n            Whether to include a Kernel Density Estimate.\n        cut: float = 20,\n            Cut point for KDE.\n        figsize: tuple\n            Figure dimensions.\n        alpha: float\n            Opacity to use in plots.\n        palette: str = \"deep\"\n            Seaborn colour plot to use.\n        file_ext: str\n            File type to save plots as 'png' (default), 'pdf', 'svg'.\n        output_dir: Union[str, Path]\n            Location to save plots to.\n        var_to_label: dict\n            Variable to label dictionary for automatically adding titles to plots.\n        hue: str\n            Dataframe column to group plots by.\n\n        Returns\n        =======\n        \"\"\"\n        self.df = df if df is not None else pd.read_csv(csv_file)\n        self.base_dir = base_dir\n        self.stat_to_sum = stat_to_sum\n        self.molecule_id = molecule_id\n        self.image_id = image_id\n        self.hist = hist\n        self.bins = bins\n        self.stat = stat\n        self.kde = kde\n        self.cut = cut\n        self.figsize = figsize\n        self.alpha = alpha\n        self.palette = palette\n        self.savefig_format = savefig_format\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n        self.var_to_label = var_to_label\n        self.hue = hue\n        self.melted_data = None\n        self.summary_data = None\n        self.label = None\n\n        # melt the data given in the init method\n        self.melted_data = self.melt_data(self.df, stat_to_summarize=self.stat_to_sum, var_to_label=self.var_to_label)\n        convert_basename_to_relative_paths(df=self.melted_data)\n        self.set_palette()\n        self._set_label(self.stat_to_sum)\n\n    def _setup_figure(self):\n        \"\"\"Setup Matplotlib figure and axes.\"\"\"\n        fig, ax = plt.subplots(1, 1, figsize=self.figsize)\n        return fig, ax\n\n    def _outfile(self, plot_suffix: str) -&gt; str:\n        \"\"\"Generate the output file name with the appropriate suffix.\n\n        Parameters\n        ----------\n        plot_suffix: str\n            The suffix to append to the output file.\n\n        Returns\n        -------\n        str:\n            Concanenated string of the outfile and plot_suffix.\n        \"\"\"\n        return f\"{self.stat_to_sum}_{plot_suffix}\"\n\n    def sns_plot(self) -&gt; Optional[Tuple[plt.Figure, plt.Axes]]:\n        \"\"\"Plot the distribution of one or more statistics as either histogram, kernel density estimates or both. Uses\n        base Seaborn.\n\n        Returns\n        -------\n        Optional[Union[Tuple[plt.Figure, plt.Axes], None]]\n            Tuple of Matplotlib figure and axes if plotting is successful, None otherwise.\n        \"\"\"\n\n        # Note: Plotting KDEs with Seaborn is not possible if all values are the same.\n        # This is because the KDE is calculated using a Gaussian kernel and if all values\n        # are the same, the standard deviation is 0 which results in a ZeroDivisionError with\n        # is caught internally but then raises a numpy linalg error.\n        # The try/catch is there to catch this error and skip plotting KDEs if all values are the same.\n\n        fig, ax = self._setup_figure()\n\n        # If histogram is requested but KDE is not, plot histogram\n        if self.hist and not self.kde:\n            outfile = self._outfile(\"hist\")\n            sns.histplot(data=self.melted_data, x=\"value\", bins=self.bins, stat=self.stat, hue=self.hue)\n        if self.kde and not self.hist:\n            outfile = self._outfile(\"kde\")\n            try:\n                sns.kdeplot(data=self.melted_data, x=\"value\", hue=self.hue)\n            except np.linalg.LinAlgError:\n                LOGGER.info(\n                    \"[plotting] KDE plot error: Numpy linalg error encountered. This is a result of all values \\\nfor KDE plot being the same. KDE plots cannot be made as there is no variance, skipping.\"\n                )\n                return None\n        if self.hist and self.kde:\n            outfile = self._outfile(\"hist_kde\")\n            try:\n                sns.histplot(\n                    data=self.melted_data,\n                    x=\"value\",\n                    bins=self.bins,\n                    stat=self.stat,\n                    hue=self.hue,\n                    kde=True,\n                    kde_kws={\"cut\": self.cut},\n                )\n            except np.linalg.LinAlgError:\n                LOGGER.info(\n                    \"[plotting] KDE plot error: Numpy linalg error encountered. This is a result of all values \\\nfor KDE plot being the same. KDE plots cannot be made as there is no variance, skipping.\"\n                )\n                return None\n\n        plt.ticklabel_format(axis=\"both\", style=\"sci\", scilimits=(-3, 3))\n        plt.title(self.label)\n        self.set_xlim()\n        self.save_plot(outfile)\n\n        return fig, ax\n\n    def sns_violinplot(self) -&gt; None:\n        \"\"\"Violin plot of data.\"\"\"\n        fig, ax = self._setup_figure()\n        # Determine whether to draw a legend\n        legend = \"full\" if len(self.melted_data[self.hue].unique()) &gt; 1 else False\n        sns.violinplot(\n            data=self.melted_data,\n            x=self.hue,\n            y=\"value\",\n            hue=self.hue,\n            alpha=self.alpha,\n            legend=legend,\n        )\n        plt.title(self.label)\n        plt.xlabel(\"directory\")\n        plt.ylabel(self.label)\n        outfile = self._outfile(\"violin\")\n        self.save_plot(outfile)\n        return fig, ax\n\n    # def sns_jointplot(self, var1: str, var2: str) -&gt; None:\n    #     \"\"\"Joint distribution of two variables.\"\"\"\n    #     fig, ax = self._setup_figure()\n    #     sns.jointplot(data=self.df, x=var1, y=var2, kind=\"reg\")\n    #     outfile = f\"{'_'.join(self.stats_to_sum.keys())}_jointplot\"\n    #     # outfile = self._outfile(\"jointplot\")\n    #     self.save_plot(outfile)\n    #     return fig, ax\n\n    @staticmethod\n    def melt_data(df: pd.DataFrame, stat_to_summarize: str, var_to_label: dict) -&gt; pd.DataFrame:\n        \"\"\"Melt a dataframe into long format for plotting with Seaborn.\"\"\"\n        melted_data = pd.melt(df.reset_index(), id_vars=[\"molecule_number\", \"basename\"], value_vars=stat_to_summarize)\n        melted_data[\"variable\"] = melted_data[\"variable\"].map(var_to_label)\n        LOGGER.info(\"[plotting] Data has been melted to long format for plotting.\")\n\n        return melted_data\n\n    def set_xlim(self, percent: float = 0.1) -&gt; None:\n        \"\"\"Set the range of the x-axis.\n\n        Parameters\n        ----------\n        percent: float\n            Percentage of the observed range by which to extend the x-axis. Only used if supplied range is outside the\n        observed values.\n        \"\"\"\n        range_percent = percent * (self.melted_data[\"value\"].max() - self.melted_data[\"value\"].min())\n        range_min = self.melted_data[\"value\"].min()\n        range_max = self.melted_data[\"value\"].max()\n        plt.xlim(range_min - range_percent, range_max + range_percent)\n        LOGGER.info(f\"[plotting] Setting x-axis range       : {range_min} - {range_max}\")\n\n    def set_palette(self):\n        \"\"\"Set the color palette.\"\"\"\n        sns.set_palette(self.palette)\n        LOGGER.info(f\"[plotting] Seaborn color palette : {self.palette}\")\n\n    def save_plot(self, outfile: Path) -&gt; None:\n        \"\"\"Save the plot to the output_dir\n\n        Parameters\n        ----------\n        outfile: str\n            Output file name to save figure to.\n        \"\"\"\n        plt.savefig(self.output_dir / f\"{outfile}.{self.savefig_format}\")\n        LOGGER.info(\n            f\"[plotting] Plotted {self.stat_to_sum} to : \"\n            f\"{str(self.output_dir / f'{outfile}.{self.savefig_format}')}\"\n        )\n\n    def _set_label(self, var: str):\n        \"\"\"Get the label based on the column name(s).\n\n        Parameters\n        ----------\n        var: str\n            The variable for which a label is required.\n        \"\"\"\n        self.label = self.var_to_label[var]\n        LOGGER.debug(f\"[plotting] self.label     : {self.label}\")\n</code></pre>"},{"location":"api/plotting/#topostats.plotting.TopoSum.__init__","title":"<code>__init__(df: pd.DataFrame = None, base_dir: Union[str, Path] = None, csv_file: Union[str, Path] = None, stat_to_sum: str = None, molecule_id: str = 'molecule_number', image_id: str = 'image', hist: bool = True, stat: str = 'count', bins: int = 12, kde: bool = True, cut: float = 20, figsize: tuple = (16, 9), alpha: float = 0.5, palette: str = 'deep', savefig_format: str = 'png', output_dir: Union[str, Path] = '.', var_to_label: dict = None, hue: str = 'basename') -&gt; None</code>","text":"<p>Initialise the class.</p>"},{"location":"api/plotting/#topostats.plotting.TopoSum.__init__--parameters","title":"Parameters","text":"<p>df: Union[pd.DataFrame]     Pandas data frame of data to be summarised. base_dir: Union[str, Path]     Base directory from which all paths are relative to. csv_file: Union[str, Path]     CSV file of data to be summarised. stat_to_sum: str     Variable to summarise. molecule_id: str     Variable that uniquely identifies molecules. image_id: str     Variable that uniquely identifies images. hist: bool     Whether to plot histograms. stat: str     Statistic to plot on histogram 'count' (default), 'freq'. bins: int     Number of bins to plot. kde: bool     Whether to include a Kernel Density Estimate. cut: float = 20,     Cut point for KDE. figsize: tuple     Figure dimensions. alpha: float     Opacity to use in plots. palette: str = \"deep\"     Seaborn colour plot to use. file_ext: str     File type to save plots as 'png' (default), 'pdf', 'svg'. output_dir: Union[str, Path]     Location to save plots to. var_to_label: dict     Variable to label dictionary for automatically adding titles to plots. hue: str     Dataframe column to group plots by.</p>"},{"location":"api/plotting/#topostats.plotting.TopoSum.__init__--returns","title":"Returns","text":"Source code in <code>topostats\\plotting.py</code> <pre><code>def __init__(\n    self,\n    df: pd.DataFrame = None,\n    base_dir: Union[str, Path] = None,\n    csv_file: Union[str, Path] = None,\n    stat_to_sum: str = None,\n    molecule_id: str = \"molecule_number\",\n    image_id: str = \"image\",\n    hist: bool = True,\n    stat: str = \"count\",\n    bins: int = 12,\n    kde: bool = True,\n    cut: float = 20,\n    figsize: tuple = (16, 9),\n    alpha: float = 0.5,\n    palette: str = \"deep\",\n    savefig_format: str = \"png\",\n    output_dir: Union[str, Path] = \".\",\n    var_to_label: dict = None,\n    hue: str = \"basename\",\n) -&gt; None:\n    \"\"\"Initialise the class.\n\n    Parameters\n    ==========\n    df: Union[pd.DataFrame]\n        Pandas data frame of data to be summarised.\n    base_dir: Union[str, Path]\n        Base directory from which all paths are relative to.\n    csv_file: Union[str, Path]\n        CSV file of data to be summarised.\n    stat_to_sum: str\n        Variable to summarise.\n    molecule_id: str\n        Variable that uniquely identifies molecules.\n    image_id: str\n        Variable that uniquely identifies images.\n    hist: bool\n        Whether to plot histograms.\n    stat: str\n        Statistic to plot on histogram 'count' (default), 'freq'.\n    bins: int\n        Number of bins to plot.\n    kde: bool\n        Whether to include a Kernel Density Estimate.\n    cut: float = 20,\n        Cut point for KDE.\n    figsize: tuple\n        Figure dimensions.\n    alpha: float\n        Opacity to use in plots.\n    palette: str = \"deep\"\n        Seaborn colour plot to use.\n    file_ext: str\n        File type to save plots as 'png' (default), 'pdf', 'svg'.\n    output_dir: Union[str, Path]\n        Location to save plots to.\n    var_to_label: dict\n        Variable to label dictionary for automatically adding titles to plots.\n    hue: str\n        Dataframe column to group plots by.\n\n    Returns\n    =======\n    \"\"\"\n    self.df = df if df is not None else pd.read_csv(csv_file)\n    self.base_dir = base_dir\n    self.stat_to_sum = stat_to_sum\n    self.molecule_id = molecule_id\n    self.image_id = image_id\n    self.hist = hist\n    self.bins = bins\n    self.stat = stat\n    self.kde = kde\n    self.cut = cut\n    self.figsize = figsize\n    self.alpha = alpha\n    self.palette = palette\n    self.savefig_format = savefig_format\n    self.output_dir = Path(output_dir)\n    self.output_dir.mkdir(parents=True, exist_ok=True)\n    self.var_to_label = var_to_label\n    self.hue = hue\n    self.melted_data = None\n    self.summary_data = None\n    self.label = None\n\n    # melt the data given in the init method\n    self.melted_data = self.melt_data(self.df, stat_to_summarize=self.stat_to_sum, var_to_label=self.var_to_label)\n    convert_basename_to_relative_paths(df=self.melted_data)\n    self.set_palette()\n    self._set_label(self.stat_to_sum)\n</code></pre>"},{"location":"api/plotting/#topostats.plotting.TopoSum.melt_data","title":"<code>melt_data(df: pd.DataFrame, stat_to_summarize: str, var_to_label: dict) -&gt; pd.DataFrame</code>  <code>staticmethod</code>","text":"<p>Melt a dataframe into long format for plotting with Seaborn.</p> Source code in <code>topostats\\plotting.py</code> <pre><code>@staticmethod\ndef melt_data(df: pd.DataFrame, stat_to_summarize: str, var_to_label: dict) -&gt; pd.DataFrame:\n    \"\"\"Melt a dataframe into long format for plotting with Seaborn.\"\"\"\n    melted_data = pd.melt(df.reset_index(), id_vars=[\"molecule_number\", \"basename\"], value_vars=stat_to_summarize)\n    melted_data[\"variable\"] = melted_data[\"variable\"].map(var_to_label)\n    LOGGER.info(\"[plotting] Data has been melted to long format for plotting.\")\n\n    return melted_data\n</code></pre>"},{"location":"api/plotting/#topostats.plotting.TopoSum.save_plot","title":"<code>save_plot(outfile: Path) -&gt; None</code>","text":"<p>Save the plot to the output_dir</p> <p>Parameters:</p> Name Type Description Default <code>outfile</code> <code>Path</code> <p>Output file name to save figure to.</p> required Source code in <code>topostats\\plotting.py</code> <pre><code>def save_plot(self, outfile: Path) -&gt; None:\n    \"\"\"Save the plot to the output_dir\n\n    Parameters\n    ----------\n    outfile: str\n        Output file name to save figure to.\n    \"\"\"\n    plt.savefig(self.output_dir / f\"{outfile}.{self.savefig_format}\")\n    LOGGER.info(\n        f\"[plotting] Plotted {self.stat_to_sum} to : \"\n        f\"{str(self.output_dir / f'{outfile}.{self.savefig_format}')}\"\n    )\n</code></pre>"},{"location":"api/plotting/#topostats.plotting.TopoSum.set_palette","title":"<code>set_palette()</code>","text":"<p>Set the color palette.</p> Source code in <code>topostats\\plotting.py</code> <pre><code>def set_palette(self):\n    \"\"\"Set the color palette.\"\"\"\n    sns.set_palette(self.palette)\n    LOGGER.info(f\"[plotting] Seaborn color palette : {self.palette}\")\n</code></pre>"},{"location":"api/plotting/#topostats.plotting.TopoSum.set_xlim","title":"<code>set_xlim(percent: float = 0.1) -&gt; None</code>","text":"<p>Set the range of the x-axis.</p> <p>Parameters:</p> Name Type Description Default <code>percent</code> <code>float</code> <p>Percentage of the observed range by which to extend the x-axis. Only used if supplied range is outside the</p> <code>0.1</code> <code>observed</code> required Source code in <code>topostats\\plotting.py</code> <pre><code>def set_xlim(self, percent: float = 0.1) -&gt; None:\n    \"\"\"Set the range of the x-axis.\n\n    Parameters\n    ----------\n    percent: float\n        Percentage of the observed range by which to extend the x-axis. Only used if supplied range is outside the\n    observed values.\n    \"\"\"\n    range_percent = percent * (self.melted_data[\"value\"].max() - self.melted_data[\"value\"].min())\n    range_min = self.melted_data[\"value\"].min()\n    range_max = self.melted_data[\"value\"].max()\n    plt.xlim(range_min - range_percent, range_max + range_percent)\n    LOGGER.info(f\"[plotting] Setting x-axis range       : {range_min} - {range_max}\")\n</code></pre>"},{"location":"api/plotting/#topostats.plotting.TopoSum.sns_plot","title":"<code>sns_plot() -&gt; Optional[Tuple[plt.Figure, plt.Axes]]</code>","text":"<p>Plot the distribution of one or more statistics as either histogram, kernel density estimates or both. Uses base Seaborn.</p> <p>Returns:</p> Type Description <code>Optional[Union[Tuple[Figure, Axes], None]]</code> <p>Tuple of Matplotlib figure and axes if plotting is successful, None otherwise.</p> Source code in <code>topostats\\plotting.py</code> <pre><code>    def sns_plot(self) -&gt; Optional[Tuple[plt.Figure, plt.Axes]]:\n        \"\"\"Plot the distribution of one or more statistics as either histogram, kernel density estimates or both. Uses\n        base Seaborn.\n\n        Returns\n        -------\n        Optional[Union[Tuple[plt.Figure, plt.Axes], None]]\n            Tuple of Matplotlib figure and axes if plotting is successful, None otherwise.\n        \"\"\"\n\n        # Note: Plotting KDEs with Seaborn is not possible if all values are the same.\n        # This is because the KDE is calculated using a Gaussian kernel and if all values\n        # are the same, the standard deviation is 0 which results in a ZeroDivisionError with\n        # is caught internally but then raises a numpy linalg error.\n        # The try/catch is there to catch this error and skip plotting KDEs if all values are the same.\n\n        fig, ax = self._setup_figure()\n\n        # If histogram is requested but KDE is not, plot histogram\n        if self.hist and not self.kde:\n            outfile = self._outfile(\"hist\")\n            sns.histplot(data=self.melted_data, x=\"value\", bins=self.bins, stat=self.stat, hue=self.hue)\n        if self.kde and not self.hist:\n            outfile = self._outfile(\"kde\")\n            try:\n                sns.kdeplot(data=self.melted_data, x=\"value\", hue=self.hue)\n            except np.linalg.LinAlgError:\n                LOGGER.info(\n                    \"[plotting] KDE plot error: Numpy linalg error encountered. This is a result of all values \\\nfor KDE plot being the same. KDE plots cannot be made as there is no variance, skipping.\"\n                )\n                return None\n        if self.hist and self.kde:\n            outfile = self._outfile(\"hist_kde\")\n            try:\n                sns.histplot(\n                    data=self.melted_data,\n                    x=\"value\",\n                    bins=self.bins,\n                    stat=self.stat,\n                    hue=self.hue,\n                    kde=True,\n                    kde_kws={\"cut\": self.cut},\n                )\n            except np.linalg.LinAlgError:\n                LOGGER.info(\n                    \"[plotting] KDE plot error: Numpy linalg error encountered. This is a result of all values \\\nfor KDE plot being the same. KDE plots cannot be made as there is no variance, skipping.\"\n                )\n                return None\n\n        plt.ticklabel_format(axis=\"both\", style=\"sci\", scilimits=(-3, 3))\n        plt.title(self.label)\n        self.set_xlim()\n        self.save_plot(outfile)\n\n        return fig, ax\n</code></pre>"},{"location":"api/plotting/#topostats.plotting.TopoSum.sns_violinplot","title":"<code>sns_violinplot() -&gt; None</code>","text":"<p>Violin plot of data.</p> Source code in <code>topostats\\plotting.py</code> <pre><code>def sns_violinplot(self) -&gt; None:\n    \"\"\"Violin plot of data.\"\"\"\n    fig, ax = self._setup_figure()\n    # Determine whether to draw a legend\n    legend = \"full\" if len(self.melted_data[self.hue].unique()) &gt; 1 else False\n    sns.violinplot(\n        data=self.melted_data,\n        x=self.hue,\n        y=\"value\",\n        hue=self.hue,\n        alpha=self.alpha,\n        legend=legend,\n    )\n    plt.title(self.label)\n    plt.xlabel(\"directory\")\n    plt.ylabel(self.label)\n    outfile = self._outfile(\"violin\")\n    self.save_plot(outfile)\n    return fig, ax\n</code></pre>"},{"location":"api/plotting/#topostats.plotting.run_toposum","title":"<code>run_toposum(args=None)</code>","text":"<p>Run Plotting</p> Source code in <code>topostats\\plotting.py</code> <pre><code>def run_toposum(args=None):\n    \"\"\"Run Plotting\"\"\"\n\n    if args.config_file is not None:\n        config = read_yaml(args.config_file)\n        LOGGER.info(f\"[plotting] Configuration file loaded from : {args.config_file}\")\n    else:\n        summary_yaml = pkg_resources.open_text(__package__, \"summary_config.yaml\")\n        config = yaml.safe_load(summary_yaml.read())\n        LOGGER.info(\"[plotting] Default configuration file loaded.\")\n    config = update_config(config, args)\n    if args.var_to_label is not None:\n        config[\"var_to_label\"] = read_yaml(args.var_to_label)\n        LOGGER.info(\"[plotting] Variable to labels mapping loaded from : {args.var_to_label}\")\n    else:\n        plotting_yaml = pkg_resources.open_text(__package__, \"var_to_label.yaml\")\n        config[\"var_to_label\"] = yaml.safe_load(plotting_yaml.read())\n        LOGGER.info(\"[plotting] Default variable to labels mapping loaded.\")\n    if args.csv_file is not None:\n        config[\"csv_file\"] = args.csv_file\n\n    # Write sample configuration if asked to do so and exit\n    if args.create_config_file:\n        write_yaml(\n            config,\n            output_dir=\"./\",\n            config_file=args.create_config_file,\n            header_message=\"Sample configuration file auto-generated\",\n        )\n        LOGGER.info(f\"A sample configuration has been written to : ./{args.create_config_file}\")\n        LOGGER.info(\n            \"Please refer to the documentation on how to use the configuration file : \\n\\n\"\n            \"https://afm-spm.github.io/TopoStats/usage.html#configuring-topostats\\n\"\n            \"https://afm-spm.github.io/TopoStats/configuration.html\"\n        )\n        sys.exit()\n    if args.create_label_file:\n        write_yaml(\n            config[\"var_to_label\"],\n            output_dir=\"./\",\n            config_file=args.create_label_file,\n            header_message=\"Sample label file auto-generated\",\n        )\n        LOGGER.info(f\"A sample label file has been written to : ./{args.create_label_file}\")\n        LOGGER.info(\n            \"Please refer to the documentation on how to use the configuration file : \\n\\n\"\n            \"https://afm-spm.github.io/TopoStats/usage.html#configuring-topostats\\n\"\n            \"https://afm-spm.github.io/TopoStats/configuration.html\"\n        )\n        sys.exit()\n\n    # Plot statistics\n    toposum(config)\n</code></pre>"},{"location":"api/plotting/#topostats.plotting.toposum","title":"<code>toposum(config: dict) -&gt; Dict</code>","text":"<p>Process plotting and summarisation of data.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Dictionary of summarisation options.</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>Dictionary of nested dictionaries. Each variable has its own dictionary with keys 'dist' and 'violin' which contain distribution like plots and violin plots respectively (if the later are required). Each 'dist' and 'violin' is itself a dictionary with two elements 'figures' and 'axes' which correspond to MatplotLib 'fig' and 'ax' for that plot.</p> Source code in <code>topostats\\plotting.py</code> <pre><code>def toposum(config: dict) -&gt; Dict:\n    \"\"\"Process plotting and summarisation of data.\n\n    Parameters\n    ----------\n    config: dict\n        Dictionary of summarisation options.\n\n    Returns\n    -------\n    Dict\n        Dictionary of nested dictionaries. Each variable has its own dictionary with keys 'dist' and 'violin' which\n        contain distribution like plots and violin plots respectively (if the later are required). Each 'dist' and\n       'violin' is itself a dictionary with two elements 'figures' and 'axes' which correspond to MatplotLib 'fig' and\n       'ax' for that plot.\n    \"\"\"\n    if \"df\" not in config.keys():\n        config[\"df\"] = pd.read_csv(config[\"csv_file\"])\n    if config[\"df\"].isna().values.all():\n        LOGGER.info(\"[plotting] No statistics in DataFrame. Exiting...\")\n        return None\n    violin = config.pop(\"violin\")\n    all_stats_to_sum = config.pop(\"stats_to_sum\")\n    pickle_plots = config.pop(\"pickle_plots\")\n    figures = defaultdict()\n\n    # Plot each variable on its own graph\n    for var in all_stats_to_sum:\n        if var in config[\"df\"].columns:\n            topo_sum = TopoSum(stat_to_sum=var, **config)\n            figures[var] = {\"dist\": None, \"violin\": None}\n            figures[var][\"dist\"] = defaultdict()\n            result_option: Optional[Tuple] = topo_sum.sns_plot()\n            # Handle the Optional[Tuple]\n            if result_option is not None:\n                figures[var][\"dist\"][\"figure\"], figures[var][\"dist\"][\"axes\"] = result_option\n\n            if violin:\n                figures[var][\"violin\"] = defaultdict()\n                (\n                    figures[var][\"violin\"][\"figure\"],\n                    figures[var][\"violin\"][\"axes\"],\n                ) = topo_sum.sns_violinplot()\n        else:\n            LOGGER.info(f\"[plotting] Statistic is not in dataframe : {var}\")\n    if pickle_plots:\n        outfile = Path(config[\"output_dir\"]) / \"distribution_plots.pkl\"\n        save_pkl(outfile=outfile, to_pkl=figures)\n        LOGGER.info(f\"[plotting] Images pickled to : {outfile}\")\n\n    return figures\n</code></pre>"},{"location":"api/plottingfuncs/","title":"Plottingfuncs Modules","text":"<p>Plotting data.</p>"},{"location":"api/plottingfuncs/#topostats.plottingfuncs.Images","title":"<code>Images</code>","text":"<p>Plots image arrays.</p> Source code in <code>topostats\\plottingfuncs.py</code> <pre><code>class Images:\n    \"\"\"Plots image arrays.\"\"\"\n\n    def __init__(\n        self,\n        data: np.array,\n        output_dir: str | Path,\n        filename: str,\n        style: str | Path = None,\n        pixel_to_nm_scaling: float = 1.0,\n        masked_array: np.array = None,\n        title: str = None,\n        image_type: str = \"non-binary\",\n        image_set: str = \"core\",\n        core_set: bool = False,\n        pixel_interpolation: str | None = None,\n        cmap: str | None = None,\n        mask_cmap: str = \"jet_r\",\n        region_properties: dict = None,\n        zrange: list = None,\n        colorbar: bool = True,\n        axes: bool = True,\n        num_ticks: list[int | None, int | None] = (None, None),\n        save: bool = True,\n        savefig_format: str | None = None,\n        histogram_log_axis: bool = True,\n        histogram_bins: int | None = None,\n        savefig_dpi: str | float | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialise the class.\n\n        Parameters\n        ----------\n        data : np.array\n            Numpy array to plot.\n        output_dir : Union[str, Path]\n            Output directory to save the file to.\n        filename : Union[str, Path]\n            Filename to save image as.\n        style: dict\n            Filename of matploglibrc Params.\n        pixel_to_nm_scaling : float\n            The scaling factor showing the real length of 1 pixel, in nm.\n        masked_array : np.ndarray\n            Optional mask array to overlay onto an image.\n        title : str\n            Title for plot.\n        image_type : str\n            The image data type - binary or non-binary.\n        image_set : str\n            The set of images to process - core or all.\n        core_set : bool\n            Flag to identify image as part of the core image set or not.\n        pixel_interpolation: Union[str, None]\n            Interpolation to use (default: None).\n        cmap : str\n            Colour map to use (default 'nanoscope', 'afmhot' also available).\n        mask_cmap : str\n            Colour map to use for the secondary (masked) data (default 'jet_r', 'blu' proivides more contrast).\n        region_properties: dict\n            Dictionary of region properties, adds bounding boxes if specified.\n        zrange : list\n            Lower and upper bound to clip core images to.\n        colorbar: bool\n            Optionally add a colorbar to plots, default is False.\n        axes: bool\n            Optionally add/remove axes from the image.\n        num_ticks: list[int, int]\n            The number of x and y ticks to display on the image.\n        save: bool\n            Whether to save the image.\n        save_format: str\n            Format to save the image as.\n        histogram_log_axis: bool\n            Optionally use a logarithmic y axis for the histogram plots.\n        histogram_bin: int\n            Number of bins for histograms to use.\n        savefig_dpi: Union[str, float]\n            The resolution of the saved plot (default 'figure').\n        \"\"\"\n        if style is None:\n            style = \"topostats.mplstyle\"\n        load_mplstyle(style)\n        if zrange is None:\n            zrange = [None, None]\n        self.data = data\n        self.output_dir = Path(output_dir)\n        self.filename = filename\n        self.pixel_to_nm_scaling = pixel_to_nm_scaling\n        self.masked_array = masked_array\n        self.title = title\n        self.image_type = image_type\n        self.image_set = image_set\n        self.core_set = core_set\n        self.interpolation = mpl.rcParams[\"image.interpolation\"] if pixel_interpolation is None else pixel_interpolation\n        cmap = mpl.rcParams[\"image.cmap\"] if cmap is None else cmap\n        self.cmap = Colormap(cmap).get_cmap()\n        self.mask_cmap = Colormap(mask_cmap).get_cmap()\n        self.region_properties = region_properties\n        self.zrange = zrange\n        self.colorbar = colorbar\n        self.axes = axes\n        self.num_ticks = num_ticks\n        self.save = save\n        self.savefig_format = mpl.rcParams[\"savefig.format\"] if savefig_format is None else savefig_format\n        self.histogram_log_axis = histogram_log_axis\n        self.histogram_bins = mpl.rcParams[\"hist.bins\"] if histogram_bins is None else histogram_bins\n        self.savefig_dpi = mpl.rcParams[\"savefig.dpi\"] if savefig_dpi is None else savefig_dpi\n\n    def plot_histogram_and_save(self):\n        \"\"\"\n        Plot and save a histogram of the height map.\n\n        Returns\n        -------\n        fig: plt.figure.Figure\n            Matplotlib.pyplot figure object\n        ax: plt.axes._subplots.AxesSubplot\n            Matplotlib.pyplot axes object\n        \"\"\"\n        if self.image_set == \"all\":\n            fig, ax = plt.subplots(1, 1)\n\n            ax.hist(self.data.flatten().astype(float), bins=self.histogram_bins, log=self.histogram_log_axis)\n            ax.set_xlabel(\"pixel height\")\n            if self.histogram_log_axis:\n                ax.set_ylabel(\"frequency in image (log)\")\n            else:\n                ax.set_ylabel(\"frequency in image\")\n            plt.title(self.title)\n            plt.savefig(\n                (self.output_dir / f\"{self.filename}_histogram.{self.savefig_format}\"),\n                bbox_inches=\"tight\",\n                pad_inches=0.5,\n                dpi=self.savefig_dpi,\n            )\n            plt.close()\n\n            return fig, ax\n        return None\n\n    def plot_and_save(self):\n        \"\"\"\n        Plot and save the images with savefig or imsave depending on config file parameters.\n\n        Returns\n        -------\n        fig: plt.figure.Figure\n            Matplotlib.pyplot figure object\n        ax: plt.axes._subplots.AxesSubplot\n            Matplotlib.pyplot axes object\n        \"\"\"\n        fig, ax = None, None\n        if self.save:\n            if self.image_set == \"all\" or self.core_set:\n                if self.axes or self.colorbar:\n                    fig, ax = self.save_figure()\n                else:\n                    if isinstance(self.masked_array, np.ndarray) or self.region_properties:\n                        fig, ax = self.save_figure()\n                    else:\n                        self.save_array_figure()\n        LOGGER.info(\n            f\"[{self.filename}] : Image saved to : {str(self.output_dir / self.filename)}.{self.savefig_format}\\\n | DPI: {self.savefig_dpi}\"\n        )\n        return fig, ax\n\n    def save_figure(self):\n        \"\"\"Save figures as plt.savefig objects.\n\n        Returns\n        -------\n        fig: plt.figure.Figure\n            Matplotlib.pyplot figure object\n        ax: plt.axes._subplots.AxesSubplot\n            Matplotlib.pyplot axes object\n        \"\"\"\n        fig, ax = plt.subplots(1, 1)\n        shape = self.data.shape\n        if isinstance(self.data, np.ndarray):\n            im = ax.imshow(\n                self.data,\n                extent=(0, shape[1] * self.pixel_to_nm_scaling, 0, shape[0] * self.pixel_to_nm_scaling),\n                interpolation=self.interpolation,\n                cmap=self.cmap,\n                vmin=self.zrange[0],\n                vmax=self.zrange[1],\n            )\n            if isinstance(self.masked_array, np.ndarray):\n                self.masked_array[self.masked_array != 0] = 1\n                # If the image is too large for singles to be resolved in the mask, then dilate the mask proportionally\n                # to image size to enable clear viewing.\n                if np.max(self.masked_array.shape) &gt; 500:\n                    dilation_strength = int(np.max(self.masked_array.shape) / 256)\n                    self.masked_array = dilate_binary_image(\n                        binary_image=self.masked_array, dilation_iterations=dilation_strength\n                    )\n                mask = np.ma.masked_where(self.masked_array == 0, self.masked_array)\n                ax.imshow(\n                    mask,\n                    cmap=self.mask_cmap,\n                    extent=(\n                        0,\n                        shape[1] * self.pixel_to_nm_scaling,\n                        0,\n                        shape[0] * self.pixel_to_nm_scaling,\n                    ),\n                    interpolation=self.interpolation,\n                    alpha=0.7,\n                )\n                patch = [Patch(color=self.mask_cmap(1, 0.7), label=\"Mask\")]\n                plt.legend(handles=patch, loc=\"upper right\", bbox_to_anchor=(1.02, 1.09))\n\n            plt.title(self.title)\n            plt.xlabel(\"Nanometres\")\n            plt.ylabel(\"Nanometres\")\n            set_n_ticks(ax, self.num_ticks)\n            plt.axis(self.axes)\n            if self.colorbar and self.image_type == \"non-binary\":\n                divider = make_axes_locatable(ax)\n                cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n                plt.colorbar(im, cax=cax, label=\"Height (Nanometres)\")\n            if self.region_properties:\n                fig, ax = add_bounding_boxes_to_plot(fig, ax, shape, self.region_properties, self.pixel_to_nm_scaling)\n            if not self.axes and not self.colorbar:\n                plt.title(\"\")\n                fig.frameon = False\n                plt.savefig(\n                    (self.output_dir / f\"{self.filename}.{self.savefig_format}\"),\n                    bbox_inches=\"tight\",\n                    pad_inches=0,\n                    dpi=self.savefig_dpi,\n                )\n            else:\n                plt.savefig((self.output_dir / f\"{self.filename}.{self.savefig_format}\"), dpi=self.savefig_dpi)\n        else:\n            plt.xlabel(\"Nanometres\")\n            plt.ylabel(\"Nanometres\")\n            self.data.show(\n                ax=ax,\n                extent=(0, shape[1] * self.pixel_to_nm_scaling, 0, shape[0] * self.pixel_to_nm_scaling),\n                interpolation=self.interpolation,\n                cmap=self.cmap,\n            )\n        plt.close()\n        return fig, ax\n\n    def save_array_figure(self) -&gt; None:\n        \"\"\"Save the image array as an image using plt.imsave().\"\"\"\n        plt.imsave(\n            (self.output_dir / f\"{self.filename}.{self.savefig_format}\"),\n            self.data,\n            cmap=self.cmap,\n            vmin=self.zrange[0],\n            vmax=self.zrange[1],\n            format=self.savefig_format,\n        )\n        plt.close()\n</code></pre>"},{"location":"api/plottingfuncs/#topostats.plottingfuncs.Images.__init__","title":"<code>__init__(data: np.array, output_dir: str | Path, filename: str, style: str | Path = None, pixel_to_nm_scaling: float = 1.0, masked_array: np.array = None, title: str = None, image_type: str = 'non-binary', image_set: str = 'core', core_set: bool = False, pixel_interpolation: str | None = None, cmap: str | None = None, mask_cmap: str = 'jet_r', region_properties: dict = None, zrange: list = None, colorbar: bool = True, axes: bool = True, num_ticks: list[int | None, int | None] = (None, None), save: bool = True, savefig_format: str | None = None, histogram_log_axis: bool = True, histogram_bins: int | None = None, savefig_dpi: str | float | None = None) -&gt; None</code>","text":"<p>Initialise the class.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array</code> <p>Numpy array to plot.</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Output directory to save the file to.</p> required <code>filename</code> <code>Union[str, Path]</code> <p>Filename to save image as.</p> required <code>style</code> <code>str | Path</code> <p>Filename of matploglibrc Params.</p> <code>None</code> <code>pixel_to_nm_scaling</code> <code>float</code> <p>The scaling factor showing the real length of 1 pixel, in nm.</p> <code>1.0</code> <code>masked_array</code> <code>ndarray</code> <p>Optional mask array to overlay onto an image.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title for plot.</p> <code>None</code> <code>image_type</code> <code>str</code> <p>The image data type - binary or non-binary.</p> <code>'non-binary'</code> <code>image_set</code> <code>str</code> <p>The set of images to process - core or all.</p> <code>'core'</code> <code>core_set</code> <code>bool</code> <p>Flag to identify image as part of the core image set or not.</p> <code>False</code> <code>pixel_interpolation</code> <code>str | None</code> <p>Interpolation to use (default: None).</p> <code>None</code> <code>cmap</code> <code>str</code> <p>Colour map to use (default 'nanoscope', 'afmhot' also available).</p> <code>None</code> <code>mask_cmap</code> <code>str</code> <p>Colour map to use for the secondary (masked) data (default 'jet_r', 'blu' proivides more contrast).</p> <code>'jet_r'</code> <code>region_properties</code> <code>dict</code> <p>Dictionary of region properties, adds bounding boxes if specified.</p> <code>None</code> <code>zrange</code> <code>list</code> <p>Lower and upper bound to clip core images to.</p> <code>None</code> <code>colorbar</code> <code>bool</code> <p>Optionally add a colorbar to plots, default is False.</p> <code>True</code> <code>axes</code> <code>bool</code> <p>Optionally add/remove axes from the image.</p> <code>True</code> <code>num_ticks</code> <code>list[int | None, int | None]</code> <p>The number of x and y ticks to display on the image.</p> <code>(None, None)</code> <code>save</code> <code>bool</code> <p>Whether to save the image.</p> <code>True</code> <code>save_format</code> <p>Format to save the image as.</p> required <code>histogram_log_axis</code> <code>bool</code> <p>Optionally use a logarithmic y axis for the histogram plots.</p> <code>True</code> <code>histogram_bin</code> <p>Number of bins for histograms to use.</p> required <code>savefig_dpi</code> <code>str | float | None</code> <p>The resolution of the saved plot (default 'figure').</p> <code>None</code> Source code in <code>topostats\\plottingfuncs.py</code> <pre><code>def __init__(\n    self,\n    data: np.array,\n    output_dir: str | Path,\n    filename: str,\n    style: str | Path = None,\n    pixel_to_nm_scaling: float = 1.0,\n    masked_array: np.array = None,\n    title: str = None,\n    image_type: str = \"non-binary\",\n    image_set: str = \"core\",\n    core_set: bool = False,\n    pixel_interpolation: str | None = None,\n    cmap: str | None = None,\n    mask_cmap: str = \"jet_r\",\n    region_properties: dict = None,\n    zrange: list = None,\n    colorbar: bool = True,\n    axes: bool = True,\n    num_ticks: list[int | None, int | None] = (None, None),\n    save: bool = True,\n    savefig_format: str | None = None,\n    histogram_log_axis: bool = True,\n    histogram_bins: int | None = None,\n    savefig_dpi: str | float | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialise the class.\n\n    Parameters\n    ----------\n    data : np.array\n        Numpy array to plot.\n    output_dir : Union[str, Path]\n        Output directory to save the file to.\n    filename : Union[str, Path]\n        Filename to save image as.\n    style: dict\n        Filename of matploglibrc Params.\n    pixel_to_nm_scaling : float\n        The scaling factor showing the real length of 1 pixel, in nm.\n    masked_array : np.ndarray\n        Optional mask array to overlay onto an image.\n    title : str\n        Title for plot.\n    image_type : str\n        The image data type - binary or non-binary.\n    image_set : str\n        The set of images to process - core or all.\n    core_set : bool\n        Flag to identify image as part of the core image set or not.\n    pixel_interpolation: Union[str, None]\n        Interpolation to use (default: None).\n    cmap : str\n        Colour map to use (default 'nanoscope', 'afmhot' also available).\n    mask_cmap : str\n        Colour map to use for the secondary (masked) data (default 'jet_r', 'blu' proivides more contrast).\n    region_properties: dict\n        Dictionary of region properties, adds bounding boxes if specified.\n    zrange : list\n        Lower and upper bound to clip core images to.\n    colorbar: bool\n        Optionally add a colorbar to plots, default is False.\n    axes: bool\n        Optionally add/remove axes from the image.\n    num_ticks: list[int, int]\n        The number of x and y ticks to display on the image.\n    save: bool\n        Whether to save the image.\n    save_format: str\n        Format to save the image as.\n    histogram_log_axis: bool\n        Optionally use a logarithmic y axis for the histogram plots.\n    histogram_bin: int\n        Number of bins for histograms to use.\n    savefig_dpi: Union[str, float]\n        The resolution of the saved plot (default 'figure').\n    \"\"\"\n    if style is None:\n        style = \"topostats.mplstyle\"\n    load_mplstyle(style)\n    if zrange is None:\n        zrange = [None, None]\n    self.data = data\n    self.output_dir = Path(output_dir)\n    self.filename = filename\n    self.pixel_to_nm_scaling = pixel_to_nm_scaling\n    self.masked_array = masked_array\n    self.title = title\n    self.image_type = image_type\n    self.image_set = image_set\n    self.core_set = core_set\n    self.interpolation = mpl.rcParams[\"image.interpolation\"] if pixel_interpolation is None else pixel_interpolation\n    cmap = mpl.rcParams[\"image.cmap\"] if cmap is None else cmap\n    self.cmap = Colormap(cmap).get_cmap()\n    self.mask_cmap = Colormap(mask_cmap).get_cmap()\n    self.region_properties = region_properties\n    self.zrange = zrange\n    self.colorbar = colorbar\n    self.axes = axes\n    self.num_ticks = num_ticks\n    self.save = save\n    self.savefig_format = mpl.rcParams[\"savefig.format\"] if savefig_format is None else savefig_format\n    self.histogram_log_axis = histogram_log_axis\n    self.histogram_bins = mpl.rcParams[\"hist.bins\"] if histogram_bins is None else histogram_bins\n    self.savefig_dpi = mpl.rcParams[\"savefig.dpi\"] if savefig_dpi is None else savefig_dpi\n</code></pre>"},{"location":"api/plottingfuncs/#topostats.plottingfuncs.Images.plot_and_save","title":"<code>plot_and_save()</code>","text":"<p>Plot and save the images with savefig or imsave depending on config file parameters.</p> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Matplotlib.pyplot figure object</p> <code>ax</code> <code>AxesSubplot</code> <p>Matplotlib.pyplot axes object</p> Source code in <code>topostats\\plottingfuncs.py</code> <pre><code>   def plot_and_save(self):\n       \"\"\"\n       Plot and save the images with savefig or imsave depending on config file parameters.\n\n       Returns\n       -------\n       fig: plt.figure.Figure\n           Matplotlib.pyplot figure object\n       ax: plt.axes._subplots.AxesSubplot\n           Matplotlib.pyplot axes object\n       \"\"\"\n       fig, ax = None, None\n       if self.save:\n           if self.image_set == \"all\" or self.core_set:\n               if self.axes or self.colorbar:\n                   fig, ax = self.save_figure()\n               else:\n                   if isinstance(self.masked_array, np.ndarray) or self.region_properties:\n                       fig, ax = self.save_figure()\n                   else:\n                       self.save_array_figure()\n       LOGGER.info(\n           f\"[{self.filename}] : Image saved to : {str(self.output_dir / self.filename)}.{self.savefig_format}\\\n| DPI: {self.savefig_dpi}\"\n       )\n       return fig, ax\n</code></pre>"},{"location":"api/plottingfuncs/#topostats.plottingfuncs.Images.plot_histogram_and_save","title":"<code>plot_histogram_and_save()</code>","text":"<p>Plot and save a histogram of the height map.</p> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Matplotlib.pyplot figure object</p> <code>ax</code> <code>AxesSubplot</code> <p>Matplotlib.pyplot axes object</p> Source code in <code>topostats\\plottingfuncs.py</code> <pre><code>def plot_histogram_and_save(self):\n    \"\"\"\n    Plot and save a histogram of the height map.\n\n    Returns\n    -------\n    fig: plt.figure.Figure\n        Matplotlib.pyplot figure object\n    ax: plt.axes._subplots.AxesSubplot\n        Matplotlib.pyplot axes object\n    \"\"\"\n    if self.image_set == \"all\":\n        fig, ax = plt.subplots(1, 1)\n\n        ax.hist(self.data.flatten().astype(float), bins=self.histogram_bins, log=self.histogram_log_axis)\n        ax.set_xlabel(\"pixel height\")\n        if self.histogram_log_axis:\n            ax.set_ylabel(\"frequency in image (log)\")\n        else:\n            ax.set_ylabel(\"frequency in image\")\n        plt.title(self.title)\n        plt.savefig(\n            (self.output_dir / f\"{self.filename}_histogram.{self.savefig_format}\"),\n            bbox_inches=\"tight\",\n            pad_inches=0.5,\n            dpi=self.savefig_dpi,\n        )\n        plt.close()\n\n        return fig, ax\n    return None\n</code></pre>"},{"location":"api/plottingfuncs/#topostats.plottingfuncs.Images.save_array_figure","title":"<code>save_array_figure() -&gt; None</code>","text":"<p>Save the image array as an image using plt.imsave().</p> Source code in <code>topostats\\plottingfuncs.py</code> <pre><code>def save_array_figure(self) -&gt; None:\n    \"\"\"Save the image array as an image using plt.imsave().\"\"\"\n    plt.imsave(\n        (self.output_dir / f\"{self.filename}.{self.savefig_format}\"),\n        self.data,\n        cmap=self.cmap,\n        vmin=self.zrange[0],\n        vmax=self.zrange[1],\n        format=self.savefig_format,\n    )\n    plt.close()\n</code></pre>"},{"location":"api/plottingfuncs/#topostats.plottingfuncs.Images.save_figure","title":"<code>save_figure()</code>","text":"<p>Save figures as plt.savefig objects.</p> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Matplotlib.pyplot figure object</p> <code>ax</code> <code>AxesSubplot</code> <p>Matplotlib.pyplot axes object</p> Source code in <code>topostats\\plottingfuncs.py</code> <pre><code>def save_figure(self):\n    \"\"\"Save figures as plt.savefig objects.\n\n    Returns\n    -------\n    fig: plt.figure.Figure\n        Matplotlib.pyplot figure object\n    ax: plt.axes._subplots.AxesSubplot\n        Matplotlib.pyplot axes object\n    \"\"\"\n    fig, ax = plt.subplots(1, 1)\n    shape = self.data.shape\n    if isinstance(self.data, np.ndarray):\n        im = ax.imshow(\n            self.data,\n            extent=(0, shape[1] * self.pixel_to_nm_scaling, 0, shape[0] * self.pixel_to_nm_scaling),\n            interpolation=self.interpolation,\n            cmap=self.cmap,\n            vmin=self.zrange[0],\n            vmax=self.zrange[1],\n        )\n        if isinstance(self.masked_array, np.ndarray):\n            self.masked_array[self.masked_array != 0] = 1\n            # If the image is too large for singles to be resolved in the mask, then dilate the mask proportionally\n            # to image size to enable clear viewing.\n            if np.max(self.masked_array.shape) &gt; 500:\n                dilation_strength = int(np.max(self.masked_array.shape) / 256)\n                self.masked_array = dilate_binary_image(\n                    binary_image=self.masked_array, dilation_iterations=dilation_strength\n                )\n            mask = np.ma.masked_where(self.masked_array == 0, self.masked_array)\n            ax.imshow(\n                mask,\n                cmap=self.mask_cmap,\n                extent=(\n                    0,\n                    shape[1] * self.pixel_to_nm_scaling,\n                    0,\n                    shape[0] * self.pixel_to_nm_scaling,\n                ),\n                interpolation=self.interpolation,\n                alpha=0.7,\n            )\n            patch = [Patch(color=self.mask_cmap(1, 0.7), label=\"Mask\")]\n            plt.legend(handles=patch, loc=\"upper right\", bbox_to_anchor=(1.02, 1.09))\n\n        plt.title(self.title)\n        plt.xlabel(\"Nanometres\")\n        plt.ylabel(\"Nanometres\")\n        set_n_ticks(ax, self.num_ticks)\n        plt.axis(self.axes)\n        if self.colorbar and self.image_type == \"non-binary\":\n            divider = make_axes_locatable(ax)\n            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n            plt.colorbar(im, cax=cax, label=\"Height (Nanometres)\")\n        if self.region_properties:\n            fig, ax = add_bounding_boxes_to_plot(fig, ax, shape, self.region_properties, self.pixel_to_nm_scaling)\n        if not self.axes and not self.colorbar:\n            plt.title(\"\")\n            fig.frameon = False\n            plt.savefig(\n                (self.output_dir / f\"{self.filename}.{self.savefig_format}\"),\n                bbox_inches=\"tight\",\n                pad_inches=0,\n                dpi=self.savefig_dpi,\n            )\n        else:\n            plt.savefig((self.output_dir / f\"{self.filename}.{self.savefig_format}\"), dpi=self.savefig_dpi)\n    else:\n        plt.xlabel(\"Nanometres\")\n        plt.ylabel(\"Nanometres\")\n        self.data.show(\n            ax=ax,\n            extent=(0, shape[1] * self.pixel_to_nm_scaling, 0, shape[0] * self.pixel_to_nm_scaling),\n            interpolation=self.interpolation,\n            cmap=self.cmap,\n        )\n    plt.close()\n    return fig, ax\n</code></pre>"},{"location":"api/plottingfuncs/#topostats.plottingfuncs.add_bounding_boxes_to_plot","title":"<code>add_bounding_boxes_to_plot(fig, ax, shape, region_properties: list, pixel_to_nm_scaling: float) -&gt; None</code>","text":"<p>Add the bounding boxes to a plot.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <p>Matplotlib.pyplot figure object</p> required <code>ax</code> <p>Matplotlib.pyplot axes object</p> required <code>shape</code> <p>Tuple of the image-to-be-plot's shape.</p> required <code>region_properties</code> <code>list</code> <p>Region properties to add bounding boxes from.</p> required <code>pixel_to_nm_scaling</code> <code>float</code> <p>The scaling factor from px to nm.</p> required <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Matplotlib.pyplot figure object.</p> <code>ax</code> <code>AxesSubplot</code> <p>Matplotlib.pyplot axes object.</p> Source code in <code>topostats\\plottingfuncs.py</code> <pre><code>def add_bounding_boxes_to_plot(fig, ax, shape, region_properties: list, pixel_to_nm_scaling: float) -&gt; None:\n    \"\"\"Add the bounding boxes to a plot.\n\n    Parameters\n    ----------\n    fig: plt.figure.Figure\n        Matplotlib.pyplot figure object\n    ax: plt.axes._subplots.AxesSubplot.\n        Matplotlib.pyplot axes object\n    shape: tuple\n        Tuple of the image-to-be-plot's shape.\n    region_properties:\n        Region properties to add bounding boxes from.\n    pixel_to_nm_scaling: float\n        The scaling factor from px to nm.\n\n    Returns\n    -------\n    fig: plt.figure.Figure\n        Matplotlib.pyplot figure object.\n    ax: plt.axes._subplots.AxesSubplot\n        Matplotlib.pyplot axes object.\n    \"\"\"\n    for region in region_properties:\n        min_y, min_x, max_y, max_x = (x * pixel_to_nm_scaling for x in region.bbox)\n        # Correct y-axis\n        min_y = (shape[0] * pixel_to_nm_scaling) - min_y\n        max_y = (shape[0] * pixel_to_nm_scaling) - max_y\n        rectangle = Rectangle((min_x, min_y), max_x - min_x, max_y - min_y, fill=False, edgecolor=\"white\", linewidth=2)\n        ax.add_patch(rectangle)\n    return fig, ax\n</code></pre>"},{"location":"api/plottingfuncs/#topostats.plottingfuncs.add_pixel_to_nm_to_plotting_config","title":"<code>add_pixel_to_nm_to_plotting_config(plotting_config: dict, pixel_to_nm_scaling: float) -&gt; dict</code>","text":"<p>Add the pixel to nanometre scaling factor to plotting configs.</p> <p>Ensures plots are in nanometres and not pixels.</p> <p>Parameters:</p> Name Type Description Default <code>plotting_config</code> <code>dict</code> <p>TopoStats plotting configuration dictionary</p> required <code>pixel_to_nm_scaling</code> <code>float</code> <p>Pixel to nanometre scaling factor for the image.</p> required <p>Returns:</p> Name Type Description <code>plotting_config</code> <code>dict</code> <p>Updated plotting config with the pixel to nanometre scaling factor applied to all the image configurations.</p> Source code in <code>topostats\\plottingfuncs.py</code> <pre><code>def add_pixel_to_nm_to_plotting_config(plotting_config: dict, pixel_to_nm_scaling: float) -&gt; dict:\n    \"\"\"Add the pixel to nanometre scaling factor to plotting configs.\n\n    Ensures plots are in nanometres and not pixels.\n\n    Parameters\n    ----------\n    plotting_config: dict\n        TopoStats plotting configuration dictionary\n    pixel_to_nm_scaling: float\n        Pixel to nanometre scaling factor for the image.\n\n    Returns\n    -------\n    plotting_config: dict\n        Updated plotting config with the pixel to nanometre scaling factor\n        applied to all the image configurations.\n    \"\"\"\n    # Update PLOT_DICT with pixel_to_nm_scaling (can't add _output_dir since it changes)\n    plot_opts = {\"pixel_to_nm_scaling\": pixel_to_nm_scaling}\n    for image, options in plotting_config[\"plot_dict\"].items():\n        plotting_config[\"plot_dict\"][image] = {**options, **plot_opts}\n    return plotting_config\n</code></pre>"},{"location":"api/plottingfuncs/#topostats.plottingfuncs.dilate_binary_image","title":"<code>dilate_binary_image(binary_image: np.ndarray, dilation_iterations: int) -&gt; np.ndarray</code>","text":"<p>Dilate a supplied binary image a given number of times.</p> <p>Parameters:</p> Name Type Description Default <code>binary_image</code> <code>ndarray</code> <p>Binary image to be dilated</p> required <code>dilation_iterations</code> <code>int</code> <p>Number of dilation iterations to be performed</p> required <p>Returns:</p> Name Type Description <code>binary_image</code> <code>ndarray</code> <p>Dilated binary image</p> Source code in <code>topostats\\plottingfuncs.py</code> <pre><code>def dilate_binary_image(binary_image: np.ndarray, dilation_iterations: int) -&gt; np.ndarray:\n    \"\"\"Dilate a supplied binary image a given number of times.\n\n    Parameters\n    ----------\n    binary_image: np.ndarray\n        Binary image to be dilated\n    dilation_iterations: int\n        Number of dilation iterations to be performed\n\n    Returns\n    -------\n    binary_image: np.ndarray\n        Dilated binary image\n    \"\"\"\n    binary_image = binary_image.copy()\n    for _ in range(dilation_iterations):\n        binary_image = binary_dilation(binary_image)\n\n    return binary_image\n</code></pre>"},{"location":"api/plottingfuncs/#topostats.plottingfuncs.load_mplstyle","title":"<code>load_mplstyle(style: str | Path) -&gt; None</code>","text":"<p>Load the Matplotlibrc parameter file.</p> <p>Parameters:</p> Name Type Description Default <code>style</code> <code>str | Path</code> <p>Path to a Matplotlib Style file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Only loads the style file.</p> Source code in <code>topostats\\plottingfuncs.py</code> <pre><code>def load_mplstyle(style: str | Path) -&gt; None:\n    \"\"\"Load the Matplotlibrc parameter file.\n\n    Parameters\n    ----------\n    style: str | Path\n        Path to a Matplotlib Style file.\n\n    Returns\n    -------\n    None\n        Only loads the style file.\n    \"\"\"\n    if style == \"topostats.mplstyle\":\n        plt.style.use(pkg_resources.files(topostats) / style)\n    else:\n        plt.style.use(style)\n</code></pre>"},{"location":"api/plottingfuncs/#topostats.plottingfuncs.set_n_ticks","title":"<code>set_n_ticks(ax: plt.Axes.axes, n_xy: list[int | None, int | None]) -&gt; None</code>","text":"<p>Set the number of ticks along the y and x axes and lets matplotlib assign the values.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>axes</code> <p>The axes to add ticks to.</p> required <code>n_xy</code> <code>list[int, int]</code> <p>The number of ticks.</p> required <p>Returns:</p> Type Description <code>axes</code> <p>The axes with the new ticks.</p> Source code in <code>topostats\\plottingfuncs.py</code> <pre><code>def set_n_ticks(ax: plt.Axes.axes, n_xy: list[int | None, int | None]) -&gt; None:\n    \"\"\"Set the number of ticks along the y and x axes and lets matplotlib assign the values.\n\n    Parameters\n    ----------\n    ax : plt.Axes.axes\n        The axes to add ticks to.\n    n_xy : list[int, int]\n        The number of ticks.\n\n    Returns\n    -------\n    plt.Axes.axes\n        The axes with the new ticks.\n    \"\"\"\n    if n_xy[0] is not None:\n        xlim = ax.get_xlim()\n        xstep = (max(xlim) - min(xlim)) / (n_xy[0] - 1)\n        xticks = np.arange(min(xlim), max(xlim) + xstep, xstep)\n        ax.set_xticks(np.round(xticks))\n    if n_xy[1] is not None:\n        ylim = ax.get_ylim()\n        ystep = (max(ylim) - min(ylim)) / (n_xy[1] - 1)\n        yticks = np.arange(min(ylim), max(ylim) + ystep, ystep)\n        ax.set_yticks(np.round(yticks))\n</code></pre>"},{"location":"api/run_topostats/","title":"Run TopoStats Module","text":"<p>Run TopoStats.</p> <p>This provides an entry point for running TopoStats as a command line programme.</p>"},{"location":"api/run_topostats/#topostats.run_topostats.run_topostats","title":"<code>run_topostats(args=None)</code>","text":"<p>Find and process all files.</p> Source code in <code>topostats\\run_topostats.py</code> <pre><code>def run_topostats(args=None):  # noqa: C901\n    \"\"\"Find and process all files.\"\"\"\n    # Parse command line options, load config (or default) and update with command line options\n    if args.config_file is not None:\n        config = read_yaml(args.config_file)\n    else:\n        default_config = pkg_resources.open_text(__package__, \"default_config.yaml\").read()\n        config = yaml.safe_load(default_config)\n    # Override the config with command line arguments passed in, eg --output_dir ./output/\n    config = update_config(config, args)\n\n    # Set logging level\n    if config[\"log_level\"] == \"warning\":\n        LOGGER.setLevel(\"WARNING\")\n    elif config[\"log_level\"] == \"error\":\n        LOGGER.setLevel(\"ERROR\")\n    elif config[\"log_level\"] == \"debug\":\n        LOGGER.setLevel(\"DEBUG\")\n    else:\n        LOGGER.setLevel(\"INFO\")\n    # Validate configuration\n    validate_config(config, schema=DEFAULT_CONFIG_SCHEMA, config_type=\"YAML configuration file\")\n\n    # Create base output directory\n    config[\"output_dir\"].mkdir(parents=True, exist_ok=True)\n\n    # Load plotting_dictionary and validate then update with command line options\n    plotting_dictionary = pkg_resources.open_text(__package__, \"plotting_dictionary.yaml\")\n    config[\"plotting\"][\"plot_dict\"] = yaml.safe_load(plotting_dictionary.read())\n    validate_config(\n        config[\"plotting\"][\"plot_dict\"], schema=PLOTTING_SCHEMA, config_type=\"YAML plotting configuration file\"\n    )\n    config[\"plotting\"] = update_config(config[\"plotting\"], args)\n\n    # Check earlier stages of processing are enabled for later.\n    check_run_steps(\n        filter_run=config[\"filter\"][\"run\"],\n        grains_run=config[\"grains\"][\"run\"],\n        grainstats_run=config[\"grainstats\"][\"run\"],\n        dnatracing_run=config[\"dnatracing\"][\"run\"],\n    )\n    # Ensures each image has all plotting options which are passed as **kwargs\n    config[\"plotting\"] = update_plotting_config(config[\"plotting\"])\n    LOGGER.debug(f\"Plotting configuration after update :\\n{pformat(config['plotting'], indent=4)}\")\n\n    LOGGER.info(f\"Configuration file loaded from      : {args.config_file}\")\n    LOGGER.info(f\"Scanning for images in              : {config['base_dir']}\")\n    LOGGER.info(f\"Output directory                    : {str(config['output_dir'])}\")\n    LOGGER.info(f\"Looking for images with extension   : {config['file_ext']}\")\n    img_files = find_files(config[\"base_dir\"], file_ext=config[\"file_ext\"])\n    LOGGER.info(f\"Images with extension {config['file_ext']} in {config['base_dir']} : {len(img_files)}\")\n    if len(img_files) == 0:\n        LOGGER.error(f\"No images with extension {config['file_ext']} in {config['base_dir']}\")\n        LOGGER.error(\"Please check your configuration and directories.\")\n        sys.exit()\n    LOGGER.info(f\"Thresholding method (Filtering)     : {config['filter']['threshold_method']}\")\n    LOGGER.info(f\"Thresholding method (Grains)        : {config['grains']['threshold_method']}\")\n    LOGGER.debug(f\"Configuration after update         : \\n{pformat(config, indent=4)}\")  # noqa : T203\n\n    processing_function = partial(\n        process_scan,\n        base_dir=config[\"base_dir\"],\n        filter_config=config[\"filter\"],\n        grains_config=config[\"grains\"],\n        grainstats_config=config[\"grainstats\"],\n        dnatracing_config=config[\"dnatracing\"],\n        plotting_config=config[\"plotting\"],\n        output_dir=config[\"output_dir\"],\n    )\n\n    all_scan_data = LoadScans(img_files, **config[\"loading\"])\n    all_scan_data.get_data()\n    # Get a dictionary of all the image data dictionaries.\n    # Keys are the image names\n    # Values are the individual image data dictionaries\n    scan_data_dict = all_scan_data.img_dict\n\n    with Pool(processes=config[\"cores\"]) as pool:\n        results = defaultdict()\n        image_stats_all = defaultdict()\n        with tqdm(\n            total=len(img_files),\n            desc=f\"Processing images from {config['base_dir']}, results are under {config['output_dir']}\",\n        ) as pbar:\n            for img, result, individual_image_stats_df in pool.imap_unordered(\n                processing_function,\n                scan_data_dict.values(),\n            ):\n                results[str(img)] = result\n                pbar.update()\n\n                # Add the dataframe to the results dict\n                image_stats_all[str(img)] = individual_image_stats_df\n\n                # Display completion message for the image\n                LOGGER.info(f\"[{img.name}] Processing completed.\")\n\n    LOGGER.info(f\"Saving image stats to : {config['output_dir']}/image_stats.csv.\")\n    # Concatenate all the dictionary's values into a dataframe. Ignore the keys since\n    # the dataframes have the file names in them already.\n    image_stats_all_df = pd.concat(image_stats_all.values())\n    image_stats_all_df.to_csv(config[\"output_dir\"] / \"image_stats.csv\")\n\n    try:\n        results = pd.concat(results.values())\n    except ValueError as error:\n        LOGGER.error(\"No grains found in any images, consider adjusting your thresholds.\")\n        LOGGER.error(error)\n\n    # Summary Statistics and Plots\n    if config[\"summary_stats\"][\"run\"]:\n        # Load summary plots/statistics configuration and validate, location depends on command line args or value in\n        # any config file given, if neither are provided the default topostats/summary_config.yaml is loaded\n        if args.summary_config is not None:\n            summary_config = read_yaml(args.summary_config)\n        elif config[\"summary_stats\"][\"config\"] is not None:\n            summary_config = read_yaml(config[\"summary_stats\"][\"config\"])\n        else:\n            summary_yaml = pkg_resources.open_text(__package__, \"summary_config.yaml\")\n            summary_config = yaml.safe_load(summary_yaml.read())\n\n        # Do not pass command line arguments to toposum as they clash with process command line arguments\n        summary_config = update_config(summary_config, config[\"plotting\"])\n\n        validate_config(summary_config, SUMMARY_SCHEMA, config_type=\"YAML summarisation config\")\n        # We never want to load data from CSV as we are using the data that has just been processed.\n        summary_config.pop(\"csv_file\")\n\n        # Load variable to label mapping\n        plotting_yaml = pkg_resources.open_text(__package__, \"var_to_label.yaml\")\n        summary_config[\"var_to_label\"] = yaml.safe_load(plotting_yaml.read())\n        LOGGER.info(\"[plotting] Default variable to labels mapping loaded.\")\n\n        # If we don't have a dataframe or we do and it is all NaN there is nothing to plot\n        if isinstance(results, pd.DataFrame) and not results.isna().values.all():\n            if results.shape[0] &gt; 1:\n                # If summary_config[\"output_dir\"] does not match or is not a sub-dir of config[\"output_dir\"] it\n                # needs creating\n                summary_config[\"output_dir\"] = config[\"output_dir\"] / \"summary_distributions\"\n                summary_config[\"output_dir\"].mkdir(parents=True, exist_ok=True)\n                LOGGER.info(f\"Summary plots and statistics will be saved to : {summary_config['output_dir']}\")\n\n                # Plot summaries\n                summary_config[\"df\"] = results.reset_index()\n                toposum(summary_config)\n            else:\n                LOGGER.warning(\n                    \"There are fewer than two grains that have been detected, so\"\n                    \" summary plots cannot be made for this image.\"\n                )\n        else:\n            LOGGER.warning(\n                \"There are no results to plot, either...\\n\\n\"\n                \"* you have disabled grains/grainstats/dnatracing.\\n\"\n                \"* no grains have been detected across all scans.\\n\"\n                \"* there have been errors.\\n\\n\"\n                \"If you are not expecting to detect grains please consider disabling\"\n                \"grains/grainstats/dnatracing/plotting/summary_stats. If you are expecting to detect grains\"\n                \" please check log-files for further information.\"\n            )\n    else:\n        summary_config = None\n\n    # Write statistics to CSV if there is data.\n    if isinstance(results, pd.DataFrame) and not results.isna().values.all():\n        results.reset_index(inplace=True)\n        results.set_index([\"image\", \"threshold\", \"molecule_number\"], inplace=True)\n        results.to_csv(config[\"output_dir\"] / \"all_statistics.csv\", index=True)\n        save_folder_grainstats(config[\"output_dir\"], config[\"base_dir\"], results)\n        results.reset_index(inplace=True)  # So we can access unique image names\n        images_processed = len(results[\"image\"].unique())\n    else:\n        images_processed = 0\n        LOGGER.warning(\"There are no grainstats or dnatracing statistics to write to CSV.\")\n    # Write config to file\n    config[\"plotting\"].pop(\"plot_dict\")\n    write_yaml(config, output_dir=config[\"output_dir\"])\n    LOGGER.debug(f\"Images processed : {images_processed}\")\n    # Update config with plotting defaults for printing\n    completion_message(config, img_files, summary_config, images_processed)\n</code></pre>"},{"location":"api/scars/","title":"Scars Module","text":"<p>Image artefact correction functions that interpolates values filling the space of any detected scars.</p>"},{"location":"api/scars/#topostats.scars.remove_scars","title":"<code>remove_scars(img: np.ndarray, filename: str, removal_iterations: int = 2, threshold_low: float = 0.25, threshold_high: float = 0.666, max_scar_width: int = 4, min_scar_length: int = 16)</code>","text":"<p>Remove scars from an image.</p> <p>Scars are long, typically 1-4 pixels wide streaks of high or low data in AFM images. They are a problem resulting from random errors in the AFM data collection process and are hard to avoid. This function detects and removes these artefacts by interpolating over them between the pixels above and below them. This method takes no parameters as it uses parameters already established as instance variables when the class was instantiated.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>A 2-D image to remove scars from.</p> required <code>filename</code> <code>str</code> <p>The filename (used for logging outputs only).</p> required <code>removal_iterations</code> <code>int</code> <p>The number of times the scar removal should run on the image. Running just once sometimes isn't enough to remove some of the more difficult to remove scars.</p> <code>2</code> <code>threshold_low</code> <code>float</code> <p>A value that when multiplied with the standard deviation, acts as a threshold to determine if an increase or decrease in height might constitute the top or bottom of a scar.</p> <code>0.25</code> <code>threshold_high</code> <code>float</code> <p>A floating point value that is used similarly to threshold_low, however sharp inclines or descents that result in values in the mask higher than this threshold are automatically considered scars.</p> <code>0.666</code> <code>max_scar_width</code> <code>int</code> <p>A value that dictates the maximum width that a scar can be. Note that this does not mean horizontal width, rather vertical, this is because we consider scars to be laying flat, horizontally, so their width is vertical and their length is horizontal.</p> <code>4</code> <code>min_scar_length</code> <code>int</code> <p>An integer that restricts the algorithm to only mark scars that are as long or longer than this length. This is important for ensuring that noise or legitimate but sharp datapoints do not get detected as scars. Note that length here is horizontal, as scars are thin, horizontal features.</p> <code>16</code> <p>Returns:</p> Type Description <code>img</code> <p>The original 2-D image with scars removed, unless the config has run set to False, in which case it will not remove the scars.</p> Source code in <code>topostats\\scars.py</code> <pre><code>def remove_scars(\n    img: np.ndarray,\n    filename: str,\n    removal_iterations: int = 2,\n    threshold_low: float = 0.250,\n    threshold_high: float = 0.666,\n    max_scar_width: int = 4,\n    min_scar_length: int = 16,\n):\n    \"\"\"\n    Remove scars from an image.\n\n    Scars are long, typically 1-4 pixels wide streaks of high or low data in AFM images. They are a problem\n    resulting from random errors in the AFM data collection process and are hard to avoid. This function\n    detects and removes these artefacts by interpolating over them between the pixels above and below them.\n    This method takes no parameters as it uses parameters already established as instance variables when the\n    class was instantiated.\n\n    Parameters\n    ----------\n    img: np.ndarray\n        A 2-D image to remove scars from.\n    filename: str\n        The filename (used for logging outputs only).\n    removal_iterations: int\n        The number of times the scar removal should run on the image.\n        Running just once sometimes isn't enough to remove some of the\n        more difficult to remove scars.\n    threshold_low: float\n        A value that when multiplied with the standard deviation, acts as a threshold to determine if an increase\n        or decrease in height might constitute the top or bottom of a scar.\n    threshold_high: float\n        A floating point value that is used similarly to threshold_low, however sharp inclines or descents\n        that result in values in the mask higher than this threshold are automatically considered scars.\n    max_scar_width: int\n        A value that dictates the maximum width that a scar can be. Note that this does not mean horizontal width,\n        rather vertical, this is because we consider scars to be laying flat, horizontally, so their width is\n        vertical and their length is horizontal.\n    min_scar_length: int\n        An integer that restricts the algorithm to only mark scars that are as long or longer than this length.\n        This is important for ensuring that noise or legitimate but sharp datapoints do not get detected as scars.\n        Note that length here is horizontal, as scars are thin, horizontal features.\n\n    Returns\n    -------\n    self.img\n        The original 2-D image with scars removed, unless the config has run set to False, in which case it\n        will not remove the scars.\n    \"\"\"\n    LOGGER.info(f\"[{filename}] : Removing scars\")\n\n    first_marked_mask = None\n    for i in range(removal_iterations):\n        marked_positive = _mark_scars(\n            img=img,\n            direction=\"positive\",\n            threshold_low=threshold_low,\n            threshold_high=threshold_high,\n            max_scar_width=max_scar_width,\n            min_scar_length=min_scar_length,\n        )\n        marked_negative = _mark_scars(\n            img=img,\n            direction=\"negative\",\n            threshold_low=threshold_low,\n            threshold_high=threshold_high,\n            max_scar_width=max_scar_width,\n            min_scar_length=min_scar_length,\n        )\n        # Combine the upper and lower scar masks\n        marked_both = np.bitwise_or(marked_positive.astype(bool), marked_negative.astype(bool))\n\n        if i == 0:\n            first_marked_mask = marked_both\n\n        _remove_marked_scars(img, np.copy(marked_both))\n\n        LOGGER.debug(\"Scars removed\")\n\n    return img, first_marked_mask\n</code></pre>"},{"location":"api/statistics/","title":"Statistics Modules","text":"<p>Function for calculating statistics about a whole image, for example number of grains or surface roughness.</p>"},{"location":"api/statistics/#topostats.statistics.image_statistics","title":"<code>image_statistics(image: np.ndarray, filename: str, pixel_to_nm_scaling: float, results_df: pd.DataFrame) -&gt; pd.DataFrame</code>","text":"<p>Calculate statistics pertaining to the whole image.</p> <p>Calculates the size of the image in pixels and metres, the root-mean-squared roughness and the grains per metre squared.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Numpy 2D image array of the image to calculate stats for.</p> required <code>filename</code> <code>str</code> <p>The name of the file being processed.</p> required <code>pixel_to_nm_scaling</code> <code>float</code> <p>Float of the scaling factor between pixels and nanometres.</p> required <code>results_df</code> <code>DataFrame</code> <p>Pandas DataFrame of statistics pertaining to individual grains including from grainstats and dna tracing.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of image statistics.</p> Source code in <code>topostats\\statistics.py</code> <pre><code>def image_statistics(\n    image: np.ndarray,\n    filename: str,\n    pixel_to_nm_scaling: float,\n    results_df: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate statistics pertaining to the whole image.\n\n    Calculates the size of the image in pixels and metres, the root-mean-squared roughness and the grains per metre\n    squared.\n\n    Parameters\n    ----------\n    image: np.ndarray\n        Numpy 2D image array of the image to calculate stats for.\n    filename: str\n        The name of the file being processed.\n    pixel_to_nm_scaling: float\n        Float of the scaling factor between pixels and nanometres.\n    results_df: pd.DataFrame\n        Pandas DataFrame of statistics pertaining to individual grains including from grainstats and\n        dna tracing.\n\n    Returns\n    -------\n    dict\n        Dictionary of image statistics.\n    \"\"\"\n    image_stats = {\n        \"image\": filename,\n        \"image_size_x_m\": None,\n        \"image_size_y_m\": None,\n        \"image_area_m2\": None,\n        \"image_size_x_px\": image.shape[1],\n        \"image_size_y_px\": image.shape[0],\n        \"image_area_px2\": None,\n        \"grains_number_above\": None,\n        \"grains_per_m2_above\": None,\n        \"grains_number_below\": None,\n        \"grains_per_m2_below\": None,\n        \"rms_roughness\": None,\n    }\n\n    # Calculate dimensions of the image\n    image_stats[\"image_size_x_m\"] = image.shape[1] * pixel_to_nm_scaling * 1e-9\n    image_stats[\"image_size_y_m\"] = image.shape[0] * pixel_to_nm_scaling * 1e-9\n    image_stats[\"image_area_m2\"] = image_stats[\"image_size_x_m\"] * image_stats[\"image_size_y_m\"]\n    image_stats[\"image_area_px2\"] = image_stats[\"image_size_x_px\"] * image_stats[\"image_size_y_px\"]\n\n    # Calculate the RMS roughness of the sample on the flattened image.\n    image_stats[\"rms_roughness\"] = roughness_rms(image=image) * 1e-9\n\n    # Calculate image stats relating to grain statistics. Note that the existence of any of these stats\n    # is not guaranteed\n    try:\n        image_stats[\"grains_number_below\"] = results_df[\"threshold\"].value_counts().get(\"below\", 0)\n        image_stats[\"grains_per_m2_below\"] = image_stats[\"grains_number_below\"] / image_stats[\"image_area_m2\"]\n    except KeyError:\n        pass\n    try:\n        image_stats[\"grains_number_above\"] = results_df[\"threshold\"].value_counts().get(\"above\", 0)\n        image_stats[\"grains_per_m2_above\"] = image_stats[\"grains_number_above\"] / image_stats[\"image_area_m2\"]\n    except KeyError:\n        pass\n\n    image_stats_df = pd.DataFrame([image_stats])\n    image_stats_df.set_index(\"image\", inplace=True)\n\n    return image_stats_df\n</code></pre>"},{"location":"api/statistics/#topostats.statistics.roughness_rms","title":"<code>roughness_rms(image: np.ndarray) -&gt; float</code>","text":"<p>Calculate the root-mean-square roughness of a heightmap image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>2D numpy array of heightmap data to calculate the roughness of</p> required <p>Returns:</p> Type Description <code>float</code> <p>The RMS roughness of the input array.</p> Source code in <code>topostats\\statistics.py</code> <pre><code>def roughness_rms(image: np.ndarray) -&gt; float:\n    \"\"\"Calculate the root-mean-square roughness of a heightmap image.\n\n    Parameters\n    ----------\n    image: np.ndarray\n        2D numpy array of heightmap data to calculate the roughness of\n\n    Returns\n    -------\n    float\n        The RMS roughness of the input array.\n    \"\"\"\n    return np.sqrt(np.mean(np.square(image)))\n</code></pre>"},{"location":"api/theme/","title":"Theme Modules","text":"<p>Custom Bruker Nanoscope colorscale.</p>"},{"location":"api/theme/#topostats.theme.Colormap","title":"<code>Colormap</code>","text":"<p>Class for setting the Colormap.</p> Source code in <code>topostats\\theme.py</code> <pre><code>class Colormap:\n    \"\"\"Class for setting the Colormap.\"\"\"\n\n    def __init__(self, name: str = \"nanoscope\"):\n        self.name = name\n        self.cmap = None\n        self.set_cmap(self.name)\n\n    def __str__(self):\n        \"\"\"Return string representation of object.\"\"\"\n        return f\"TopoStats Colormap: {self.name}\"\n\n    def set_cmap(self, name: str):\n        \"\"\"Set the ColorMap.\n\n        Parameters\n        ----------\n        name: str\n            Name of colormap\n        \"\"\"\n        if name.lower() == \"nanoscope\":\n            self.cmap = self.nanoscope()\n        elif name.lower() == \"gwyddion\":\n            self.cmap = self.gwyddion()\n        elif name.lower() == \"blu\":\n            self.cmap = self.blu()\n        else:\n            # Get one of the matplotlib colormaps\n            self.cmap = mpl.colormaps[name]\n        LOGGER.debug(f\"[theme] Colormap set to : {name}\")\n\n    def get_cmap(self):\n        \"\"\"Return the matplotlib.cm colormap object.\"\"\"\n        return self.cmap\n\n    @staticmethod\n    def nanoscope():\n        \"\"\"\n        Matplotlib compatible colormap that replicates the Bruker Nanoscope colorscale.\n\n        The colormap is implemented in Gwyddion's GwyGradient via 'Nanoscope.txt'.\n        \"\"\"\n        cdict = {\n            \"red\": (\n                (0.0, 0.0, 0.0),\n                (0.124464, 0.0, 0.0),\n                (0.236052, 0.0670103, 0.0670103),\n                (0.371245, 0.253338, 0.253338),\n                (0.472103, 0.392344, 0.392344),\n                (0.611588, 0.584587, 0.584587),\n                (0.708155, 0.717678, 0.717678),\n                (0.714052, 0.725806, 0.725806),\n                (0.890558, 0.969072, 0.969072),\n                (0.933476, 0.987464, 0.987464),\n                (0.944709, 0.992278, 0.992278),\n                (0.965682, 0.995207, 0.995207),\n                (0.971401, 0.996006, 0.996006),\n                (1, 1, 1),\n            ),\n            \"green\": (\n                (0.0, 0.0, 0.0),\n                (0.124464, 0.0, 0.0),\n                (0.236052, 0.0, 0.0),\n                (0.371245, 0.0, 0.0),\n                (0.472103, 0.0721649, 0.0721649),\n                (0.611588, 0.334114, 0.334114),\n                (0.708155, 0.515464, 0.515464),\n                (0.714052, 0.527471, 0.527471),\n                (0.890558, 0.886843, 0.886843),\n                (0.933476, 0.974227, 0.974227),\n                (0.944709, 0.980523, 0.980523),\n                (0.965682, 0.992278, 0.992278),\n                (0.971401, 0.993565, 0.993565),\n                (1, 1, 1),\n            ),\n            \"blue\": (\n                (0.0, 0.0, 0.0),\n                (0.124464, 0.0, 0.0),\n                (0.236052, 0.0, 0.0),\n                (0.371245, 0.0, 0.0),\n                (0.472103, 0.0, 0.0),\n                (0.611588, 0.0, 0.0),\n                (0.708155, 0.252575, 0.252575),\n                (0.714052, 0.268, 0.268),\n                (0.890558, 0.76343, 0.76343),\n                (0.933476, 0.883897, 0.883897),\n                (0.944709, 0.915426, 0.915426),\n                (0.965682, 0.974293, 0.974293),\n                (0.971401, 0.990347, 0.990347),\n                (1, 1, 1),\n            ),\n        }\n\n        return LinearSegmentedColormap(\"nanoscope\", cdict)\n\n    @staticmethod\n    def gwyddion():\n        \"\"\"Set RGBA colour map for the Gwyddion.net colour gradient.\"\"\"\n        N = 4  # Number of values\n        vals = np.ones((N, 4))  # Initialise the array to be full of 1.0\n        vals[0] = [0.0, 0.0, 0.0, 1]\n        vals[1] = [168 / 256, 40 / 256, 15 / 256, 1.0]\n        vals[2] = [243 / 256, 194 / 256, 93 / 256, 1.0]\n        vals[3] = [1.0, 1.0, 1.0, 1.0]\n\n        return LinearSegmentedColormap.from_list(\"gwyddion\", vals, N=256)\n\n    @staticmethod\n    def blu():\n        \"\"\"Set RGBA colour map of just the colour blue.\"\"\"\n        return ListedColormap([[32 / 256, 226 / 256, 205 / 256]], \"blu\", N=256)\n</code></pre>"},{"location":"api/theme/#topostats.theme.Colormap.__str__","title":"<code>__str__()</code>","text":"<p>Return string representation of object.</p> Source code in <code>topostats\\theme.py</code> <pre><code>def __str__(self):\n    \"\"\"Return string representation of object.\"\"\"\n    return f\"TopoStats Colormap: {self.name}\"\n</code></pre>"},{"location":"api/theme/#topostats.theme.Colormap.blu","title":"<code>blu()</code>  <code>staticmethod</code>","text":"<p>Set RGBA colour map of just the colour blue.</p> Source code in <code>topostats\\theme.py</code> <pre><code>@staticmethod\ndef blu():\n    \"\"\"Set RGBA colour map of just the colour blue.\"\"\"\n    return ListedColormap([[32 / 256, 226 / 256, 205 / 256]], \"blu\", N=256)\n</code></pre>"},{"location":"api/theme/#topostats.theme.Colormap.get_cmap","title":"<code>get_cmap()</code>","text":"<p>Return the matplotlib.cm colormap object.</p> Source code in <code>topostats\\theme.py</code> <pre><code>def get_cmap(self):\n    \"\"\"Return the matplotlib.cm colormap object.\"\"\"\n    return self.cmap\n</code></pre>"},{"location":"api/theme/#topostats.theme.Colormap.gwyddion","title":"<code>gwyddion()</code>  <code>staticmethod</code>","text":"<p>Set RGBA colour map for the Gwyddion.net colour gradient.</p> Source code in <code>topostats\\theme.py</code> <pre><code>@staticmethod\ndef gwyddion():\n    \"\"\"Set RGBA colour map for the Gwyddion.net colour gradient.\"\"\"\n    N = 4  # Number of values\n    vals = np.ones((N, 4))  # Initialise the array to be full of 1.0\n    vals[0] = [0.0, 0.0, 0.0, 1]\n    vals[1] = [168 / 256, 40 / 256, 15 / 256, 1.0]\n    vals[2] = [243 / 256, 194 / 256, 93 / 256, 1.0]\n    vals[3] = [1.0, 1.0, 1.0, 1.0]\n\n    return LinearSegmentedColormap.from_list(\"gwyddion\", vals, N=256)\n</code></pre>"},{"location":"api/theme/#topostats.theme.Colormap.nanoscope","title":"<code>nanoscope()</code>  <code>staticmethod</code>","text":"<p>Matplotlib compatible colormap that replicates the Bruker Nanoscope colorscale.</p> <p>The colormap is implemented in Gwyddion's GwyGradient via 'Nanoscope.txt'.</p> Source code in <code>topostats\\theme.py</code> <pre><code>@staticmethod\ndef nanoscope():\n    \"\"\"\n    Matplotlib compatible colormap that replicates the Bruker Nanoscope colorscale.\n\n    The colormap is implemented in Gwyddion's GwyGradient via 'Nanoscope.txt'.\n    \"\"\"\n    cdict = {\n        \"red\": (\n            (0.0, 0.0, 0.0),\n            (0.124464, 0.0, 0.0),\n            (0.236052, 0.0670103, 0.0670103),\n            (0.371245, 0.253338, 0.253338),\n            (0.472103, 0.392344, 0.392344),\n            (0.611588, 0.584587, 0.584587),\n            (0.708155, 0.717678, 0.717678),\n            (0.714052, 0.725806, 0.725806),\n            (0.890558, 0.969072, 0.969072),\n            (0.933476, 0.987464, 0.987464),\n            (0.944709, 0.992278, 0.992278),\n            (0.965682, 0.995207, 0.995207),\n            (0.971401, 0.996006, 0.996006),\n            (1, 1, 1),\n        ),\n        \"green\": (\n            (0.0, 0.0, 0.0),\n            (0.124464, 0.0, 0.0),\n            (0.236052, 0.0, 0.0),\n            (0.371245, 0.0, 0.0),\n            (0.472103, 0.0721649, 0.0721649),\n            (0.611588, 0.334114, 0.334114),\n            (0.708155, 0.515464, 0.515464),\n            (0.714052, 0.527471, 0.527471),\n            (0.890558, 0.886843, 0.886843),\n            (0.933476, 0.974227, 0.974227),\n            (0.944709, 0.980523, 0.980523),\n            (0.965682, 0.992278, 0.992278),\n            (0.971401, 0.993565, 0.993565),\n            (1, 1, 1),\n        ),\n        \"blue\": (\n            (0.0, 0.0, 0.0),\n            (0.124464, 0.0, 0.0),\n            (0.236052, 0.0, 0.0),\n            (0.371245, 0.0, 0.0),\n            (0.472103, 0.0, 0.0),\n            (0.611588, 0.0, 0.0),\n            (0.708155, 0.252575, 0.252575),\n            (0.714052, 0.268, 0.268),\n            (0.890558, 0.76343, 0.76343),\n            (0.933476, 0.883897, 0.883897),\n            (0.944709, 0.915426, 0.915426),\n            (0.965682, 0.974293, 0.974293),\n            (0.971401, 0.990347, 0.990347),\n            (1, 1, 1),\n        ),\n    }\n\n    return LinearSegmentedColormap(\"nanoscope\", cdict)\n</code></pre>"},{"location":"api/theme/#topostats.theme.Colormap.set_cmap","title":"<code>set_cmap(name: str)</code>","text":"<p>Set the ColorMap.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of colormap</p> required Source code in <code>topostats\\theme.py</code> <pre><code>def set_cmap(self, name: str):\n    \"\"\"Set the ColorMap.\n\n    Parameters\n    ----------\n    name: str\n        Name of colormap\n    \"\"\"\n    if name.lower() == \"nanoscope\":\n        self.cmap = self.nanoscope()\n    elif name.lower() == \"gwyddion\":\n        self.cmap = self.gwyddion()\n    elif name.lower() == \"blu\":\n        self.cmap = self.blu()\n    else:\n        # Get one of the matplotlib colormaps\n        self.cmap = mpl.colormaps[name]\n    LOGGER.debug(f\"[theme] Colormap set to : {name}\")\n</code></pre>"},{"location":"api/thresholds/","title":"Thresholds Modules","text":"<p>Functions for calculating thresholds.</p>"},{"location":"api/thresholds/#topostats.thresholds.threshold","title":"<code>threshold(image: np.ndarray, method: str = None, otsu_threshold_multiplier: float = None, **kwargs: dict) -&gt; float</code>","text":"<p>Thresholding for producing masks.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Method to use for thresholding, currently supported methods are otsu (default), mean and minimum.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to skimage methods.</p> <code>{}</code> <p>Returns:</p> Type Description <code>float</code> <p>Threshold of image using specified method.</p> Source code in <code>topostats\\thresholds.py</code> <pre><code>def threshold(image: np.ndarray, method: str = None, otsu_threshold_multiplier: float = None, **kwargs: dict) -&gt; float:\n    \"\"\"Thresholding for producing masks.\n\n    Parameters\n    ----------\n    method : str\n        Method to use for thresholding, currently supported methods are otsu (default), mean and minimum.\n    **kwargs : dict\n        Additional keyword arguments to pass to skimage methods.\n\n    Returns\n    -------\n    float\n        Threshold of image using specified method.\n    \"\"\"\n    thresholder = _get_threshold(method)\n    return thresholder(image, otsu_threshold_multiplier=otsu_threshold_multiplier, **kwargs)\n</code></pre>"},{"location":"api/utils/","title":"Utils Modules","text":"<p>Utilities.</p>"},{"location":"api/utils/#topostats.utils.bound_padded_coordinates_to_image","title":"<code>bound_padded_coordinates_to_image(coordinates: npt.NDArray, padding: int, image_shape: tuple) -&gt; tuple</code>","text":"<p>Ensure the padding of coordinates points does not fall outside of the image shape.</p> <p>This function is primarily used in the dnaTrace.get_fitted_traces() method which aims to adjust the points of a skeleton to sit on the highest points of a traced molecule. In order to do so it takes the ordered skeleton, which may not lie on the highest points as it is generated from a binary mask that is unaware of the heights, and then defines a padded boundary of 3nm profile perpendicular to the backbone of the DNA (which at this point is the skeleton based on a mask). Each point along the skeleton therefore needs padding by a minimum of 2 pixels (in this case each pixel equates to a cell in a NumPy array). If a point is within 2 pixels (i.e. 2 cells) of the border then we can not pad beyond this region, we have to stop at the edge of the image and so the coordinates is adjusted such that the padding will lie on the edge of the image/array.</p> <p>Parameters:</p> Name Type Description Default <code>coordinates</code> <code>NDArray</code> <p>Coordinates of a point on the mask based skeleton.</p> required <code>padding</code> <code>int</code> <p>Number of pixels/cells to pad around the point.</p> required <code>image_shape</code> <code>tuple</code> <p>The shape of the original image from which the pixel is obtained.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>Returns a tuple of coordinates that ensure that when the point is padded by the noted padding width in subsequent calculations it will not be outside of the image shape.</p> Source code in <code>topostats\\utils.py</code> <pre><code>def bound_padded_coordinates_to_image(coordinates: npt.NDArray, padding: int, image_shape: tuple) -&gt; tuple:\n    \"\"\"Ensure the padding of coordinates points does not fall outside of the image shape.\n\n    This function is primarily used in the dnaTrace.get_fitted_traces() method which aims to adjust the points of a\n    skeleton to sit on the highest points of a traced molecule. In order to do so it takes the ordered skeleton, which\n    may not lie on the highest points as it is generated from a binary mask that is unaware of the heights, and then\n    defines a padded boundary of 3nm profile perpendicular to the backbone of the DNA (which at this point is the\n    skeleton based on a mask). Each point along the skeleton therefore needs padding by a minimum of 2 pixels (in this\n    case each pixel equates to a cell in a NumPy array). If a point is within 2 pixels (i.e. 2 cells) of the border then\n    we can not pad beyond this region, we have to stop at the edge of the image and so the coordinates is adjusted such\n    that the padding will lie on the edge of the image/array.\n\n    Parameters\n    ----------\n    coordinates : npt.NDArray\n        Coordinates of a point on the mask based skeleton.\n    padding : int\n        Number of pixels/cells to pad around the point.\n    image_shape : tuple\n        The shape of the original image from which the pixel is obtained.\n\n    Returns\n    -------\n    tuple\n        Returns a tuple of coordinates that ensure that when the point is padded by the noted padding width in\n        subsequent calculations it will not be outside of the image shape.\n    \"\"\"\n    # Calculate the maximum row and column indexes\n    max_row = image_shape[0] - 1\n    max_col = image_shape[1] - 1\n    row_coord, col_coord = coordinates\n\n    def check(coord, max_val):\n        if coord - padding &lt; 0:\n            coord = padding\n        elif coord + padding &gt; max_val:\n            coord = max_val - padding\n        return coord\n\n    return check(row_coord, max_row), check(col_coord, max_col)\n</code></pre>"},{"location":"api/utils/#topostats.utils.convert_path","title":"<code>convert_path(path: str | Path) -&gt; Path</code>","text":"<p>Ensure path is Path object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to be converted.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>pathlib Path</p> Source code in <code>topostats\\utils.py</code> <pre><code>def convert_path(path: str | Path) -&gt; Path:\n    \"\"\"Ensure path is Path object.\n\n    Parameters\n    ----------\n    path: Union[str, Path]\n        Path to be converted.\n\n    Returns\n    -------\n    Path\n        pathlib Path\n    \"\"\"\n    return Path().cwd() if path == \"./\" else Path(path).expanduser()\n</code></pre>"},{"location":"api/utils/#topostats.utils.create_empty_dataframe","title":"<code>create_empty_dataframe(columns: set = ALL_STATISTICS_COLUMNS, index: tuple = 'molecule_number') -&gt; pd.DataFrame</code>","text":"<p>Create an empty data frame for returning when no results are found.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>set</code> <p>Columns of the empty dataframe.</p> <code>ALL_STATISTICS_COLUMNS</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Empty Pandas DataFrame.</p> Source code in <code>topostats\\utils.py</code> <pre><code>def create_empty_dataframe(columns: set = ALL_STATISTICS_COLUMNS, index: tuple = \"molecule_number\") -&gt; pd.DataFrame:\n    \"\"\"Create an empty data frame for returning when no results are found.\n\n    Parameters\n    ----------\n    columns: list\n        Columns of the empty dataframe.\n\n    Returns\n    -------\n    pd.DataFrame\n        Empty Pandas DataFrame.\n    \"\"\"\n    empty_df = pd.DataFrame(columns=columns)\n    return empty_df.set_index(index)\n</code></pre>"},{"location":"api/utils/#topostats.utils.get_mask","title":"<code>get_mask(image: np.ndarray, thresholds: dict, img_name: str = None) -&gt; np.ndarray</code>","text":"<p>Mask data that should not be included in flattening.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>2D Numpy array of the image to have a mask derived for.</p> required <code>thresholds</code> <code>dict</code> <p>Dictionary of thresholds, at a bare minimum must have key 'below' with an associated value, second key is to have an 'above' threshold.</p> required <code>img_name</code> <code>str</code> <p>Image name that is being masked.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>2D Numpy boolean array of points to mask.</p> Source code in <code>topostats\\utils.py</code> <pre><code>def get_mask(image: np.ndarray, thresholds: dict, img_name: str = None) -&gt; np.ndarray:\n    \"\"\"Mask data that should not be included in flattening.\n\n    Parameters\n    ----------\n    image: np.ndarray\n        2D Numpy array of the image to have a mask derived for.\n\n    thresholds: dict\n        Dictionary of thresholds, at a bare minimum must have key 'below' with an associated value, second key is\n        to have an 'above' threshold.\n    img_name: str\n        Image name that is being masked.\n\n    Returns\n    -------\n    np.ndarray\n        2D Numpy boolean array of points to mask.\n    \"\"\"\n    # Both thresholds are applicable\n    if \"below\" in thresholds and \"above\" in thresholds:\n        mask_above = _get_mask(image, thresh=thresholds[\"above\"], threshold_direction=\"above\", img_name=img_name)\n        mask_below = _get_mask(image, thresh=thresholds[\"below\"], threshold_direction=\"below\", img_name=img_name)\n        # Masks are combined to remove both the extreme high and extreme low data points.\n        return mask_above + mask_below\n    # Only below threshold is applicable\n    if \"below\" in thresholds:\n        return _get_mask(image, thresh=thresholds[\"below\"], threshold_direction=\"below\", img_name=img_name)\n    # Only above threshold is applicable\n    return _get_mask(image, thresh=thresholds[\"above\"], threshold_direction=\"above\", img_name=img_name)\n</code></pre>"},{"location":"api/utils/#topostats.utils.get_thresholds","title":"<code>get_thresholds(image: np.ndarray, threshold_method: str, otsu_threshold_multiplier: float = None, threshold_std_dev: dict = None, absolute: dict = None, **kwargs) -&gt; dict</code>","text":"<p>Obtain thresholds for masking data points.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>2D Numpy array of image to be masked</p> required <code>threshold_method</code> <code>str</code> <p>Method for thresholding, 'otsu', 'std_dev' or 'absolute' are valid options.</p> required <code>threshold_std_dev</code> <code>dict</code> <p>Dict of above and below thresholds for the standard deviation method.</p> <code>None</code> <code>absolute</code> <code>tuple</code> <p>Dict of below and above thresholds.</p> <code>None</code> <code>**kwargs</code> <code>{}</code> <p>Returns:</p> Type Description <code>Dict</code> <p>Dictionary of thresholds, contains keys 'below' and optionally 'above'.</p> Source code in <code>topostats\\utils.py</code> <pre><code>def get_thresholds(  # noqa: C901\n    image: np.ndarray,\n    threshold_method: str,\n    otsu_threshold_multiplier: float = None,\n    threshold_std_dev: dict = None,\n    absolute: dict = None,\n    **kwargs,\n) -&gt; dict:\n    \"\"\"Obtain thresholds for masking data points.\n\n    Parameters\n    ----------\n    image : np.ndarray\n        2D Numpy array of image to be masked\n    threshold_method : str\n        Method for thresholding, 'otsu', 'std_dev' or 'absolute' are valid options.\n    threshold_std_dev : dict\n        Dict of above and below thresholds for the standard deviation method.\n    absolute : tuple\n        Dict of below and above thresholds.\n    **kwargs:\n\n    Returns\n    -------\n    Dict\n        Dictionary of thresholds, contains keys 'below' and optionally 'above'.\n    \"\"\"\n    thresholds = defaultdict()\n    if threshold_method == \"otsu\":\n        thresholds[\"above\"] = threshold(image, method=\"otsu\", otsu_threshold_multiplier=otsu_threshold_multiplier)\n    elif threshold_method == \"std_dev\":\n        try:\n            if threshold_std_dev[\"below\"] is not None:\n                thresholds[\"below\"] = threshold(image, method=\"mean\") - threshold_std_dev[\"below\"] * np.nanstd(image)\n            if threshold_std_dev[\"above\"] is not None:\n                thresholds[\"above\"] = threshold(image, method=\"mean\") + threshold_std_dev[\"above\"] * np.nanstd(image)\n        except TypeError as typeerror:\n            raise typeerror\n    elif threshold_method == \"absolute\":\n        if absolute[\"below\"] is not None:\n            thresholds[\"below\"] = absolute[\"below\"]\n        if absolute[\"above\"] is not None:\n            thresholds[\"above\"] = absolute[\"above\"]\n    else:\n        if not isinstance(threshold_method, str):\n            raise TypeError(\n                f\"threshold_method ({threshold_method}) should be a string. Valid values : 'otsu' 'std_dev' 'absolute'\"\n            )\n        if threshold_method not in [\"otsu\", \"std_dev\", \"absolute\"]:\n            raise ValueError(\n                f\"threshold_method ({threshold_method}) is invalid. Valid values : 'otsu' 'std_dev' 'absolute'\"\n            )\n    return thresholds\n</code></pre>"},{"location":"api/utils/#topostats.utils.update_config","title":"<code>update_config(config: dict, args: dict | Namespace) -&gt; dict</code>","text":"<p>Update the configuration with any arguments.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Dictionary of configuration (typically read from YAML file specified with '-c/--config ') required <code>args</code> <code>dict | Namespace</code> <p>Command line arguments</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>Dictionary updated with command arguments.</p> Source code in <code>topostats\\utils.py</code> <pre><code>def update_config(config: dict, args: dict | Namespace) -&gt; dict:\n    \"\"\"Update the configuration with any arguments.\n\n    Parameters\n    ----------\n    config: dict\n        Dictionary of configuration (typically read from YAML file specified with '-c/--config &lt;filename&gt;')\n    args: Namespace\n        Command line arguments\n\n    Returns\n    -------\n    Dict\n        Dictionary updated with command arguments.\n    \"\"\"\n    args = vars(args) if isinstance(args, Namespace) else args\n\n    config_keys = config.keys()\n    for arg_key, arg_value in args.items():\n        if isinstance(arg_value, dict):\n            update_config(config, arg_value)\n        else:\n            if arg_key in config_keys and arg_value is not None:\n                original_value = config[arg_key]\n                config[arg_key] = arg_value\n                LOGGER.info(f\"Updated config config[{arg_key}] : {original_value} &gt; {arg_value} \")\n    if \"base_dir\" in config.keys():\n        config[\"base_dir\"] = convert_path(config[\"base_dir\"])\n    if \"output_dir\" in config.keys():\n        config[\"output_dir\"] = convert_path(config[\"output_dir\"])\n    return config\n</code></pre>"},{"location":"api/utils/#topostats.utils.update_plotting_config","title":"<code>update_plotting_config(plotting_config: dict) -&gt; dict</code>","text":"<p>Update the plotting config for each of the plots in plot_dict.</p> <p>Ensures that each entry has all the plotting configuration values that are needed.</p> Source code in <code>topostats\\utils.py</code> <pre><code>def update_plotting_config(plotting_config: dict) -&gt; dict:\n    \"\"\"Update the plotting config for each of the plots in plot_dict.\n\n    Ensures that each entry has all the plotting configuration values that are needed.\n    \"\"\"\n    main_config = plotting_config.copy()\n    for opt in [\"plot_dict\", \"run\"]:\n        main_config.pop(opt)\n    for image, options in plotting_config[\"plot_dict\"].items():\n        plotting_config[\"plot_dict\"][image] = {**options, **main_config}\n        # Make it so that binary images do not have the user-defined z-scale\n        # applied, but non-binary images do.\n        if plotting_config[\"plot_dict\"][image][\"image_type\"] == \"binary\":\n            plotting_config[\"plot_dict\"][image][\"zrange\"] = [None, None]\n\n    return plotting_config\n</code></pre>"},{"location":"api/validation/","title":"Validation Module","text":"<p>Validation of configuration.</p>"},{"location":"api/validation/#topostats.validation.validate_config","title":"<code>validate_config(config: dict, schema: Schema, config_type: str) -&gt; None</code>","text":"<p>Validate configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Config dictionary imported by read_yaml() and parsed through clean_config().</p> required <code>schema</code> <code>Schema</code> <p>A schema against which the configuration is to be compared.</p> required <code>config_type</code> <code>str</code> required Source code in <code>topostats\\validation.py</code> <pre><code>def validate_config(config: dict, schema: Schema, config_type: str) -&gt; None:\n    \"\"\"Validate configuration.\n\n    Parameters\n    ----------\n    config: dict\n        Config dictionary imported by read_yaml() and parsed through clean_config().\n    schema: Schema\n        A schema against which the configuration is to be compared.\n    config_type: str\n    \"\"\"\n    try:\n        schema.validate(config)\n        LOGGER.info(f\"The {config_type} is valid.\")\n    except SchemaError as schema_error:\n        raise SchemaError(\n            f\"There is an error in your {config_type} configuration. \"\n            \"Please refer to the first error message above for details\"\n        ) from schema_error\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>This document describes how to contribute to the development of this software.</p>"},{"location":"contributing/#contribution-workflow","title":"Contribution Workflow","text":""},{"location":"contributing/#create-an-issue","title":"Create an Issue","text":"<p>Before starting please search for and review the existing issues (both <code>open</code> and <code>closed</code>) and pull requests to see if anyone has reported the bug or requested the feature already or work is in progress. If nothing exists then you should create a new issue using one of the templates provided.</p>"},{"location":"contributing/#cloning-the-repository","title":"Cloning the repository","text":"<p>If you wish to make changes yourself you will have to fork the repository to your own account and then clone that if you are not a member of AFM-SPM Organisation. If you are a member then you can clone the repository and make contributions directly.</p> <pre><code># Member of AFM-SPM Organisation\ngit clone git@github.com:AFM-SPM/TopoStats.git\n# Non-member of AFM-SPM cloning fork\ngit clone git@github.com:&lt;YOUR_GITHUB_USERNAME&gt;/TopoStats.git\n</code></pre>"},{"location":"contributing/#install-additional-dependencies","title":"Install Additional Dependencies","text":"<p>If you are going to contribute you should install the additional dependencies for undertaking such work. There are three groups of additional dependencies, <code>dev</code>, <code>docs</code> and <code>tests</code> and you should install all three using <code>pip</code> as shown below.</p> <pre><code>cd TopoStats\npip install \".[dev,docs,tests]\"\n</code></pre>"},{"location":"contributing/#creating-a-branch","title":"Creating a branch","text":"<p>Typically you will now create a branch to work on the issue you wish to address. It is not compulsory but we try to use a consistent nomenclature for branches that shows who has worked on the branch, the issue it pertains to and a short description of the work. To which end you will see branches with the form <code>&lt;GITHUB_USERNAME&gt;/&lt;GITHUB_ISSUE&gt;-&lt;DESCRIPTION&gt;</code>. Some examples are shown below...</p> Branch User Issue Description <code>ns-rse/259-contributing</code> <code>ns-rse</code> 259 <code>contributing</code> is short for the issue subject Add contributing section to documentation. <code>SylviaWhittle/204-nanometre-scaling</code> <code>SylviaWhittle</code> 204 <code>nanometre-scaling</code> is short for the issue subject Colour scale in nanometers not pixels. <p>How you create a branch depends on how you use Git, some use the integration provided by their IDE, others dedicated clients such as GitKraken and some may use the command line interface. These instructions use the later but you are of course free to use your chosen method of managing Git and GitHub.</p> <p>In this example we branch from <code>dev</code> and create a new branch called <code>ns-rse/000-fix-an-issue</code>.</p> <pre><code># Ensure you are up-to-date on the main branch\ngit checkout main\ngit pull\n# Create and checkout a branch in one step\ngit checkout -b ns-rse/000-fix-an-issue\n# Create and checkout a branch in two steps\ngit branch dev ns-rse/000-fix-an-issue\ngit checkout ns-rse/000-fix-an-issue\n</code></pre> <p>You can now start working on your issue and making regular commits, but please bear in mind the following section on Coding Standards.</p>"},{"location":"contributing/#software-development","title":"Software Development","text":"<p>To make the codebase easier to maintain we ask that you follow the guidelines below on coding style, linting, typing, documentation and testing. These entail a number of additional dependencies that can be installed with the following command.</p> <pre><code>pip install -e .[dev,tests,docs]\n</code></pre> <p>This will pull in all the dependencies we use for development (<code>dev</code>), tests (<code>tests</code>) and writing documentation (<code>docs</code>)</p>"},{"location":"contributing/#coding-stylelinting","title":"Coding Style/Linting","text":"<p>Using a consistent coding style has many benefits (see Linting : What is all the fluff about?). For this project we aim to adhere to PEP8 - the style Guide for Python Code and do so using the formatting linters black and ruff. Ruff implements the checks made by Flake8, isort and pydocstyle and has some overlap with both Black and Pylint.</p> <p>We also like to ensure the code passes pylint which helps identify code duplication and reduces some of the code smells that we are all prone to making. A <code>.pylintrc</code> is included in the repository. Currently this isn't strictly applied but it is planned for part of the CI/CD pipeline and so we would be grateful if you could lint your code before making Pull Requests.</p> <p>Many popular IDEs such as VSCode, PyCharm, Spyder and Emacs all have support for integrating these linters into your workflow such that when you save a file the linting/formatting is automatically applied.</p>"},{"location":"contributing/#pre-commit","title":"Pre-commit","text":"<p>pre-commit is a powerful and useful tool that runs hooks on your code prior to making commits. For a more detailed exposition see pre-commit : Protecting your future self.</p> <p>The repository includes <code>pre-commit</code> as a development dependency as well as a <code>.pre-commit-config.yaml</code>. To use these locally you should have already installed all the <code>dev</code> dependencies in your virtual environment. You then need to install <code>pre-commit</code> configuration and hooks (NB this will download specific virtual environments that <code>pre-commit</code> uses when running hooks so the first time this is run may take a little while).</p> <pre><code>pre-commit install --install-hooks\n</code></pre> <p>Currently there are hooks to remove trailing whitespace, check YAML configuration files and a few other common checks as well as hooks for <code>black</code> and <code>ruff</code>. If these fail then you will not be able to make a commit until they are fixed. The <code>black</code> hook will automatically format failed files so you can simply <code>git add</code> those and try committing straight away. <code>flake8</code> does not correct files automatically so the errors will need manually correcting.</p> <p>If you do not enable and resolve issues reported by <code>pre-commit</code> locally before making a pull request you will find the <code>pre-commit.ci</code> GitHub Action will fail, preventing your Pull Request from being merged. You can shorten the feedback loop and speed up the resolution of errors by enabling <code>pre-commit</code> locally and resolving issues before making your commits.</p>"},{"location":"contributing/#typing","title":"Typing","text":"<p>Whilst Python is a dynamically typed language (that is the type of an object is determined dynamically) the use of Type Hints is strongly encouraged as it makes reading and understanding the code considerably easier for contributors. For more on Type Hints see PEP483 and PEP484</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>All classes, methods and functions should have Numpy Docstrings defining their functionality, parameters and return values and pylint will note and report the absence of docstrings by way of the <code>missing-function-docstring</code> condition.</p> <p>Further, when new methods are incorporated into the package that introduce changes to the configuration they should be documented under Parameter Configuration. pre-commit has the markdownlint-cli2 hook enabled to lint all Markdown files and will where possible automatically fix things, but some issues need resolving manually.</p>"},{"location":"contributing/#testing","title":"Testing","text":"<p>New features should have unit-tests written and included under the <code>tests/</code> directory to ensure the functions work as expected. The pytest framework is used for running tests along with a number of plugins (pytest-regtest for regression testing; pytest-mpl for testing generated Matplotlib images).</p> <p>In conjunction with pre-commit we leverage pytest-testmon to run tests on each commit, but as the test suite is large and can take a while to run <code>pytest-testmon</code> restricts tests to only files that have changed (code or tests) or changes in environment variables and dependencies. You will need to create a database locally on first run and so should run the following before undertaking any development.</p> <pre><code>pytest --testmon\n</code></pre> <p>This will create a database (<code>.testmondata</code>) which tracks the current state of the repository, this file is ignored by Git (via <code>.gitignore</code>) but keeps track of the state of the repository and what has changed so that the <code>pre-commit</code> hook <code>Pytest (testmon)</code> only attempts to run the tests when changes have been made to files that impact the tests.</p>"},{"location":"contributing/#debugging","title":"Debugging","text":"<p>To aid with debugging we include the snoop package. The package is disabled by default, but when you have a class, method or function you wish to debug you should add <code>snoop.install(enabled=True)</code> to the file you wish to debug and use the <code>@snoop</code> decorator around the function/method you wish to debug.</p>"},{"location":"contributing/#configuration","title":"Configuration","text":"<p>As described in Parameter Configuration options are primarily passed to TopoStats via a YAML configuration file. When introducing new features that require configuration options you will have to ensure that the default configuration file (<code>topostats/default.yaml</code>) is updated to include your options.</p> <p>Further the <code>topostats.validation.validate.config()</code> function, which checks a valid configuration file with all necessary fields has been passed when invoking <code>topostats</code> sub-commands, will also need updating to include new options in the Schema against which validation of configuration files is made.</p>"},{"location":"contributing/#ide-configuration","title":"IDE Configuration","text":"<p>Linters such as <code>black</code>, <code>flake8</code> and <code>pylint</code> can be configured to work with your IDE so that say Black and/or formatting is applied on saving a file or the code is analysed with <code>pylint</code> on saving and errors reported. Setting up and configuring IDEs to work in this manner is beyond the scope of this document but some links to articles on how to do so are provided.</p> <ul> <li>Linting Python in Visual Studio Code</li> <li>Code Analysis \u2014 Spyder for <code>pylint</code> for Black see How to use   code formatter Black with Spyder.</li> <li>Code Quality Assistance Tips and Tricks, or How to Make Your Code Look Pretty? |   PyCharm</li> <li>Reformat and rearrange code | PyCharm</li> </ul>"},{"location":"contributing/adding_modules/","title":"Adding Modules","text":""},{"location":"contributing/adding_modules/#basic-functionality","title":"Basic Functionality","text":"<p>This should reside in <code>topostats/</code> or optionally <code>topostats/&lt;sub_dir&gt;</code></p> <p>Unit tests should be written for each function, classes and method and integration tests to ensure the overall functionality is as expected now and in the future should refactoring be undertaken.</p>"},{"location":"contributing/adding_modules/#adding-configuration","title":"Adding configuration","text":"<p>Configuration options live in <code>topostats/default_config.yaml</code> you should create a nested object for the options corresponding to your module. The configuration nomenclature should match exactly the options of your modules class/function as this allows the <code>**kwargs</code> to be used to pass the options from the loaded dictionary to the function without having to explicitly map them to the class/function arguments. This might seem fickle or excessive but it saves you and others work in the long run.</p>"},{"location":"contributing/adding_modules/#modularity","title":"Modularity","text":"<p>TopoStats is a command line program (for now at least!) and the main command <code>topsotats</code> has sub-commands which allow individual steps of the processing to be undertaken, making it faster to test out the impact of different options. For example once loaded and saved as <code>.topostats</code> files the <code>filter</code> stage could be re-run to improve flattening. Once flattening is complete changes in configuration could be made to detect different grains or features using the already flattened images.</p> <p>Once you have a module that works it should ideally be included in this pipeline. Here we described how to include it.</p>"},{"location":"contributing/adding_modules/#processing","title":"Processing","text":"<p>Each module needs a processing function. This is defined the <code>topostats/processing.py</code> module and should be called from the <code>process_all()</code> function to include it in the end to end pipeline.</p>"},{"location":"contributing/adding_modules/#process_all","title":"<code>process_all</code>","text":"<p>All of these modules are run in order to process individual images in parallel and this is achieved via the <code>process_all()</code> function which has a parameter for a single image and the configuration dictionary and calls each module in turn.</p> <p>This is run in parallel using the <code>functools.partial()</code> and <code>Pool</code>. The former takes the <code>process_all()</code> function as an argument along with all other configuration options. This is combined with a list of images to be processed and run through the <code>Pool</code> class (see examples below).</p> <p>To add your functionality to this pipeline you should add the calls to your class and its method to the <code>process_all()</code> function.</p>"},{"location":"contributing/adding_modules/#modular-analyses","title":"Modular Analyses","text":"<p>It is however useful to be able to call the function in isolation, this means we can add in sub-commands to <code>topostats</code> so that we can experiment with different configurations without having to run the whole pipeline for all images in one big batch job.</p> <p>As an example we will look at the <code>Filter</code> class and how it is added as a sub-command.</p> <p>The <code>topostats.processing.run_filters()</code> command is the processing command that takes in the various options, instantiates an instance of the <code>Filters()</code> class and runs the various methods required to process the image. There are other classes such as <code>Grains()</code>, <code>Grainstats()</code>, <code>Tracing()</code> which form part of the pipeline.</p>"},{"location":"contributing/adding_modules/#adding-sub-command-arguments","title":"Adding Sub-Command Arguments","text":"<p>You need to add a sub-parser for your module to run it in isolation. To do this entries need to be made to <code>topostats/entry_point.py</code> which sets up <code>argparse</code>. At the top level the <code>create_parser()</code> function is defined where the <code>parser</code> has the main options shown by <code>topostats --help</code> are defined. Each sub-command has its own parser, the most commonly used is <code>process_parser</code> which defines the arguments to and the help shown by <code>topostats process --help</code>. A <code>filter_parser</code> subparser is added which defines the arguments and help shown by <code>topostats filter</code> and this has various arguments added to it with <code>.add_argument()</code>. Note it is important to ensure that the <code>dest</code> for each added argument matches the name used in the <code>default_config.yaml</code> as these values are used to update the loaded dictionary. If the names don't match the values do not get updated (and/or an error will occur).</p> <pre><code># Filter\nfilter_parser = subparsers.add_parser(\n    \"filter\",\n    description=\"Load and filter images, saving as .topostats files for subsequent processing.\",\n    help=\"Load and filter images, saving as .topostats files for subsequent processing.\",\n)\nfilter_parser.add_argument(\n    \"--row-alignment-quantile\",\n    dest=\"row_alignment_quantile\",\n    type=float,\n    required=False,\n    help=\"Lower values may improve flattening of larger features.\",\n)\nfilter_parser.add_argument(\n    \"--threshold-method\",\n    dest=\"threshold_method\",\n    type=str,\n    required=False,\n    help=\"Method for thresholding Filtering. Options are otsu, std_dev, absolute.\",\n)\nfilter_parser.add_argument(\n    \"--otsu-threshold-multiplier\",\n    dest=\"otsu_threshold_multiplier\",\n    type=float,\n    required=False,\n    help=\"Factor for scaling the Otsu threshold during Filtering.\",\n)\nfilter_parser.add_argument(\n    \"--threshold-std-dev-below\",\n    dest=\"threshold_std_dev_below\",\n    type=float,\n    required=False,\n    help=\"Threshold for data below the image background for std dev method during Filtering.\",\n)\n...\nfilter_parser.add_argument(\n    \"--scars-max-scar-length\",\n    dest=\"scars_max_scar_length\",\n    type=int,\n    required=False,\n    help=\"Maximum length of scars in pixels\",\n)\n# Run the relevant function with the arguments\nfilter_parser.set_defaults(func=run_modules.filters)\n</code></pre> <ol> <li>Create a subparser with <code>&lt;module_name&gt;_parser = subparsers.add_parser(...)</code>.</li> <li> <p>Add arguments to subparser with <code>&lt;module_name&gt;_parser.add_argument(...)</code>, include...</p> </li> <li> <p>Single letter flag (optional).</p> </li> <li>Longer flag (required).</li> <li><code>dest</code> the destination to which the variable will be saved. This should match the corresponding value in the   <code>default_config.yaml</code> which makes updating the configuration options straight-forward and will occur automatically   (the code is in place you don't need to add anything, just ensure the names match).</li> <li><code>type</code> should ideally be included as this helps define the type of the stored variable, particularly useful if for   example paths are provided which should be stored as <code>Path</code> pathlib objects.</li> <li><code>default</code> a sensible default value, typically <code>None</code> though so that the value in <code>default_config.yaml</code> is used.</li> <li><code>help</code> a useful description of what the configuration option changes along with possible options.</li> </ol> <ol> <li>Set the default function that will be called with <code>&lt;module_name&gt;_parser.set_defaults(func=run_modules.&lt;function&gt;)</code> which will call the function you have defined in <code>topostats/run_modules.py</code> which runs your module.</li> </ol> <p>NB In the above substitute <code>&lt;module_name&gt;</code> for the meaningful name you have created for your module.</p>"},{"location":"contributing/adding_modules/#running-in-parallel","title":"Running in Parallel","text":"<p>One of the main advantages of TopoStats is the ability to batch process images. As modern computers typically have multiple cores it means the processing can be done in parallel making the processing faster. To achieve this we use wrapper functions in <code>topostats/run_modules.py</code> that leverage the <code>Pool</code> multiprocessing class.</p> <p>Your function should first load the <code>.topostats</code> files you wish to process that are found from the user specified path (default being the current directory <code>./</code>).</p> <p>You then need to define a <code>partial()</code> function with the <code>topostats/processing.py&lt;your_module_processing_function&gt;</code> as the first argument and the remaining configuration options. These will typically be a subset/list from the <code>default_config.yaml</code> where the configuration options use the exact same names as the arguments of the function you defined your <code>topostats/processing.py</code>. As mentioned above keeping configuration names consistent between configuration files and functions means <code>**kwargs</code> can be used when passing options to functions.</p> <p>Continuing with our example let's look at the [<code>topostats.processing.run_filters()</code>][topostats_entry_point_filters] function.</p> <pre><code>def run_filters(\n    unprocessed_image: npt.NDArray,\n    pixel_to_nm_scaling: float,\n    filename: str,\n    filter_out_path: Path,\n    core_out_path: Path,\n    filter_config: dict,\n    plotting_config: dict,\n) -&gt; npt.NDArray | None:\n    \"\"\"\n    Filter and flatten an image. Optionally plots the results, returning the flattened image.\n\n    Parameters\n    ----------\n    unprocessed_image : npt.NDArray\n        Image to be flattened.\n    pixel_to_nm_scaling : float\n        Scaling factor for converting pixel length scales to nanometres.\n        ie the number of pixels per nanometre.\n    filename : str\n        File name for the image.\n    filter_out_path : Path\n        Output directory for step-by-step flattening plots.\n    core_out_path : Path\n        General output directory for outputs such as the flattened image.\n    filter_config : dict\n        Dictionary of configuration for the Filters class to use when initialised.\n    plotting_config : dict\n        Dictionary of configuration for plotting output images.\n\n    Returns\n    -------\n    npt.NDArray | None\n        Either a numpy array of the flattened image, or None if an error occurs or\n        flattening is disabled in the configuration.\n    \"\"\"\n    if filter_config[\"run\"]:\n        filter_config.pop(\"run\")\n        LOGGER.debug(f\"[{filename}] Image dimensions: {unprocessed_image.shape}\")\n        LOGGER.info(f\"[{filename}] : *** Filtering ***\")\n        filters = Filters(\n            image=unprocessed_image,\n            filename=filename,\n            pixel_to_nm_scaling=pixel_to_nm_scaling,\n            **filter_config,\n        )\n        filters.filter_image()\n        # Optionally plot filter stage\n        if plotting_config[\"run\"]:\n            plotting_config.pop(\"run\")\n            LOGGER.info(f\"[{filename}] : Plotting Filtering Images\")\n            if plotting_config[\"image_set\"] == \"all\":\n                filter_out_path.mkdir(parents=True, exist_ok=True)\n                LOGGER.debug(\n                    f\"[{filename}] : Target filter directory created : {filter_out_path}\"\n                )\n            # Generate plots\n            for plot_name, array in filters.images.items():\n                if plot_name not in [\"scan_raw\"]:\n                    if plot_name == \"extracted_channel\":\n                        array = np.flipud(array.pixels)\n                    plotting_config[\"plot_dict\"][plot_name][\"output_dir\"] = (\n                        core_out_path\n                        if plotting_config[\"plot_dict\"][plot_name][\"core_set\"]\n                        else filter_out_path\n                    )\n                    try:\n                        Images(\n                            array, **plotting_config[\"plot_dict\"][plot_name]\n                        ).plot_and_save()\n                        Images(\n                            array, **plotting_config[\"plot_dict\"][plot_name]\n                        ).plot_histogram_and_save()\n                    except AttributeError:\n                        LOGGER.info(\n                            f\"[{filename}] Unable to generate plot : {plot_name}\"\n                        )\n            plotting_config[\"run\"] = True\n        # Always want the 'z_threshed' plot (aka \"Height Thresholded\") but in the core_out_path\n        plot_name = \"z_threshed\"\n        plotting_config[\"plot_dict\"][plot_name][\"output_dir\"] = core_out_path\n        Images(\n            filters.images[\"gaussian_filtered\"],\n            filename=filename,\n            **plotting_config[\"plot_dict\"][plot_name],\n        ).plot_and_save()\n        LOGGER.info(f\"[{filename}] : Filters stage completed successfully.\")\n        return filters.images[\"gaussian_filtered\"]\n    # Otherwise, return None and warn that initial processing is disabled.\n    LOGGER.error(\n        \"You have not included running the initial filter stage. This is required for all subsequent \"\n        \"stages of processing. Please check your configuration file.\"\n    )\n    return None\n</code></pre> <p>This instantiates (creates) the object <code>filters</code> of the class <code>Filters</code> with the supplied options and then runs the <code>filter_image()</code> method to perform the filtering. The rest of the code determines what images to plot based on the configuration. At the end the <code>gaussian_filtered</code> image is returned.</p> <p>This function only processes a single image. As mentioned we use the <code>functools.partial()</code> function to define a function with the command we want to run, in this case <code>topostats.processing.run_filters()</code> (imported as just <code>filters()</code>) as the first argument and then all of the parameters we would normally pass in to <code>filters()</code> as the remainder arguments. This is then used with the <code>Pool</code> class and given a list of the images that are to be processed and the <code>partial</code>ly defined function is run with each image.</p> <p>In our example the <code>run_modules.filters()</code> this looks like the following.</p> <pre><code>def filters(args: argparse.Namespace | None = None) -&gt; None:\n    \"\"\"\n    Load files from disk and run filtering.\n\n    Parameters\n    ----------\n    args : None\n        Arguments.\n    \"\"\"\n    config, img_files = _parse_configuration(args)\n    # If loading existing .topostats files the images need filtering again so we need to extract the raw image\n    if config[\"file_ext\"] == \".topostats\":\n        config[\"loading\"][\"extract\"] = \"raw\"\n    all_scan_data = LoadScans(img_files, **config[\"loading\"])\n    all_scan_data.get_data()\n\n    processing_function = partial(\n        process_filters,\n        base_dir=config[\"base_dir\"],\n        filter_config=config[\"filter\"],\n        plotting_config=config[\"plotting\"],\n        output_dir=config[\"output_dir\"],\n    )\n\n    with Pool(processes=config[\"cores\"]) as pool:\n        results = defaultdict()\n        with tqdm(\n            total=len(img_files),\n            desc=f\"Processing images from {config['base_dir']}, results are under {config['output_dir']}\",\n        ) as pbar:\n            for img, result in pool.imap_unordered(\n                processing_function,\n                all_scan_data.img_dict.values(),\n            ):\n                results[str(img)] = result\n                pbar.update()\n\n                # Display completion message for the image\n                LOGGER.info(f\"[{img}] Filtering completed.\")\n\n    # Write config to file\n    config[\"plotting\"].pop(\"plot_dict\")\n    write_yaml(config, output_dir=config[\"output_dir\"])\n    LOGGER.debug(f\"Images processed : {len(results)}\")\n    # Update config with plotting defaults for printing\n    completion_message(\n        config, img_files, summary_config=None, images_processed=sum(results.values())\n    )\n</code></pre> <p>The files that are to be processed are first loaded to give the list of images that need processing and the <code>partial()</code> function is defined as <code>processing_function</code> with all of the arguments before running in parallel using <code>Pool</code>. The <code>tqdm</code> package is leverage to give a progress bar and after completion the configuration file is written to file before a completion message is run.</p>"},{"location":"contributing/adding_modules/#conclusion","title":"Conclusion","text":"<p>Adding functionality is useful but it has to integrate into the workflow and ideally be accessible as a stand alone step in the process. Hopefully the above helps demystify the steps required to achieve this.</p>"},{"location":"usage/","title":"Getting Started","text":"<p>After having installed TopoStats you are ready to run it. For convenience TopoStats provides a command line interface <code>topostats</code> that will load a default configuration file and process all images with reasonable default configuration options.</p> <p>However, because the location of your image files can not be known in advance you must make a copy of the default configuration and modify it to work with your files. This guide will hopefully take you through the process of running TopoStats and customising the configuration file with which it is run. If you encounter any problems please ask questions in the Discussions. If you think you have encountered a bug or have a feature suggestion please create an Issue.</p>"},{"location":"usage/#organising-scans","title":"Organising Scans","text":"<p>You should place all files you wish to batch process in a single directory. They can be nested in separate folders as TopoStats will scan for all images within this directory but currently it will only process one scan type at a time (i.e. <code>.spm</code> or <code>.jpk</code> or <code>.asd</code>). This may change in the future.</p>"},{"location":"usage/#command-line-navigation","title":"Command Line Navigation","text":"<p>TopoStats currently runs as a command-line programme. To use it you will have to use a \"prompt\" or \"terminal\" (they're essentially the same thing). What you use will depend on your operating system, but the following are some simple commands on navigation. If you use Windows then for consistency it is recommended to install and use PowerShell.</p> <p>At the command line you use <code>cd</code> to <code>c</code>hange <code>d</code>irectory to the location of your files. For example if your scans are on the C-drive in <code>C:\\User\\me\\work\\spm\\2022-12-08\\scans</code> then you would</p> <pre><code>cd c:/User/me/work/spm/2022-12-08/scans\n</code></pre> <p>If you are on a Linux or OSX system then paths are not prefixed with letters and your files may be saved to <code>/home/me/work/spm/2022-12-08/scans</code>. To change directory there you would...</p> <pre><code>cd /home/me/work/spm/2022-12-08/scans\n</code></pre> <p>NB - Always use a forward-slash (<code>/</code>) when typing directory paths. Windows will display back-slash (<code>\\</code>) but understands forward-slash. Under Linux and OSX they mean different things and so you should always use forward-slash (<code>/</code>).</p> <p>You can always find out what location you are at in the command line using the <code>pwd</code> command (<code>p</code>rint <code>w</code>orking <code>d</code>irectory) and it will print out the directory you are currently at.</p> <pre><code>pwd\n/home/me/work/spm/2022-12-08/scans\n</code></pre> <p>To navigate up one directory level use <code>cd ..</code>. These can be chained together and directories separated with <code>/</code>.</p> <pre><code># Move up a single directory level\ncd ..\npwd\n/home/me/work/spm/2022-12-08\n# Move up another two directory levels\ncd ../../\npwd\n/home/me/\n</code></pre> <p>You can list the files in a directory using the <code>ls</code> command.</p> <pre><code>ls\nsample_image_scan_2022-12-08-1204.spm\n</code></pre> <p>To learn more about the command line see the Introduction to the Command Line for Genomics.</p>"},{"location":"usage/#running-topostats","title":"Running TopoStats","text":"<p>The default location that TopoStats looks for scans is the directory from which it is invoked. Once you start your shell/terminal you will therefore need to do two things.</p> <ol> <li>Navigate to the location of the scans you wish to process using <code>cd /path/to/where/scans/are/located</code>.</li> <li>Activate the virtual environment under which you installed TopoStats (refer to installed if unsure).</li> </ol> <p>You can now run topostats by invoking <code>topostats process</code> and you should start to see some output similar to that below.</p> <pre><code>cd /path/to/where/scans/are/located\ntopostats process\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Configuration is valid.\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Plotting configuration is valid.\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Configuration file loaded from      : None\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Scanning for images in              : /home/neil/work/projects/topostats/TopoStats\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Output directory                    : output\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Looking for images with extension   : .spm\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Images with extension .spm in /home/neil/work/projects/topostats/TopoStats : 32\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Thresholding method (Filtering)     : std_dev\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Thresholding method (Grains)        : std_dev\n...\n</code></pre> <p>On a successful completion you should see a message similar to the following which indicates various aspects of the run along with information about how to give feedback, report bugs and cite the software.</p> <pre><code>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n  _______      _____      __ __       _____     ______    _______      _____      _______    ______\n/\\_______)\\   ) ___ (    /_/\\__/\\    ) ___ (   / ____/\\ /\\_______)\\   /\\___/\\   /\\_______)\\ / ____/\\\n\\(___  __\\/  / /\\_/\\ \\   ) ) ) ) )  / /\\_/\\ \\  ) ) __\\/ \\(___  __\\/  / / _ \\ \\  \\(___  __\\/ ) ) __\\/\n  / / /     / /_/ (_\\ \\ /_/ /_/ /  / /_/ (_\\ \\  \\ \\ \\     / / /      \\ \\(_)/ /    / / /      \\ \\ \\\n ( ( (      \\ \\ )_/ / / \\ \\ \\_\\/   \\ \\ )_/ / /  _\\ \\ \\   ( ( (       / / _ \\ \\   ( ( (       _\\ \\ \\\n  \\ \\ \\      \\ \\/_\\/ /   )_) )      \\ \\/_\\/ /  )____) )   \\ \\ \\     ( (_( )_) )   \\ \\ \\     )____) )\n  /_/_/       )_____(    \\_\\/        )_____(   \\____\\/    /_/_/      \\/_/ \\_\\/    /_/_/     \\____\\/\n\n\n[Thu, 14 Nov 2024 09:40:56] [INFO    ] [topostats]\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ COMPLETE ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n  Base Directory              : /home/neil/work/projects/topostats/TopoStats\n  File Extension              : .spm\n  Files Found                 : 1\n  Successfully Processed      : 1 (100.0%)\n  Configuration               : output/config.yaml\n  All statistics              : output/all_statistics.csv\n  Distribution Plots          : output/summary_distributions\n\n  Email                       : topostats@sheffield.ac.uk\n  Documentation               : https://afm-spm.github.io/topostats/\n  Source Code                 : https://github.com/AFM-SPM/TopoStats/\n  Bug Reports/Feature Request : https://github.com/AFM-SPM/TopoStats/issues/new/choose\n  Citation File Format        : https://github.com/AFM-SPM/TopoStats/blob/main/CITATION.cff\n\n  If you encounter bugs/issues or have feature requests please report them at the above URL\n  or email us.\n\n  If you have found TopoStats useful please consider citing it. A Citation File Format is\n  linked above and available from the Source Code page.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n</code></pre>"},{"location":"usage/#help-options","title":"Help Options","text":"<p>The main <code>topostats</code> programme has a number of flags which can be specified to change the behaviour of how the programme runs. You can view the possible options by supplying the <code>-h</code> or <code>--help</code> flag.</p> <pre><code>\u2771 topostats --help\nusage: topostats [-h] [-v] [-c CONFIG_FILE] [-s SUMMARY_CONFIG] [--matplotlibrc MATPLOTLIBRC] [-b BASE_DIR] [-o OUTPUT_DIR] [-l LOG_LEVEL] [-j CORES] [-f FILE_EXT] [--channel CHANNEL] {process,load,filter,grains,grainstats,disordered-tracing,nodestats,ordered-tracing,splining,summary,create-config,create-matplotlibrc} ...\n\nRun various programs relating to AFM data. Add the name of the program you wish to run.\n\noptions:\n  -h, --help            show this help message and exit\n  -v, --version         Report the current version of TopoStats that is installed\n  -c CONFIG_FILE, --config-file CONFIG_FILE\n                        Path to a YAML configuration file.\n  -s SUMMARY_CONFIG, --summary-config SUMMARY_CONFIG\n                        Path to a YAML configuration file for summary plots and statistics.\n  --matplotlibrc MATPLOTLIBRC\n                        Path to a matplotlibrc file.\n  -b BASE_DIR, --base-dir BASE_DIR\n                        Base directory to scan for images.\n  -o OUTPUT_DIR, --output-dir OUTPUT_DIR\n                        Output directory to write results to.\n  -l LOG_LEVEL, --log-level LOG_LEVEL\n                        Logging level to use, default is 'info' for verbose output use 'debug'.\n  -j CORES, --cores CORES\n                        Number of CPU cores to use when processing.\n  -f FILE_EXT, --file-ext FILE_EXT\n                        File extension to scan for.\n  --channel CHANNEL     Channel to extract.\n\nprogram:\n  Available programs, listed below:\n\n  {process,load,filter,grains,grainstats,disordered-tracing,nodestats,ordered-tracing,splining,summary,create-config,create-matplotlibrc}\n    process             Process AFM images. Additional arguments over-ride defaults or those in the configuration file.\n    load                Load and save all images as .topostats files for subsequent processing.\n    filter              WIP DO NOT USE - Load and filter images, saving as .topostats files for subsequent processing.\n    grains              WIP DO NOT USE - Load filtered images from '.topostats' files and detect grains.\n    grainstats          WIP DO NOT USE - Load images with grains from '.topostats' files and calculate statistics.\n    disordered-tracing  WIP DO NOT USE - Skeletonise and prune objects to disordered traces.\n    nodestats           WIP DO NOT USE - Calculate node statistics and disentangle molecules.\n    ordered-tracing     WIP DO NOT USE - Ordered traces of pruned skeletons.\n    splining            WIP DO NOT USE - Splining of traced molecules to produce smooth curves.\n    summary             Plotting and summary of TopoStats output statistics.\n    create-config       Create a configuration file using the defaults.\n    create-matplotlibrc\n                        Create a Matplotlibrc parameters file using the defaults.\n</code></pre> <p>The global flags/options for modifying behaviour are listed. You then need to provide the name of the programme you wish to run which are listed at the bottom of the output along with a description.</p> <p>Each sub-programme has its own specific set of options too which can be specified to override the settings in the configuration file that is loaded (either the default or the user specified configuration). To view these again use the <code>-h</code> or <code>--help</code> flag. For a more detailed description of the options see the configuration page.</p> <pre><code> \u2771 topostats create-config --help\nusage: topostats create-config [-h] [-f FILENAME] [-o OUTPUT_DIR] [-c CONFIG] [-s]\n\nCreate a configuration file using the defaults.\n\noptions:\n  -h, --help            show this help message and exit\n  -f FILENAME, --filename FILENAME\n                        Name of YAML file to save configuration to (default 'config.yaml').\n  -o OUTPUT_DIR, --output-dir OUTPUT_DIR\n                        Path to where the YAML file should be saved (default './' the current directory).\n  -c CONFIG, --config CONFIG\n                        Configuration to use, currently only one is supported, the 'default'.\n  -s, --simple          Create a simple configuration file with only the most common options.\n</code></pre> <p>NB The <code>process</code> programme has a lot of options as it runs the processing pipeline in full.</p>"},{"location":"usage/#reducing-output","title":"Reducing Output","text":"<p>If you find the output too verbose or of no use you can reduce it by setting the <code>log_level</code> to either <code>error</code> or <code>warning</code>. This can be done either in the configuration file (see Configuration below) or using the <code>-l</code>/<code>--log-level</code> flag for example <code>topostats process --log_level warning</code>.</p>"},{"location":"usage/#configuring-topostats","title":"Configuring TopoStats","text":"<p>Configuration of TopoStats is done through a YAML file and a full description of the fields used can be found under the configuration section.</p> <p>Here we will go through generating a configuration file to edit and some of the common changes that you are likely to want to make to the default configuration and how to make them.</p>"},{"location":"usage/#generating-configuration-file","title":"Generating Configuration File","text":"<p>TopoStats will use some reasonable default parameters by default, but typically you will want to customise the parameters that are used. This is achieved using a configuration file. This is a YAML file that contains parameters for different settings. For convenience you can generate a sample configuration file in your current working directory using the <code>topostats create-config</code> sub-command. It takes a single argument, the name of the file to save the configuration to (e.g. <code>config.yaml</code> or <code>settings.yaml</code>), and it will write the current default configuration to that file.</p> <pre><code>topostats create-config --filename my_config.yaml\nls -l\nmy_config.yaml\nsample_image_scan_2022-12-08-1204.spm\n</code></pre> <p>You can now edit and/or rename the <code>my_config.yaml</code>. It can be called anything you want, e.g. <code>todays_first_run_configuration.yaml</code> is a valid name.</p>"},{"location":"usage/#editing-configyaml","title":"Editing <code>config.yaml</code>","text":"<p>IMPORTANT This file is an ASCII text file and you should use NotePad (Windows), TextEdit (OSX) or Nano/Emacs/Vim (GNU/Linux) or any other text editor. Do not use Microsoft Word or any other Word Processor to edit this file.</p> <p>You can now start customising the configuration you are going to run TopoStats with. All fields have defaults but the ones you may want to change are....</p> <ul> <li><code>base_dir</code> (default: <code>./</code>) the directory in which to search for scans. By default this is <code>./</code> which represents the   directory from which <code>topostats process</code> is called and it is good practice to have one configuration file per batch of   scans that are being processed.</li> <li><code>output_dir</code> (default: <code>output</code>) the location where the output is saved, by default this is the directory <code>output</code>   which will be created if it doesn't exist. If you wish for the output to be somewhere else specify it here. If you   want <code>Processed</code> directories to sit within the directories that images are found then simply set the <code>output_dir</code> to   the same value as <code>base_dir</code>.</li> <li><code>log_level</code> (default: <code>info</code>) the verbosity of output to the console and log file, the options in order of verbosity   are <code>debug</code> &gt; <code>info</code> &gt; <code>warning</code> &gt; <code>error</code>. If you want less output set to <code>warning</code> or <code>error</code>. If you encounter   errors please set to <code>debug</code> and run again and include the log in your bug   report.</li> <li><code>cores</code> (default: <code>2</code>) the number of parallel processes to run processing of all found images. Set this to a maximum   of one less than the number of cores on your computers CPU. If unsure leave as is, but chances are you can increase   this to at least <code>4</code> quite safely.</li> <li><code>file_ext</code> (default: <code>.spm</code>) the file extension of scans to search for within the current directory. The default is   <code>.spm</code> but other file format support is in the pipeline.</li> <li><code>plotting</code> : <code>image_set</code> (default <code>core</code>) specifies which steps of the processing to plot images of. The value <code>all</code>   gets images for all stages, <code>core</code> saves only a subset of images.</li> </ul> <p>Most of the other configuration options can be left on their default values for now. Once you have made any changes save the file and return to your terminal.</p>"},{"location":"usage/#running-topostats-with-my_configyaml","title":"Running TopoStats with <code>my_config.yaml</code>","text":"<p>To use your new configuration file you need to inform <code>topostats process</code> to use that file rather than the defaults, this is done using the <code>--config config.yaml</code> file.</p> <p>NB this assumes that you are in the same directory as your scans where you have saved the <code>my_config.yaml</code> file that you edited. That doesn't have to be the case but it makes life easier for if you are not familiar with absolute and relative paths.</p> <pre><code>topostats --config my_config.yaml process\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Configuration is valid.\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Plotting configuration is valid.\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Configuration file loaded from      : None\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Scanning for images in              : /home/neil/work/projects/topostats/TopoStats\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Output directory                    : output\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Looking for images with extension   : .spm\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Images with extension .spm in /home/neil/work/projects/topostats/TopoStats : 1\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Thresholding method (Filtering)     : std_dev\n[Tue, 15 Nov 2022 12:39:48] [INFO    ] [topostats] Thresholding method (Grains)        : std_dev\n...\n</code></pre> <p>On successful completion you should see the same message noted above.</p>"},{"location":"usage/#output","title":"Output","text":"<p>The output from running TopoStats is saved in the location defined in the configuration file by <code>output_dir</code>. The default is the directory <code>output</code> within the directory from which <code>topostats process</code>. This may differ if you have used your own customised configuration file (specifically if you have modified the <code>output_dir:</code> option).</p> <p>At the top level of the output directory are a few files produced:</p> <ul> <li><code>config.yaml</code> : a copy of the configuration used to process the images.</li> <li><code>all_statistics.csv</code> : a Comma Separated Variable ASCII plain-text file of the grain statistics.</li> <li><code>all_disordered_segment_statistics.csv</code> : a Comma Separated Variable ASCII plain-text file of the branched skeleton   statistics.</li> <li><code>all_mol_statistics.csv</code> : a Comma Separated Variable ASCII plain-text file of the molecule statistics.</li> </ul> <p>Note: - If all grains / branch segments of a column have a <code>None</code> or <code>NaN</code> value, the column will not be present in the output <code>.csv</code> file.</p> <p>The remaining directories of results is contingent on the structure of files within the <code>base_dir</code> that is specified in the configuration. If all files are in the top-level directory (i.e. no nesting) then you will have just a <code>Processed</code> directory. If there is a nested structure then there will be a <code>Processed</code> directory in each folder that an image with the specified <code>file_ext</code> has been found. This is perhaps best illustrated by way of example.</p> <p>If you have the following three <code>.spm</code> files within your current directory, one at the top level, one under <code>level1</code> and one under <code>level1/a</code>...</p> <pre><code>[4.0K Nov 15 13:55]  .\n|-- [4.0K Nov 15 13:54]  ./level1\n|   |-- [4.0K Nov 15 13:54]  ./level1/a\n|   |-- [ 32M Nov 15 13:54]  ./level1/a/minicircle.spm\n|   |-- [ 32M Nov 15 13:54]  ./level1/minicircle.spm\n|-- [ 32M Nov 15 13:54]  ./minicircle.spm\n</code></pre> <p>...then under <code>output</code> (the default for<code>output_dir</code>) you will see the following directory structure.</p> <pre><code>[4.0K Nov 15 14:06]  output\n|-- [ 381 Nov 15 14:06]  output/all_statistics.csv\n|-- [ 733 Nov 15 14:06]  output/all_disordered_tracing_statistics.csv\n|-- [ 254 Nov 15 14:06]  output/all_mol_statistics.csv\n|-- [7.4K Nov 15 14:06]  output/config.yaml\n|-- [ 222 Nov 15 14:06]  output/image_stats.csv\n|-- [4.0K Nov 15 14:06]  output/level1\n|   |-- [4.0K Nov 15 14:06]  output/level1/a\n|   |   |-- [4.0K Nov 15 14:06]  output/level1/a/Processed\n|   |-- [4.0K Nov 15 14:06]  output/level1/Processed\n|-- [4.0K Nov 15 14:06]  output/Processed\n</code></pre> <p>...where there is one <code>Processed</code> directory at the sub-directory level that each image was found.</p> <p>NB If you want <code>Processed</code> directories to sit within the directories that images are found then simply set the <code>output_dir</code> to the same value as <code>base_dir</code>.</p> <p>Within each <code>Processed</code> directory is a directory for each file found with the specified <code>file_ext</code> and within these are the resulting images from processing scans. If the <code>plotting</code> : <code>image_set</code> is <code>core</code> then there is a single image for each. If this option is <code>all</code> then there is also a sub-directory for each image found within which there are the directories <code>filters</code>, <code>grains/below</code> and <code>grains/above</code> which contain additional images from the processing stages and an accompanying histogram for each image showing the distribution of pixel heights for that image.</p>"},{"location":"usage/#summary-plots","title":"Summary Plots","text":"<p>By default TopoStats will take the data that has been summarised across all files and generate a series of plots, histograms with Kernel Density Estimates (KDE) overlaid and Violin plots. The default location of these if no custom configuration file is used is <code>output/summary_distributions</code>. If you have used a custom configuration file it will be the sub-directory <code>summary_distributions</code> nested under the directory specified for the <code>output</code>, e.g. if you used the current directory as output you will have a <code>summary_distributions</code> directory present.</p> <p>Sometimes you may have a <code>all_statistics.csv</code> from a run and wish to plot distributions of additional statistics that were not already plotted. This can be achieved using the command line programme <code>toposum</code> which is included.</p> <p>NB Because of the inherent complexity of plots this script is, by design, limited in the scope to which plots can be configured. It uses the plotting library Seaborn (which is built on top of Matplotlib) to produce basic plots, which are not intended for publication. If you want to tweak or customise plots it is recommended to load <code>all_statistics.csv</code> into a Jupyter Notebook and generate the plots you want there. A sample notebook is included to show how to do this.</p>"},{"location":"usage/#configuring-summary-plots","title":"Configuring Summary Plots","text":"<p>Configuration of summary plots is also via a YAML configuration file a description of the fields can be found under configuration page. You can generate a sample configuration by invoking the <code>--create-config-file</code> option to <code>toposum</code></p> <pre><code>toposum --create-config-file custom_summary_config.yaml\n</code></pre> <p>The file <code>custom_summary_config.yaml</code> can then be edited to change what plots are generated, where they are saved to and so forth. Typically you will only want to adjust a few settings such as toggling the types of plots (<code>hist</code>, <code>kde</code> and <code>violin</code>), the number of <code>bins</code> in a histogram or the statistic to plot in histograms (<code>count</code>, <code>frequency</code> etc.). You can change the <code>palette</code> that is used by Seaborn and crucially toggle which statistics are summarised by commenting and uncommenting the statistic names under <code>stats_to_sum</code>.</p>"},{"location":"usage/#labels","title":"Labels","text":"<p>Labels for the plots are generated from the file <code>topostats/var_to_label.yaml</code> which provides a dictionary that maps the variable name as the dictionary <code>key</code> to its description stored in the dictionary <code>value</code>. If you wish to customise these you can do so and pass it to <code>toposum</code> using the <code>--plotting_dictionary</code> which takes as an argument the path to the file you have created.</p>"},{"location":"usage/#pickles","title":"Pickles","text":"<p>The option <code>pickle_plots: True</code> will save to the specified <code>output_dir</code> the file <code>distribution_plots.pkl</code> which is a binary format that saves the plots that have been generated and saved in nested dictionaries so that they can be loaded again. The Notebook <code>notebooks/02-Summary-statistics-and-plots.ipynb</code> shows how to load these and make simple modifications to the the plots.</p>"},{"location":"usage/configuration/","title":"Configuration","text":"<p>Configuration for TopoStats is done using a YAML configuration file that is specified on the command line when invoking. If no configuration file is provided this default configuration is loaded automatically and used.</p> <p>The current configuration file is provided in the TopoStats repository at <code>topostats/default_config.yaml</code> but please be aware this may not work with your installed version, particularly if you installed from PyPI.</p>"},{"location":"usage/configuration/#generating-a-configuration","title":"Generating a configuration","text":"<p>You can always generate a configuration file appropriate for the version you have installed (bar v2.0.0 as this option was added afterwards). This writes the default configuration to the specified filename (i.e. it does not have to be called <code>config.yaml</code> it could be called <code>spm-2023-02-20.yaml</code>). There are a few options available (use <code>topostats create-config --help</code> for further details).</p> <pre><code>topostats create-config\n</code></pre>"},{"location":"usage/configuration/#partial-configurations","title":"Partial configurations","text":"<p>TopoStats supports using a partial configuration, where you specify only the fields you wish to override. This is useful if you only want to change a few parameters from the default configuration or would like to use a configuration file that is smaller and easier to read.</p> <p>To create a partial configuration file, simply create a new config file and delete anything you don't want to override.</p> <p>TopoStats will take the partial configuration file and merge it with the default configuration file, with the partial configuration taking precedence. This means that any fields you specify in the partial configuration will override the default configuration, while any fields you don't specify will be taken from the default configuration. Command-line arguments will override both the default and partial configurations.</p> <p>For example, you could use a configuration as simple as:</p> <pre><code>base_dir: ./mydata/\noutput_dir: ./myoutput/\nfilter:\n  remove_scars:\n    run: true\ngrains:\n  threshold_method: absolute\n  threshold_absolute:\n    above: 1.2\n  absolute_area_threshold:\n    above: [400, 1000]\n</code></pre>"},{"location":"usage/configuration/#using-a-custom-configuration","title":"Using a custom configuration","text":"<p>If you have generated a configuration file you can modify and edit a configuration it to change the parameters (see fields below). Once these changes have been saved, you can run TopoStats with this configuration file as shown below.</p> <pre><code>topostats --config my_config.yaml process\n</code></pre> <p>On completion a copy of the configuration that was used is written to the output directory so you have a record of the parameters used to generate the results you have. This file can be used in subsequent runs of TopoStats.</p>"},{"location":"usage/configuration/#yaml-structure","title":"YAML Structure","text":"<p>YAML files have key and value pairs, the first word, e.g. <code>base_dir</code> is the key this is followed by a colon to separate it from the value that it takes, by default <code>base_dir</code> takes the value <code>./</code> (which means the current directory) and so the entry in the file is a single line with <code>base_dir: ./</code>. Other data structures are available in YAML files including nested values and lists.</p> <p>A list in YAML consists of a key (e.g. <code>above:</code>) followed by the values in square brackets separated by commas such as <code>above: [ 500, 800 ]</code>. This means the <code>above</code> key is a list of the values <code>500</code> and <code>800</code>. Long lists can be split over separate lines as shown below</p> <pre><code>above:\n  - 100\n  - 200\n  - 300\n  - 400\n</code></pre>"},{"location":"usage/configuration/#fields","title":"Fields","text":"<p>Aside from the comments in YAML file itself the fields are described below.</p> Section Sub-Section Data Type Default Description <code>base_dir</code> string <code>./</code> Directory to recursively search for files within. (See Absolute v Relative Paths) <code>output_dir</code> string <code>./output</code> Directory that output should be saved to. (See Absolute v Relative Paths) <code>log_level</code> string <code>info</code> Verbosity of logging, options are (in increasing order) <code>warning</code>, <code>error</code>, <code>info</code>, <code>debug</code>. <code>cores</code> integer <code>2</code> Number of cores to run parallel processes on. <code>file_ext</code> string <code>.spm</code> File extensions to search for. <code>loading</code> <code>channel</code> string <code>Height</code> The channel of data to be processed, what this is will depend on the file-format you are processing and the channel you wish to process. <code>extract</code> string <code>raw</code> The array to extract when loading from <code>.topostats</code> images. <code>filter</code> <code>run</code> boolean <code>true</code> Whether to run the filtering stage, without this other stages won't run so leave as <code>true</code>. <code>threshold_method</code> str <code>std_dev</code> Threshold method for filtering, options are <code>ostu</code>, <code>std_dev</code> or <code>absolute</code>. <code>otsu_threshold_multiplier</code> float <code>1.0</code> Factor by which the derived Otsu Threshold should be scaled. <code>threshold_std_dev</code> dictionary <code>10.0, 1.0</code> A pair of values that scale the standard deviation, after scaling the standard deviation <code>below</code> is subtracted from the image mean to give the below/lower threshold and the <code>above</code> is added to the image mean to give the above/upper threshold. These values should always be positive. <code>threshold_absolute</code> dictionary <code>-1.0, 1.0</code> Below (first) and above (second) absolute threshold for separating data from the image background. <code>gaussian_size</code> float <code>0.5</code> The number of standard deviations to build the Gaussian kernel and thus affects the degree of blurring. See skimage.filters.gaussian and <code>sigma</code> for more information. <code>gaussian_mode</code> string <code>nearest</code> <code>filter</code>  \u221f <code>remove_scars</code> <code>run</code> bool <code>true</code> Whether to run scar removal. <code>removal_iterations</code> int <code>2</code> The number of times to run scar removal. More iterations can improve scar removal by tidying up remaining artefacts after removal, though will cause more data distortion. <code>threshold_low</code> float <code>0.250</code> The threshold determining whether to further assess if a pixel is a scar. This is the first check, and lowering it will allow more pixels to undergo further analysis in determining if they are scars. <code>threshold_high</code> float <code>0.666</code> The threshold above which a pixel ridge is automatically determined to be a scar. Lowering this value will increase the number of pixels that are flagged as scars with no additional checks. <code>max_scar_width</code> int <code>4</code> The maximum thickness of scars in pixels, along their short axis, ie vertical distance in an AFM image. This parameter can be reduced to only allow marking of thin scars or increased to allow thicker regions to be marked as scars. Be careful - if this value in pixels approaches the thickness of DNA, then it will start deleting regions of DNA (or other relevant data). <code>min_scar_length</code> int <code>16</code> The minimum length of scars in pixels, along their long axis, ie horizontal distance in an AFM image. This parameter can be reduced to allow shorter ridges to be marked as scars, or increased to only allow longer regions to be marked. This can be used to attempt to avoid marking data such as DNA from being marked as a scar, as it may be unlikely that you have a section of DNA that is straight and more than 16 pixels long. <code>grains</code> <code>run</code> boolean <code>true</code> Whether to run grain finding. Options <code>true</code>, <code>false</code> <code>row_alignment_quantile</code> float <code>0.5</code> Quantile (0.0 to 1.0) to be used to determine the average background for the image. below values may improve flattening of large features. <code>smallest_grain_size_nm2</code> int <code>100</code> The smallest size of grains to be included (in nm^2), anything smaller than this is considered noise and removed. NB must be <code>&gt; 0.0</code>. <code>threshold_method</code> float <code>std_dev</code> Threshold method for grain finding. Options : <code>otsu</code>, <code>std_dev</code>, <code>absolute</code> <code>otsu_threshold_multiplier</code> <code>1.0</code> Factor by which the derived Otsu Threshold should be scaled. <code>threshold_std_dev</code> dictionary <code>10.0, 1.0</code> A pair of values that scale the standard deviation, after scaling the standard deviation <code>below</code> is subtracted from the image mean to give the below/lower threshold and the <code>above</code> is added to the image mean to give the above/upper threshold. These values should always be positive. <code>threshold_absolute</code> dictionary <code>-1.0, 1.0</code> Below (first), above (second) absolute threshold for separating grains from the image background. <code>direction</code> <code>above</code> Defines whether to look for grains above or below thresholds or both. Options: <code>above</code>, <code>below</code>, <code>both</code> <code>smallest_grain_size</code> int <code>50</code> Catch-all value for the minimum size of grains. Measured in nanometres squared. All grains with area below than this value are removed. <code>absolute_area_threshold</code> dictionary <code>[300, 3000], [null, null]</code> Area thresholds for above the image background (first) and below the image background (second), which grain sizes are permitted, measured in nanometres squared. All grains outside this area range are removed. <code>remove_edge_intersecting_grains</code> boolean <code>true</code> Whether to remove grains that intersect the image border. Do not change this unless you know what you are doing. This will ruin any statistics relating to grain size, shape and DNA traces. <code>grains</code>  \u221f <code>unet_config</code> <code>path_to_model</code> str <code>null</code> The path to the U-Net model to override traditional segmentation. Supply a path to a tensorflow U-net model to use, else U-Net segmentation will be skipped. <code>grain_crop_padding</code> int <code>0</code> The amount of padding to be applied to grain crops before they are passed to the U-Net model. Increasing this value within reason may reduce edge-anomalies within the crops. Additionally, models are usually trained assuming the grain will take up a certain proportion of the image. If segmentation is poor, try increasing this. <code>upper_norm_bound</code> float <code>5.0</code> The upper normalisation bound for normalising grain crops before sending to the segmentation model. The model will have been trained with particular normalisation bounds, use those. If in doubt, talk to the person who trained the model or use a sensible range, eg if DNA is expected between 0 and 2nm, try using -1 to 3 as normalisation bounds. <code>lower_norm_bound</code> float <code>-1</code> The lower normalisation bound for normalising grain crops before sending to the segmentation model. The model will have been trained with particular normalisation bounds, use those. If in doubt, talk to the person who trained the model or use a sensible range, eg if DNA is expected between 0 and 2nm, try using -1 to 3 as normalisation bounds. <code>grains</code>  \u221f <code>vetting</code> <code>class_region_number_thresholds</code> list[tuple[int, int, int]] <code>null</code> Class region number thresholds, list of lists, <code>[[class, low, high]]</code>, eg: <code>[[1, 2, 4], [2, 1 ,1]]</code> for class 1 to have 2-4 regions and class 2 to have 1 region. Can use Noneto not set an upper/lower bound. <code>class_conversion_size_thresholds</code> list[tuple[tuple[int, int, int], tuple[int, int]]] <code>null</code> Class conversion size thresholds, list of tuples of 3 integers and 2 integers, ie <code>list[tuple[tuple[int, int, int], tuple[int, int]]]</code> eg <code>[[[1, 2, 3], [5, 10]]]</code> for each region of class 1 to convert to 2 if smaller than 5 nm^2 and to class 3 if larger than 10 nm^2. <code>class_size_thresholds</code> list[tuple[int, int, int]] null Class size thresholds (nm^2), list of tuples of 3 integers, ie <code>[[class, low, high],]</code> eg <code>[[1, 100, 1000], [2, 1000, None]]</code> for class 1 to have 100-1000 nm^2 and class 2 to have 1000-any nm^2. Can use None to not set an upper/lower bound. <code>nearby_conversion_classes_to_convert</code> list[tuple[tuple[int, int], tuple[int, int]]] <code>null</code> Class conversion for nearby regions, list of tuples of two-integer tuples, eg <code>[[[1, 2], [3, 4]]]</code> to convert class 1 to 2 and 3 to 4 for small touching regions <code>class_touching_threshold</code> int <code>5</code> Number of dilation steps to use for detecting touching regions, higher value will mean further away regions will be considered touching <code>keep_largest_labelled_regions_classes</code> list[int] <code>null</code> Classes to keep only the largest labelled regions for, list of integers eg <code>[1, 2]</code> to keep only the largest labelled regions for classes 1 and 2 <code>class_connection_point_thresholds</code> list[tuple[tuple[int, int] tuple[int, int]]] <code>null</code> Class connection point thresholds, <code>[[[class_1, class_2], [min, max]]]</code> eg <code>[[[1, 2], [1, 1]]]</code> for class 1 to have 1 connection point with class 2 <code>grainstats</code> <code>run</code> boolean <code>true</code> Whether to calculate grain statistics. Options : <code>true</code>, <code>false</code> <code>cropped_size</code> float <code>40.0</code> Force cropping of grains to this length (in nm) of square cropped images (can take <code>-1</code> for grain-sized box) <code>edge_detection_method</code> str <code>binary_erosion</code> Type of edge detection method to use when determining the edges of grain masks before calculating statistics on them. Options : <code>binary_erosion</code>, <code>canny</code>. <code>disordered_tracing</code> <code>run</code> boolean <code>true</code> Whether to run the Disordered Traces pipeline. Options : true, false <code>min_skeleton_size</code> int <code>10</code> The minimum number of pixels a skeleton should be for statistics to be calculated on it. Anything smaller than this is dropped but grain statistics are retained. <code>pad_width</code> str <code>1</code> Padding for individual grains when tracing. This is sometimes required if the bounding box around grains is too tight and they touch the edge of the image. <code>disordered_tracing</code>  \u221f <code>mask_smoothing_params</code> <code>gaussian_sigma</code> float <code>2</code> Amount of smoothing by a gaussian kernel. This will compete with <code>dilation_iteration</code> to see which changes the grain mask least, ensuring quality over different scan sizes. <code>dialtion_iterations</code> int <code>2</code> The the number of dilations to perform to smooth. This will compete with <code>gaussian_sigma</code> to see which changes the grain mask least, ensuring quality over different scan sizes. <code>holearea_min_max</code> list <code>[0, null]</code> As smoothing fill holes in the mask, this replaces those within a size range (in nm^2). <code>disordered_tracing</code>  \u221f <code>skeletonisation_params</code> <code>method</code> str <code>topostats</code> The Skeletonisation method to use, possible options are <code>zhang</code>, <code>lee</code>, <code>thin</code> (from Scikit-image Morphology module) or the original bespoke TopoStats (height-biasing) method topostats. <code>height_bias</code> float <code>0.6</code> The percentage of lowest pixels to remove during each skeletonisation iteration of the <code>topostats</code> method. <code>disordered_tracing</code>  \u221f <code>pruning_params</code> <code>max_length</code> float <code>-1</code> The length in nanometres below which to prune branches. Default is <code>-1</code>, meaning 15% of the total length. <code>height_threshold</code> float <code>null</code> The height threshold in nanometres below which to prune branches. <code>method_values</code> str <code>mid</code> The method that determines how branch height is calculated. Options: <code>min</code>, <code>median</code>, <code>mid</code> (middle). <code>method_outliers</code> str <code>mean_abs</code> How to compare the threshold and branch heights to remove low branches. Options are; the inter-quartile range <code>iqr</code>, the height_threshold as an absolute value <code>abs</code>, or the mean of all branches minus the height_threshold value <code>mean_abs</code>. <code>nodestats</code> <code>run</code> boolean <code>true</code> Whether to quantify the crossings in an image. Required for over/under tracing through crossings. Options : true, false <code>node_joining_length</code> float <code>7.0</code> The distance (nm) over which to join nearby crossing points as the skeletonisation will not always force crossing points to connect. <code>node_extend_list</code> float <code>14.0</code> The distance (nm) over which to join nearby odd-branched nodes. <code>branch_pairing_length</code> float <code>nodestats</code> The length (nm) from the crossing point to pair the emainating branches and trace along to obtain the over/under distinguishing full-width half-maximum (FWHM's) values. <code>pair_odd_branches</code> boolean <code>true</code> Whether to try and pair branches at odd-branch crossing regions and leave one hanging branch, or to leave all branches hanging here. Options: <code>true</code> or <code>false</code>. <code>pad_width</code> str <code>1</code> Padding for individual grains when tracing. This is sometimes required if the bounding box around grains is too tight and they touch the edge of the image. <code>ordered_tracing</code> <code>run</code> boolean <code>true</code> Whether to order the pruned skeletons of Disordered Traces. Options : true, false <code>ordering_method</code> str <code>nodestats</code> The method of ordering the disordered traces either using the nodestats output or solely the disordered traces. Options: <code>nodestats</code> or <code>topostats</code>. <code>pad_width</code> int 10 Padding for individual grains when tracing. This is sometimes required if the bounding box around grains is too tight and they touch the edge of the image. <code>splining</code> <code>run</code> boolean <code>true</code> Whether to run ordered trace splining to generate smooth traces. Options : true, false <code>method</code> int <code>rolling_window</code> The method used to smooth out the ordered traces. Options: <code>rolling_window</code> or <code>spline</code>. <code>rolling_window_size</code> int <code>20.0e-9</code> The length (in meters) of the coordinate averaging window to smooth the ordered trace. <code>spline_step_size</code> int <code>7.0e-9</code> The The sampling length of the spline (in meters) to obtain an average of splines. <code>spline_linear_smoothing</code> int <code>5.0</code> The amount of smoothing to apply to linear molecule splines. <code>spline_circular_smoothing</code> int <code>5.0</code> The amount of smoothing to apply to circular molecule splines. <code>spline_degree</code> int <code>3</code> The polynomial degree of the spline. Smaller, odd degrees work best SciPy - slprep. <code>plotting</code> <code>run</code> boolean <code>true</code> Whether to run plotting. Options : <code>true</code>, <code>false</code> <code>style</code> str <code>topostats.mplstyle</code> The default loads a custom matplotlibrc param file that comes with TopoStats. Users can specify the path to their own style file as an alternative. <code>save_format</code> string <code>null</code> Format to save images in, <code>null</code> defaults to <code>png</code> see matplotlib.pyplot.savefig <code>savefig_dpi</code> string / float <code>null</code> Dots Per Inch (DPI), if <code>null</code> then the value <code>figure</code> is used, for other values (typically integers) see [#further-customisation] and Matplotlib. Low DPI's improve processing time but can reduce the plotted trace (but not the actual trace) accuracy. <code>pixel_interpolation</code> string <code>null</code> Interpolation method for image plots. Recommended default 'null' prevents banding that occurs in some images. If interpolation is needed, we recommend <code>gaussian</code>. See matplotlib imshow interpolations documentation for details. <code>image_set</code> string <code>all</code> Which images to plot. Options : <code>all</code>, <code>core</code> (flattened image, grain mask overlay and trace overlay only). <code>zrange</code> list <code>[0, 3]</code> Low (first number) and high (second number) height range for core images (can take [null, null]). NB <code>low &lt;= high</code> otherwise you will see a <code>ValueError: minvalue must be less than or equal to maxvalue</code> error. <code>colorbar</code> boolean <code>true</code> Whether to include the colorbar scale in plots. Options <code>true</code>, <code>false</code> <code>axes</code> boolean <code>true</code> Whether to include the axes in the produced plots. <code>num_ticks</code> null / int <code>null</code> Number of ticks to have along the x and y axes. Options : <code>null</code> (auto) or an integer &gt;1 <code>cmap</code> string <code>null</code> Colormap/colourmap to use. Defaults to 'nanoscope' if null (defined in <code>topostats/topostats.mplstyle</code>). Other options are 'afmhot', 'viridis' etc., see Matplotlib : Choosing Colormaps. <code>mask_cmap</code> string <code>blu</code> Color used when masking regions. Options <code>blu</code>, <code>jet_r</code> or any valid Matplotlib colour. <code>histogram_log_axis</code> boolean <code>false</code> Whether to plot hisograms using a logarithmic scale or not. Options: <code>true</code>, <code>false</code>. <code>summary_stats</code> <code>run</code> boolean <code>true</code> Whether to generate summary statistical plots of the distribution of different metrics grouped by the image that has been processed. <code>config</code> str <code>null</code> Path to a summary config YAML file that configures/controls how plotting is done. If one is not specified either the command line argument <code>--summary_config</code> value will be used or if that option is not invoked the default <code>topostats/summary_config.yaml</code> will be used."},{"location":"usage/configuration/#summary-configuration","title":"Summary Configuration","text":"<p>Plots summarising the distribution of metrics are generated by default. The behaviour is controlled by a configuration file. The default example can be found in <code>topostats/summary_config.yaml</code>. The fields of this file are described below.</p> Section Sub-Section Data Type Default Description <code>output_dir</code> <code>str</code> <code>./output/</code> Where output plots should be saved to. <code>csv_file</code> <code>str</code> <code>null</code> Where the results file should be loaded when running <code>toposum</code> <code>file_ext</code> <code>str</code> <code>png</code> File type to save images as. <code>var_to_label</code> <code>str</code> <code>null</code> Optional YAML file that maps variable names to labels, uses <code>topostats/var_to_label.yaml</code> if null. <code>molecule_id</code> <code>str</code> <code>molecule_number</code> Variable containing the molecule number. <code>image_id</code> <code>str</code> <code>image</code> Variable containing the image identifier. <code>hist</code> <code>bool</code> <code>True</code> Whether to plot a histogram of statistics. <code>bins</code> <code>int</code> <code>20</code> Number of bins to plot in histogram. <code>stat</code> <code>str</code> <code>count</code> What metric to plot on histogram valid values are <code>count</code> (default), <code>frequency</code>, <code>probability</code>, <code>percent</code>, <code>density</code> <code>kde</code> <code>bool</code> <code>True</code> Whether to include a Kernel Density Estimate on histograms. NB if both <code>hist</code> and <code>kde</code> are true they are overlaid. <code>violin</code> <code>bool</code> <code>True</code> Whether to generate Violin Plots. <code>figsize</code> <code>list</code> <code>[16, 9]</code> Figure size (x then y dimensions). <code>alpha</code> <code>float</code> <code>0.5</code> Level of transparency to use when plotting. <code>palette</code> <code>str</code> <code>bright</code> Seaborn color palette. Options <code>colorblind</code>, <code>deep</code>, <code>muted</code>, <code>pastel</code>, <code>bright</code>, <code>dark</code>, <code>Spectral</code>, <code>Set2</code> <code>stats_to_sum</code> <code>list</code> <code>str</code> A list of strings of variables to plot, comment (placing a <code>#</code> at the start of the line) and uncomment as required. Possible values are <code>area</code>, <code>area_cartesian_bbox</code>, <code>aspect_ratio</code>, <code>banding_angle</code>, <code>contour_length</code>, <code>end_to_end_distance</code>, <code>height_max</code>, <code>height_mean</code>, <code>height_median</code>, <code>height_min</code>, <code>radius_max</code>, <code>radius_mean</code>, <code>radius_median</code>, <code>radius_min</code>, <code>smallest_bounding_area</code>, <code>smallest_bounding_length</code>, <code>smallest_bounding_width</code>, <code>volume</code>"},{"location":"usage/configuration/#validation","title":"Validation","text":"<p>Configuration files are validated against a schema to check that the values in the configuration file are within the expected ranges or valid parameters. This helps capture problems early and should provide informative messages as to what needs correcting if there are errors.</p>"},{"location":"usage/configuration/#matplotlib-style","title":"Matplotlib Style","text":"<p>TopoStats generates a number of images of the scans at various steps in the processing. These are plotted using the Python library Matplotlib. A custom <code>matplotlibrc</code> file is included in TopoStats which defines the default parameters for generating images. This covers all aspects of a plot that can be customised, for example we define custom colour maps <code>nanoscope</code> and <code>afmhot</code>. By default the former is configured to be used. Other parameters that are customised are the <code>font.size</code> which affects axis labels and titles.</p> <p>If you wish to modify the look of all images that are output you can generate a copy of the default configuration using <code>topostats create-matplotlibrc</code> command which will write the output to <code>topostats.mplstyle</code> by default (NB there are flags which allow you to specify the location and filename to write to, see <code>topostats create-matplotlibrc --help</code> for further details).</p> <p>You should read and understand this commented file in detail. Once changes have been made you can run TopoStats using this custom file using the following command (substituting <code>my_custom_topostats.mplstyle</code> for whatever you have saved your file as).</p> <pre><code>topostats --matplotlibrc my_custom_topostats.mplstyle process\n</code></pre> <p>NB Plotting with Matplotlib is highly configurable and there are a plethora of options that you may wish to tweak. Before delving into customising <code>matplotlibrc</code> files it is recommended that you develop and build the style of plot you wish to generate using Jupyter Notebooks and then translate them to the configuration file. Detailing all of the possible options is beyond the scope of TopoStats but the Matplotlib documentation is comprehensive and there are some sample Jupyter Notebooks (see <code>notebooks/03-plotting-scans.ipynb</code>) that guide you through the basics.</p>"},{"location":"usage/configuration/#further-customisation","title":"Further customisation","text":"<p>Whilst the overall look of images is controlled in this manner there is one additional file that controls how images are plotted in terms of filenames, titles and image types and whether an image is part of the <code>core</code> subset (flattened image, grain mask overlay and trace overlay) that are always generated or not.</p> <p>This is the <code>topostats/plotting_dictionary.yaml</code> which for each image stage defines whether it is a component of the <code>core</code> subset of images that are always generated, sets the <code>filename</code>, the <code>title</code> on the plot, the <code>image_type</code> (whether it is a binary image), the <code>savefig_dpi</code> which controls the Dots Per Inch (essentially the resolution). Each image has the following structure.</p> <pre><code>z_threshed:\n  title: \"Height Thresholded\"\n  image_type: \"non-binary\"\n  savefig_dpi: 100\n  core_set: true\n</code></pre> <p>Whilst it is possible to edit this file it is not recommended to do so.</p> <p>The following section describes how to override the DPI settings defined in this file and change the global <code>cmap</code> (colormap/colourmap) used in plotting and output format.</p>"},{"location":"usage/configuration/#dpi","title":"DPI","text":"<p>During development it was found that setting high DPI globally for all images had a detrimental impact on processing speeds, slowing down the overall processing time. The solution we have implemented is to use the <code>topostats/plotting_dictionary.yaml</code> file and set the <code>savefig_dpi</code> parameter on a per-image basis.</p> <p>If you wish to change the DPI there are two options, you can change the value for all images by modifying the setting in your a custom configuration by modifying the <code>savefig_dpi</code> from <code>null</code> to your desired value. The example below shows a section of the configuration file you can generate and setting this value to <code>400</code>.</p> <pre><code>plotting:\n  run: true # Options : true, false\n  style: topostats.mplstyle # Options : topostats.mplstyle or path to a matplotlibrc params file\n  savefig_format: null # Options : null (defaults to png) or see https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html\n  savefig_dpi: 400 # Options : null (defaults to format) see https://afm-spm.github.io/TopoStats/main/configuration.html#further-customisation and https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html\n</code></pre> <p>The value in the configuration file (or the default if none is specified) can also be configured at run-time using the <code>--savefig-dpi ###</code> option to the <code>topostats process</code>. This will over-ride both the default or any value specified in a custom configuration you may have set. The following sets this to <code>400</code></p> <pre><code>topostats process --savefig-dpi 400\n</code></pre> <p>NB Changing the DPI in this manner will apply to all images and may significantly reduce processing speed as it takes longer to write images with high DPI to disk.</p> <p>If you wish to have fine grained control over the DPI on a per-image basis when batch processing then your only recourse is to change the values in <code>topostats/plotting_dictionary.yaml</code>. Where this is depends on how you have installed TopoStats, if it is from a clone of the Git repository then it can be found in <code>TopoStats/topostats/plotting_dictionary.yaml</code>. If you have installed from PyPI using <code>pip install topostats</code> then it will be under the virtual environment you have created e.g. <code>~/.virtualenvs/topostats/lib/python3.11/site-packages/topostats/topostats/plotting_dictionary.yaml</code> if you are using plain virtual environments or <code>~/miniconda3/envs/topostats/lib/python3.11/site-packages/topostats/topostats/plotting_dictionary.yaml</code> if you are using Conda environments and chose <code>~/miniconda3</code> as the base directory when installing Conda.</p> <p>If you have installed TopoStats from the cloned Git repository the file will be under <code>TopoStats/topostats/plotting_dictionary.yaml</code>.</p> <p>NB The exact location will be highly specific to your system so the above are just guides as to where to find things.</p>"},{"location":"usage/configuration/#colormap","title":"Colormap","text":"<p>The colormap used to plot images is set globally in <code>topostats/default_config.yaml</code>. TopoStats includes two custom colormaps <code>nanoscope</code> and <code>afmhot</code> but any colormap recognised by Matplotlib can be used (see the Matplotlib Colormap reference for choices).</p> <p>If you want to modify the colormap that is used you have two options. Firstly you can generate a configuration file and modify the field <code>cmap</code> to your choice. The example below shows changing this from <code>null</code> (which defaults to <code>nanoscope</code> as defined in <code>topostats.mplstyle</code>) to <code>rainbow</code>.</p> <pre><code>plotting:\n  ...\n  cmap: rainbow # Colormap/colourmap to use (default is 'nanoscope' which is used if null, other options are 'afmhot', 'viridis' etc.)\n</code></pre> <p>Alternatively it is possible to specify the colormap that is used on the command line using the <code>--cmap</code> option to <code>topostats process</code>. This will over-ride both the default or any value specified in a custom configuration you may have set. The following sets this to <code>rainbow</code>.</p> <pre><code>topostats process --cmap rainbow\n</code></pre>"},{"location":"usage/configuration/#saved-image-format","title":"Saved Image format","text":"<p>Matplotlib, and by extension TopoStats, supports saving images in a range of different formats including <code>png</code> (Portable Network Graphic), <code>svg</code> (Scalable Vector Graphics), <code>pdf</code> (Portable Document Format), and <code>tif</code> (Tag Image File Format). The default is <code>png</code> but, as with both DPI and Colormap, these can be easily changed via a custom configuration file or command line options to change these without having to edit the Matplotlib Style file. If using <code>tif</code> it is worth being aware that although the image will be saved, this will be without metadata since this is not supported for <code>tif</code> files (see the note under <code>metadata</code> of Matplotlib savefig).</p> <p>If you want to modify the output file format that is used you have two options. Firstly you can generate a configuration file and modify the field <code>savefig_format</code> to your choice. The example below shows changing this from <code>null</code> (which defaults to <code>png</code> as defined in <code>topostats.mplstyle</code>) to <code>svg</code>.</p> <pre><code>plotting:\n  ...\n  savefig_format: svg # Options : null (defaults to png) or see https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html\n</code></pre> <p>Alternatively it is possible to specify the output image format that is used on the command line using the <code>--savefig-format</code> option to <code>topostats process</code>. This will over-ride both the default or any value specified in a custom configuration you may have set. The following sets this to <code>svg</code>.</p> <pre><code>topostats process --savefig-format svg\n</code></pre> <p>NB Note that these options are not mutually exclusive and can therefore be combined along with any of the other options available to <code>topostats process</code>. The following would use a DPI of <code>400</code>, set the colormap to <code>rainbow</code> and the output format to <code>svg</code> when running Topostats and would over-ride options in any custom configuration file or matplotlib style file.</p> <pre><code>topostats process --savefig-dpi 400 --cmap rainbow --savefig-format svg\n</code></pre>"},{"location":"usage/configuration/#absolute-v-relative-paths","title":"Absolute v Relative paths","text":"<p>When writing file paths you can use absolute or relative paths. On Windows systems absolute paths start with the drive letter (e.g. <code>c:/</code>) on Linux and OSX systems they start with <code>/</code>. Relative paths are started either with a <code>./</code> which denotes the current directory or one or more <code>../</code> which means the higher level directory from the current directory. You can always find the current directory you are in using the <code>pwd</code> (<code>p</code>rint <code>w</code>orking <code>d</code>irectory). If your work is in <code>/home/user/path/to/my/data</code> and <code>pwd</code> prints <code>/home/user</code> then the relative path to your data is <code>./path/to/my/data</code>. The <code>cd</code> command is used to <code>c</code>hange <code>d</code>irectory.</p> <pre><code>pwd\n/home/user/\n# Two ways of changing directory using a relative path\ncd ./path/to/my/data\npwd\n/home/user/path/to/my/data\n# Using an absolute path\ncd /home/user/path/to/my/data\npwd\n/home/user/path/to/my/data\n</code></pre>"},{"location":"usage/data_dictionary/","title":"Data Dictionary","text":"<p>Output from TopoStats includes two sets of statistics in ASCII text <code>.csv</code> files. The tables below detail the columns of these files, the data types, a description and their units where appropriate.</p>"},{"location":"usage/data_dictionary/#all_statisticscsv","title":"<code>all_statistics.csv</code>","text":"<p>The <code>all_statistics.csv</code> file contains details on each grain that has been detected and traced and has the following fields.</p> Column / field / feature Description Type Units <code>image</code> Filename (minus extension) of scan. <code>str</code> N/A <code>threshold</code> Whether grain is <code>above</code> or <code>below</code> a threshold. <code>str</code> N/A <code>grain_number</code> Number of found grain (starts at <code>0</code>) <code>int</code> N/A <code>centre_x</code> x coordinate of grain centre. <code>float</code> m <code>centre_y</code> y coordinate of grain centre. <code>float</code> m <code>radius_min</code> minimum distance from the centroid to edge of the grain. <code>float</code> m <code>radius_max</code> maximum distance from the centroid to edge of the grain. <code>float</code> m <code>radius_mean</code> mean distance from the centroid to the edge of the grain. <code>float</code> m <code>radius_median</code> median distance from the centroid to the edge of the grain. <code>float</code> m <code>height_min</code> Minimum height of grain. <code>float</code> m <code>height_max</code> Maximum height of grain. <code>float</code> m <code>height_median</code> Median height of grain. <code>float</code> m <code>height_mean</code> Mean height of grain. <code>float</code> m <code>volume</code> Volume of the grain calculated as the number of pixels multiplied by each height and scaled to metres. <code>float</code> m^3 <code>area</code> Area of the grain itself calculated as the number of pixels scaled to metres. <code>float</code> m^2 <code>area_cartesian_bbox</code> Area of the bounding box for the grain along the cartesian axes. (Not the smallest bounding box). <code>float</code> m^2 <code>smallest_bounding_width</code> Width of the smallest bounding box for the grain (not along cartesian axes). <code>float</code> m <code>smallest_bounding_length</code> Length of the smallest bounding box for the grain (not along cartesian axes). <code>float</code> m <code>smallest_bounding_area</code> Area of the smallest bounding box for the grain (not along cartesian axes). <code>float</code> m^2 <code>aspect_ratio</code> Aspect ratio of the grain (length / width), always &gt;= 1. <code>float</code> N/A <code>max_feret</code> Longest length of the grain (see Feret diameter). <code>float</code> m <code>min_feret</code> Shortest width of the grain (see Feret diameter). <code>float</code> m <code>basename</code> Directory in which images was found. <code>str</code> N/A <code>grain_endpoints</code> The number of pixels designated as endpoints (only 1 neighbour) in the pruned skeleton. NB molecules with zero end-points are circular/closed loops. <code>integer</code> N/A <code>grain_junctions</code> The number of pixels designated as junctions (&gt;2 neighbours) in the pruned skeleton. <code>integer</code> N/A <code>total_branch_length</code> The sum of all branch lengths in the pruned skeleton. <code>float</code> m <code>grain_width_mean</code> The mean width of the grain. <code>float</code> m <code>num_crossings</code> The number of crossing regions found in the grain. Note: this will be equal to or lower than the number of junctions explained in the previous section. <code>integer</code> N/A <code>avg_crossing_confidence</code> The average of all pseudo crossing confidences. Used to estimate quality of predictions. <code>float</code> N/A <code>min_crossing_confidence</code> The minimum of all pseudo crossing confidences. Used to estimate quality of predictions. <code>float</code> N/A <code>num_molecules</code> The number of molecules found by following the tracing paths. Note: This will always be 1 for the TopoStats method. <code>integer</code> N/A <code>writhe_string</code> The writhe sign (+/-) which describes the crossing directionality. If a crossing contains &gt; 2 crossing branches, the single crossing region is split into pairs and the writhe calculated in brackets i.e. \"+(-++)\". <code>str</code> N/A <code>total_contour_length</code> The total length along the splined trace of all identified molecules. <code>float</code> m <code>average_end_to_end_distance</code> The average distance from two endpoints of the spline of all identified linear molecules. <code>float</code> m"},{"location":"usage/data_dictionary/#image_statscsv","title":"<code>image_stats.csv</code>","text":"<p>The <code>image_stats.csv</code> summarises the metrics</p> Column / field / feature Description Type Units <code>image</code> Filename of image statistics pertain to. <code>str</code> N/A <code>image_size_x_m</code> Width of image. <code>float</code> m <code>image_size_y_m</code> Height of image. <code>float</code> m <code>image_area_m2</code> Area of image (width x height). <code>float</code> m^2 <code>image_size_x_px</code> Width of image in pixels. <code>int</code> N/A <code>image_size_y_px</code> Height of image in pixels. <code>int</code> N/A <code>image_area_px2</code> Area of image in pixels squared. <code>int</code> N/A <code>grains_number_above</code> Number of grains found above threshold. <code>int</code> N/A <code>grains_per_m2_above</code> Density of grains above upper threshold. <code>int</code> N/A <code>grains_number_below</code> Number of grains found below threshold. <code>int</code> N/A <code>grains_per_m2_below</code> Density of grains below lower threshold. <code>int</code> N/A <code>rms_roughness</code> Root Mean Square Roughness, the square root of the mean squared heights across the surface (Surface Roughness; Surface roughness (Wikipedia)) <code>float</code> N/A"},{"location":"usage/data_dictionary/#all_disordered_segment_statisticscsv","title":"<code>all_disordered_segment_statistics.csv</code>","text":"<p>Please refer to the specific sections on output from running Disordered Segment Statistics Tracing section of the Disordered Tracing page for the data dictionary of the <code>all_disordered_segment_statistics.csv</code> output.</p>"},{"location":"usage/installation/","title":"Installation","text":"<p>NB - If you have trouble installing TopoStats please do checkout the discussion for possible solutions. If your problem isn't covered then please do not hesitate to ask a question.</p> <p>TopoStats is a Python package designed to run at the command line. If you are using Microsoft Windows you should use Powershell. You may have Python installed on your system but should use a Python Virtual Environment such as Miniconda and install and use TopoStats under the Virtual Environment. The versions of Python supported are Python &gt;=3.8 and so when creating your virtual environment you should specify this <code>3.8</code> as the minimum.</p>"},{"location":"usage/installation/#setting-up-conda","title":"Setting up Conda","text":"<p>Once you have downloaded and installed Miniconda you can create a virtual environment for installing TopoStats for installing and running TopoStats. We will call this environment <code>topostats</code> (specified with the <code>--name topostats</code> option) and use Python 3.10 (the option <code>python=3.10</code>). After creating it we can, as per the instructions printed out, activate the environment.</p> <pre><code>conda create --name topostats python=3.10\nconda activate topostats\n</code></pre> <p>You are now ready to install TopoStats.</p> <p>NB If you are using an Apple M1 Macbook then you need to install Anaconda &gt;= 2022.05.</p>"},{"location":"usage/installation/#installing-topostats","title":"Installing TopoStats","text":"<p>There are two options for installing TopoStats depending on your usage</p> <ol> <li>Python Package Index - appropriate if you are just using TopoStats and don't need to dig into    the code.</li> <li>Cloning the GitHub Repository - if you want to look at the code, contribute to it, debug errors or perhaps test a new    feature before a release.</li> </ol>"},{"location":"usage/installation/#pypi-installation","title":"PyPI Installation","text":"<p>After activating your <code>topostats</code> Conda environment you can install TopoStats from PyPI using the following command.</p> <pre><code>pip install topostats\n</code></pre> <p>This will install TopoStats under your virtual environment and the command <code>topostats</code> will be available at the command line. It has a number of sub-commands which can be displayed by invoking it without any options. You can upgrade <code>topostats</code> by using the <code>--upgrade</code> flag...</p> <pre><code>pip install --upgrade topostats\n</code></pre> <p>You can always install a specific version from PyPI</p> <pre><code>pip install topostats==2.0.0\n</code></pre> <p>For more information on using <code>pip</code> to install and manage packages please refer to the pip documentation.</p>"},{"location":"usage/installation/#installing-from-github","title":"Installing from GitHub","text":"<p>You may wish to consider cloning and installing TopoStats from GitHub if...</p> <ul> <li>You wish to try out new features that have been developed since the last release (if you find problems please create   an issue).</li> <li>If you have found an issue in a released version and want to see if it has been fixed in the unreleased version.</li> <li>If you wish to develop and extend TopoStats with new features yourself.</li> </ul> <p>There are two options to install from GitHub, which you use will depend on what you want to do.</p> <ol> <li>Using PyPI to install directly.</li> <li>Clone the repository and install from there.</li> </ol> <p>If all you want to do is use the development version of TopoStats then you can use option 1. If you wish to change the underlying code you should use option 2.</p>"},{"location":"usage/installation/#installing-from-github-using-pypi","title":"Installing from GitHub using PyPI","text":"<p><code>pip</code> supports installing packages from GitHub. To install the <code>main</code> branch of TopoStats use the following in your Virtual Environment.</p> <pre><code>pip install git+https://github.com/AFM-SPM/TopoStats.git@main\n</code></pre> <p>You can install any branch on GitHub by modifying the last argument (<code>@main</code>) to the branch you wish to install, e.g. <code>@another_branch</code> would install the <code>another_branch</code> (if it existed).</p>"},{"location":"usage/installation/#cloning-the-repository-and-installing","title":"Cloning the Repository and installing","text":"<p>If you do not have Git already installed please see Git Installation. If you intend to contribute to the development of TopoStats please read through the contributing section. If you are familiar with the command line then you can clone and install TopoStats with the following after activating your virtual environment. By installing in editable mode (with the <code>-e</code> flag) switching branches will make the branch available.</p> <pre><code>cd ~/where/to/clone\ngit clone git@github.com:AFM-SPM/TopoStats.git\ncd TopoStats\npip install -e .\n</code></pre> <p>If you plan to contribute to development by adding features or address an existing issue please refer to the contributing section and pay particular attention to the section about installing additional dependencies.</p> <p>We include notebooks which show how to use different aspects of TopoStats. If you wish to try these out the Jupyter Noteooks then you can install the dependencies that are required from the cloned TopoStats repository using...</p> <pre><code>pip install \".[notebooks]\"\n</code></pre>"},{"location":"usage/installation/#cloning-using-gitkraken","title":"Cloning Using GitKraken","text":"<p>If you are using GitKraken you can clone the repository by selecting \"Clone\" and then \"GitHub.com\" and typing <code>TopoStats</code> into the box next to \"Repository to Clone\" and you should be presented with the option of selecting \"TopoStats\" from the AFM-SPM organisation. Once cloned follow the above instructions to install with <code>pip</code> under your virtual environment.</p>"},{"location":"usage/installation/#tests","title":"Tests","text":"<p>One of the major changes in the refactoring is the introduction of unit tests. These require certain packages to be installed which are not installed to your virtual environment by setuptools in the above steps. If you are intending to modify or contribute to the development of TopoStats or make changes to the code base you will likely want to be able to run the tests. Install the necessary dependencies to do so with...</p> <pre><code>cd TopoStats\ngit checkout dev\npip install \".[tests]\"\npytest\n</code></pre>"},{"location":"usage/installation/#git","title":"Git","text":"<p>Git is a version control system for managing software development and is required to be installed on your computer in order to clone the TopoStats repository. Instructions on installing Git can be found at Git Guides - install git.</p> <p>A nice Graphical User Interface for working with Git is GitKraken which includes everything you need.</p>"},{"location":"usage/matplotlib-style/","title":"Matplotlib Style","text":"<p>TopoStats includes its own Matplotlib style file . This resides at <code>topostats/topostats.mplstyle</code> (see also GitHub repository).</p> <p>If you wish to customise the style of plots you can create a copy of this using <code>topostats create-matplotlibrc</code>. For more information see</p> <pre><code>topostats create-matplotlibrc --help\n</code></pre> <p>Once you have modified and saved the file you can run your analyses with it using...</p> <pre><code>topostats --matplotlibrc &lt;filename&gt;\n</code></pre> <p>Alternatively you can change the parameters in a custom configuration file to point to the newly created style file.</p>"},{"location":"usage/matplotlib-style/#color-maps","title":"Color Maps","text":"<p>Several custom colormaps for plotting data are also included. These are defined within the <code>topostats.themes.Colormap</code> class. For full details refer to the API.</p> <ul> <li><code>nanoscope</code> colormap is provided and used by default.</li> <li><code>gwyddion</code> colormap is provided that matches the colormap used by default in the Gwyddion</li> <li><code>blue</code> colormap is used for masks by default.</li> <li><code>blue_purple_green</code> colormap is available and is used when plotting traces of overlapping molecules.</li> </ul>"},{"location":"usage/notebooks/","title":"Notebooks","text":"<p>A series of Jupyter Notebooks are provided that demonstrate how to use the TopoStats package in a more interactive manner, calling individual steps. This is useful as it allows the user to explore interactively and with rapid feedback the parameters that may need adjusting in order to process a batch of scans. The notebooks can be found in the <code>notebook/</code> directory after cloning the GitHub repository.</p> Notebook Description <code>00-Walkthrough-minicircle.ipynb</code> Step-by-step walkthrough of processing <code>minicircle.spm</code> from the <code>tests/resources/</code> directory. <code>01-Walthrhgouh-interactive.ipynb</code> Work in Progress As above but uploading a single scan. Will be deployed in Google Colab/Binder for interactive use. <code>02-Summary-statistics-and-plots.ipynb</code> Plotting statistics interactively. <code>03-Plotting-scans.ipynb</code> Plotting NumPy arrays of scans from different stages of processing."},{"location":"usage/notebooks/#installation","title":"Installation","text":"<p>To be able to run the Notebooks you need some additional Python packages installed. You will have to clone the repository from GitHub (see installation) and then install the Notebook dependencies with the following commands under your Virtual Environment (e.g. Conda)...</p> <pre><code>cd TopoStats\npip install \".[notebooks]\"\n</code></pre>"},{"location":"usage/notebooks/#running-notebooks","title":"Running Notebooks","text":"<p>Start a Jupyter server under the Virtual Environment you have installed the dependencies from and a web-browser page will open from which you can choose which notebook to launch.</p> <pre><code>cd TopoStats/notebooks\njupyter notebook\n</code></pre> <p>For more on Jupyter Notebooks please refer to the official documentation.</p>"},{"location":"usage/workflow/","title":"Workflow","text":"<p>This section gives a broad overview of the workflow and how the different modules interact.</p> graph TB     subgraph Input         IF[Input Files]         IO[\"Input/Output Layer\"]:::io     end      subgraph Configuration         CM[\"Configuration Management\"]:::config         VL[\"Validation Layer\"]:::validation     end      subgraph \"Core Processing\"         PL[\"Processing Layer\"]:::processing         FM[\"Filtering Module\"]:::filter     end      subgraph \"Analysis Layer\"         GA[\"Grain Analysis\"]:::analysis         SM[\"Statistics Module\"]:::analysis          subgraph \"Tracing System\"             TS[\"Tracing Subsystem\"]:::tracing             OT[\"Ordered Tracing\"]             DT[\"Disordered Tracing\"]             NS[\"Node Statistics\"]             SP[\"Splining\"]             DC[\"DNA Curvature\"]         end     end      subgraph \"Measurement System\"         GM[\"Geometry\"]:::measure         CM2[\"Curvature\"]:::measure         HP[\"Height Profiles\"]:::measure         FA[\"Feret Analysis\"]:::measure     end      subgraph \"Visualization\"         PS[\"Plotting System\"]:::viz         TM[\"Theme Management\"]:::viz         OUT[\"Output Generation\"]     end      IF --&gt; IO     IO --&gt; PL     CM --&gt; VL     VL --&gt; PL     PL --&gt; FM     FM --&gt; GA     FM --&gt; SM     GA --&gt; TS     SM --&gt; TS      TS --&gt; OT     TS --&gt; DT     OT --&gt; NS     DT --&gt; NS     NS --&gt; SP     SP --&gt; DC      GA --&gt; GM     GA --&gt; CM2     GA --&gt; HP     GA --&gt; FA      GM --&gt; PS     CM2 --&gt; PS     HP --&gt; PS     FA --&gt; PS     TS --&gt; PS     PS --&gt; TM     TM --&gt; OUT      classDef io fill:#90EE90     classDef config fill:#A9A9A9     classDef processing fill:#87CEEB     classDef filter fill:#87CEEB     classDef analysis fill:#FFA500     classDef tracing fill:#FFA500     classDef measure fill:#DDA0DD     classDef viz fill:#9370DB     classDef validation fill:#A9A9A9      click IO \"https://github.com/AFM-SPM/TopoStats/blob/main/topostats/io.py\"     click PL \"https://github.com/AFM-SPM/TopoStats/blob/main/topostats/processing.py\"     click FM \"https://github.com/AFM-SPM/TopoStats/blob/main/topostats/filters.py\"     click GA \"https://github.com/AFM-SPM/TopoStats/blob/main/topostats/grains.py\"     click SM \"https://github.com/AFM-SPM/TopoStats/blob/main/topostats/statistics.py\"     click TS \"https://github.com/AFM-SPM/TopoStats/tree/main/topostats/tracing/\"     click CM \"https://github.com/AFM-SPM/TopoStats/blob/main/topostats/default_config.yaml\"     click VL \"https://github.com/AFM-SPM/TopoStats/blob/main/topostats/validation.py\"     click GM \"https://github.com/AFM-SPM/TopoStats/blob/main/topostats/measure/geometry.py\"     click CM2 \"https://github.com/AFM-SPM/TopoStats/blob/main/topostats/measure/curvature.py\"     click HP \"https://github.com/AFM-SPM/TopoStats/blob/main/topostats/measure/height_profiles.py\"     click FA \"https://github.com/AFM-SPM/TopoStats/blob/main/topostats/measure/feret.py\"     click PS \"https://github.com/AFM-SPM/TopoStats/blob/main/topostats/plotting.py\"     click TM \"https://github.com/AFM-SPM/TopoStats/blob/main/topostats/theme.py\"  <p>Generated using GitDiagram</p>"}]}